{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# README: How to run this notebook\n",
        "\n",
        "The whole jupyter notebook could be seperated into three parts. The first part would be data preparation and model building, including package import and basic set up. The second part would be image classification part, three major cells are included, including training, resuming training and validation of the image classification work. The final part would be the works related to image verification and the detailed explanation would suggest as follows\n",
        "\n",
        "For the first main part, the first three cells are basic set up, each cell check the GPU type, mount the drivers and import packages respectively. Then the next three cells are model implementation, ranging from the simple convolutional neural network, mobileNet and ConvNext. The next two following cells are for the preperation of dataloader and hyperparameter settings like optimizer and schedular. In this case, we also need to check the number of parameters used in the model to ensure not exceed the 35M\n",
        "\n",
        "For the second main part, it is used to train the model from epoch 1 to end, normally, colab might disconnect occasionally, so another cell is to resume training from specific epoch. The following several blocks are used to validate the training and submit to kaggle.\n",
        "\n",
        "For the third main part, we are focusing on the verification task. We implemented the triplet loss and load the best model we trained in the classification task and use triplet loss fine tuning the model. We would train the model, output the auc score in each epoch and finally select the best model to submit to kaggle.\n"
      ],
      "metadata": {
        "id": "ail8TL-nOi7W"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCemyIldqimj"
      },
      "source": [
        "# Import the package and necessary setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyO5BH_p9NmA",
        "outputId": "6175323d-45d3-41e7-90e1-9eafdd2684bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Mar 15 12:30:27 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    47W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elWGCgQWAxZo"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "%cd /content\n",
        "!mkdir drive\n",
        "%cd drive\n",
        "%cd ..\n",
        "!google-drive-ocamlfuse /content/drive\n",
        "\n",
        "import json\n",
        "\n",
        "TOKEN = {\"username\":\"yuxuanwucmu\",\"key\":\"81ded4babfd327efdc7655be369299b5\"}\n",
        "\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "! mkdir -p .kaggle\n",
        "! mkdir -p /content & mkdir -p /content/.kaggle & mkdir -p /root/.kaggle/\n",
        "\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(TOKEN, file)\n",
        "\n",
        "! pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "! ls \"/content/.kaggle\"\n",
        "! chmod 600 /content/.kaggle/kaggle.json\n",
        "! cp /content/.kaggle/kaggle.json /root/.kaggle/\n",
        "\n",
        "! kaggle config set -n path -v /content/drive\n",
        "\n",
        "!kaggle competitions download -c 11-785-s22-hw2p2-classification\n",
        "!kaggle competitions download -c 11-785-s22-hw2p2-verification\n",
        "\n",
        "!unzip -q /content/drive/competitions/11-785-s22-hw2p2-classification/11-785-s22-hw2p2-classification.zip\n",
        "!unzip -q /content/drive/competitions/11-785-s22-hw2p2-verification/11-785-s22-hw2p2-verification.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwLEd0gdPbSc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "import torchvision\n",
        "import torchvision.transforms as ttf\n",
        "import os\n",
        "import os.path as osp\n",
        "\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def create_folder(folder_path):\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.mkdir(folder_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBTLCyocZBGS"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13usn4nYZCvJ"
      },
      "outputs": [],
      "source": [
        "from torch.nn.modules import normalization\n",
        "\"\"\"\n",
        "The well-accepted SGD batch_size & lr combination for CNN classification is 256 batch size for 0.1 learning rate.\n",
        "When changing batch size for SGD, follow the linear scaling rule - halving batch size -> halve learning rate, etc.\n",
        "This is less theoretically supported for Adam, but in my experience, it's a decent ballpark estimate.\n",
        "\"\"\"\n",
        "batch_size = 256\n",
        "lr = 0.1\n",
        "epochs = 150\n",
        "dataset = \"full\"\n",
        "model = \"ConvNextSelfImple_dropout\"#\"MobileNetMorePara\"\n",
        "transformation = \"RandAug_plus_0.1_label_smoothing\"\n",
        "normalization = \"BatchNorm2d_lr0.1\"\n",
        "scheduler = \"CosineAnnealing\" #\"CosineAnnealing\" #\"ReduceOnPlatue\"\n",
        "optimizerPara = \"SGD\"#\"AdamW\"\n",
        "\n",
        "checkpoints_store_path = \"/content/drive/hw2/store_checkpoints/\"\n",
        "model_id = f\"{model}_{dataset}_{transformation}_{scheduler}_{optimizerPara}_{normalization}\"\n",
        "data_path = checkpoints_store_path + model_id\n",
        "create_folder(data_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIqmojPaWD0H"
      },
      "source": [
        "# Very Simple Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny-mh_ocWIJR"
      },
      "outputs": [],
      "source": [
        "class Conv2dGroup(nn.Module):\n",
        "    def __init__(self, in_size, out_size, kernel_size, stride):\n",
        "        super(Conv2dGroup, self).__init__()\n",
        "\n",
        "        standard_layer = [\n",
        "            nn.Conv2d(in_channels=in_size, out_channels=out_size, kernel_size=kernel_size, stride=stride),\n",
        "            nn.BatchNorm2d(out_size),\n",
        "            nn.ReLU(),\n",
        "        ]\n",
        "        self.layer = nn.Sequential(*standard_layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "class Network(nn.Module):\n",
        "    \"\"\"\n",
        "    The Very Low early deadline architecture is a 4-layer CNN.\n",
        "    The first Conv layer has 64 channels, kernel size 7, and stride 4.\n",
        "    The next three have 128, 256, and 512 channels. Each have kernel size 3 and stride 2.\n",
        "    Think about what the padding should be for each layer to not change spatial resolution.\n",
        "    Each Conv layer is accompanied by a Batchnorm and ReLU layer.\n",
        "    Finally, you want to average pool over the spatial dimensions to reduce them to 1 x 1.\n",
        "    Then, remove (Flatten?) these trivial 1x1 dimensions away.\n",
        "    Look through https://pytorch.org/docs/stable/nn.html \n",
        "    TODO: Fill out the model definition below! \n",
        "\n",
        "    Why does a very simple network have 4 convolutions?\n",
        "    Input images are 224x224. Note that each of these convolutions downsample.\n",
        "    Downsampling 2x effectively doubles the receptive field, increasing the spatial\n",
        "    region each pixel extracts features from. Downsampling 32x is standard\n",
        "    for most image models.\n",
        "\n",
        "    Why does a very simple network have high channel sizes?\n",
        "    Every time you downsample 2x, you do 4x less computation (at same channel size).\n",
        "    To maintain the same level of computation, you 2x increase # of channels, which \n",
        "    increases computation by 4x. So, balances out to same computation.\n",
        "    Another intuition is - as you downsample, you lose spatial information. Want\n",
        "    to preserve some of it in the channel dimension.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=7000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.backbone = nn.Sequential(\n",
        "            \n",
        "            # nn.BatchNorm2d(3),\n",
        "            Conv2dGroup(3, 64, 7, 4),\n",
        "            Conv2dGroup(64, 128, 3, 2),\n",
        "            Conv2dGroup(128, 256, 3, 2),\n",
        "            Conv2dGroup(256, 512, 3, 2),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten()\n",
        "\n",
        "            ) \n",
        "        \n",
        "        self.cls_layer = nn.Linear(512, num_classes)\n",
        "    \n",
        "    def forward(self, x, return_feats=False):\n",
        "        \"\"\"\n",
        "        What is return_feats? It essentially returns the second-to-last-layer\n",
        "        features of a given image. It's a \"feature encoding\" of the input image,\n",
        "        and you can use it for the verification task. You would use the outputs\n",
        "        of the final classification layer for the classification task.\n",
        "\n",
        "        You might also find that the classification outputs are sometimes better\n",
        "        for verification too - try both.\n",
        "        \"\"\"\n",
        "        feats = self.backbone(x)\n",
        "        out = self.cls_layer(feats)\n",
        "\n",
        "        if return_feats:\n",
        "            return feats\n",
        "        else:\n",
        "            return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZI9Ejh4bOfa"
      },
      "source": [
        "# MobileNetV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rf_U8bRkbLr1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "\n",
        "class InvertedResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Intuitively, layers in MobileNet can be split into \"feature mixing\" \n",
        "    and \"spatial mixing\" layers. You can think of feature mixing as each pixel\n",
        "    \"thinking on its own\" about its own features, and you can think of spatial\n",
        "    mixing as pixels \"talking with each other\". Alternating these two builds\n",
        "    up a CNN.\n",
        "\n",
        "    In a bit more detail:\n",
        "\n",
        "    - The purpose of the \"feature mixing\" layers is what you've already seen in \n",
        "    hw1p2. Remember, in hw1p2, we went from some low-level audio input to\n",
        "    semantically rich representations of phonemes. Featuring mixing is simply a \n",
        "    linear layer (a weight matrix) that transforms simpler features into \n",
        "    something more advanced.\n",
        "\n",
        "    - The purpose of the \"spatial mixing\" layers is to mix features from different\n",
        "    spatial locations. You can't figure out a face by looking at each pixel on\n",
        "    its own, right? So we need 3x3 convolutions to mix features from neighboring\n",
        "    pixels to build up spatially larger features.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 stride,\n",
        "                 expand_ratio):\n",
        "        super().__init__()  # Just have to do this for all nn.Module classes\n",
        "\n",
        "        # Can only do identity residual connection if input & output are the\n",
        "        # same channel & spatial shape.\n",
        "        if stride == 1 and in_channels == out_channels:\n",
        "            self.do_identity = True\n",
        "        else:\n",
        "            self.do_identity = False\n",
        "\n",
        "        # Expand Ratio is like 6, so hidden_dim >> in_channels\n",
        "        hidden_dim = in_channels * expand_ratio\n",
        "\n",
        "        \"\"\"\n",
        "        What is this doing? It's a 1x1 convolutional layer that drastically\n",
        "        increases the # of channels (feature dimension). 1x1 means each pixel\n",
        "        is thinking on its own, and increasing # of channels means the network\n",
        "        is seeing if it can \"see\" more clearly in a higher dimensional space.\n",
        "\n",
        "        Some patterns are just more obvious/separable in higher dimensions.\n",
        "\n",
        "        Also, note that bias = False since BatchNorm2d has a bias term built-in.\n",
        "\n",
        "        As you go, note the relationship between kernel_size and padding. As you\n",
        "        covered in class, padding = kernel_size // 2 (kernel_size being odd) to\n",
        "        make sure input & output spatial resolution is the same.\n",
        "        \"\"\"\n",
        "        self.feature_mixing = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_dim, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(hidden_dim),\n",
        "            nn.ReLU6()\n",
        "\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        What is this doing? Let's break it down.\n",
        "        - kernel_size = 3 means neighboring pixels are talking with each other.\n",
        "          This is different from feature mixing, where kernel_size = 1.\n",
        "\n",
        "        - stride. Remember that we sometimes want to down-sample spatially. \n",
        "          Downsampling is done to reduce # of pixels (less computation to do), \n",
        "          and also to increase receptive field (if a face was 32x32, and now\n",
        "          it's 16x16, a 3x3 convolution covers more of the face, right?). It\n",
        "          makes sense to put the downsampling in the spatial mixing portion\n",
        "          since this layer is \"in charge\" of messing around spatially anyway.\n",
        "\n",
        "          Note that most of the time, stride is 1. It's just the first block of\n",
        "          every \"stage\" (layer \\subsetof block \\subsetof stage) that we have\n",
        "          stride = 2.\n",
        "\n",
        "        - groups = hidden_dim. Remember depthwise separable convolutions in \n",
        "          class? If not, it's fine. Usually, when we go from hidden_dim channels\n",
        "          to hidden_dim channels, they're densely connected (like a linear \n",
        "          layer). So you can think of every pixel/grid in an input\n",
        "          3 x 3 x hidden_dim block being connected to every single pixel/grid \n",
        "          in the output 3 x 3 x hidden_dim block.\n",
        "          What groups = hidden_dim does is remove a lot of these connections.\n",
        "\n",
        "          Now, each input 3 x 3 block/region is densely connected to the\n",
        "          corresponding output 3 x 3 block/region. This happens for each of the\n",
        "          hidden_dim input/output channel pairs independently.\n",
        "          So we're not even mixing different channels together - we're only \n",
        "          mixing spatial neighborhoods. \n",
        "          \n",
        "          Try to draw this out, or come to my (Jinhyung Park)'s OH if you want \n",
        "          a more in-depth explanation.\n",
        "          https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728\n",
        "        \"\"\"\n",
        "        self.spatial_mixing = nn.Sequential(\n",
        "            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1, stride=stride, groups=hidden_dim, bias=False),\n",
        "            nn.BatchNorm2d(hidden_dim),\n",
        "            nn.ReLU6()\n",
        "\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        What's this? Remember that hidden_dim is quite large - six times the \n",
        "        in_channels. So it was nice to do the above operations in this high-dim\n",
        "        space, where some patterns might be more clear. But we still want to \n",
        "        bring it back down-to-earth.\n",
        "\n",
        "        Intuitively, you can takeaway two reasons for doing this:\n",
        "        - Reduces computational cost by a lot. 6x in & out channels means 36x\n",
        "          larger weights, which is crazy. We're okay with just one of input or \n",
        "          output of a convolutional layer being large when mixing channels, but \n",
        "          not both.\n",
        "        \n",
        "        - We also want a residual connection from the input to the output. To \n",
        "          do that without introducing another convolutional layer, we want to\n",
        "          condense the # of channels back to be the same as the in_channels.\n",
        "          (out_channels and in_channels are usually the same).\n",
        "        \"\"\"\n",
        "        self.bottleneck_channels = nn.Sequential(\n",
        "            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.feature_mixing(x)\n",
        "        out = self.spatial_mixing(out)\n",
        "        out = self.bottleneck_channels(out)\n",
        "\n",
        "        if self.do_identity:\n",
        "            return x + out\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "\n",
        "class MobileNetV2(nn.Module):\n",
        "    \"\"\"\n",
        "    The heavy lifting is already done in InvertedBottleneck.\n",
        "\n",
        "    Why MobileNetV2 and not V3? V2 is the foundation for V3, which uses \"neural\n",
        "    architecture search\" to find better configurations of V2. If you understand\n",
        "    V2 well, you can totally implement V3!\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=7000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        \"\"\"\n",
        "        First couple of layers are special, just do them here.\n",
        "        This is called the \"stem\". Usually, methods use it to downsample or twice.\n",
        "        \"\"\"\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU6(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, groups=32, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU6(),\n",
        "            nn.Conv2d(32, 16, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        Since we're just repeating InvertedResidualBlocks again and again, we\n",
        "        want to specify their parameters like this.\n",
        "        The four numbers in each row (a stage) are shown below.\n",
        "        - Expand ratio: We talked about this in InvertedResidualBlock\n",
        "        - Channels: This specifies the channel size before expansion\n",
        "        - # blocks: Each stage has many blocks, how many?\n",
        "        - Stride of first block: For some stages, we want to downsample. In a\n",
        "          downsampling stage, we set the first block in that stage to have\n",
        "          stride = 2, and the rest just have stride = 1.\n",
        "\n",
        "        Again, note that almost every stage here is downsampling! By the time\n",
        "        we get to the last stage, what is the image resolution? Can it still\n",
        "        be called an image for our dataset? Think about this, and make changes\n",
        "        as you want.\n",
        "        \"\"\"\n",
        "        self.stage_cfgs = [\n",
        "            # expand_ratio, channels, # blocks, stride of first block\n",
        "            [6, 24, 10, 2],\n",
        "            [6, 32, 12, 2],\n",
        "            [6, 64, 16, 2],\n",
        "            [6, 96, 18, 2],\n",
        "            [6, 128, 16, 2],\n",
        "            [6, 320, 10, 2],\n",
        "            [6, 640, 2, 2],\n",
        "        ]\n",
        "\n",
        "        # Remember that our stem left us off at 16 channels. We're going to \n",
        "        # keep updating this in_channels variable as we go\n",
        "        in_channels = 16\n",
        "\n",
        "        # Let's make the layers\n",
        "        layers = []\n",
        "        for curr_stage in self.stage_cfgs:\n",
        "            expand_ratio, num_channels, num_blocks, stride = curr_stage\n",
        "\n",
        "            for block_idx in range(num_blocks):\n",
        "                out_channels = num_channels\n",
        "                layers.append(InvertedResidualBlock(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    # only have non-trivial stride if first block\n",
        "                    stride=stride if block_idx == 0 else 1,\n",
        "                    expand_ratio=expand_ratio\n",
        "                ))\n",
        "                # In channels of the next block is the out_channels of the current one\n",
        "                in_channels = out_channels\n",
        "\n",
        "        self.layers = nn.Sequential(*layers)  # Done, save them to the class\n",
        "\n",
        "        # Some final feature mixing\n",
        "        self.final_block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 1280, kernel_size=1, padding=0, stride=1, bias=False),\n",
        "            nn.BatchNorm2d(1280),\n",
        "            nn.ReLU6()\n",
        "        )\n",
        "\n",
        "        # Now, we need to build the final classification layer.\n",
        "        self.cls_layer = nn.Sequential(\n",
        "            # Pool over & collapse the spatial dimensions to (1, 1)\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            # Collapse the trivial (1, 1) dimensions\n",
        "            nn.Flatten(),\n",
        "            # Project to our # of classes\n",
        "            nn.Linear(1280, num_classes)\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"\n",
        "        Usually, I like to use default pytorch initialization for stuff, but\n",
        "        MobileNetV2 made a point of putting in some custom ones, so let's just\n",
        "        use them.\n",
        "        \"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x, return_feats=False):\n",
        "        out = self.stem(x) # down-sample\n",
        "        out = self.layers(out) # InvertedResidualBlocks\n",
        "        feats = self.final_block(out)\n",
        "        out = self.cls_layer(feats)\n",
        "\n",
        "        if return_feats:\n",
        "            return feats\n",
        "        else:\n",
        "            return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8VSL_p0AnZv"
      },
      "source": [
        "# ConvNext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogWoRSGgAQWB",
        "outputId": "4c953368-422d-45c0-a7ed-c9d806bf5e64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.5.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torchsummary import summary\n",
        "import numpy as np\n",
        "from timm.models.layers import trunc_normal_, DropPath\n",
        "from timm.models.registry import register_model\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 expand_ratio, drop_path=0.):\n",
        "        super().__init__()  # Just have to do this for all nn.Module classes\n",
        "\n",
        "        # change the kernel size to 7 (spatial mix)\n",
        "        # depth wise convolution\n",
        "        # 3*3 input padding 1, stride ==1\n",
        "        self.spatial_mixing = nn.Conv2d(in_channels, in_channels, kernel_size=7, padding=3, stride=1,\n",
        "                                        groups=in_channels)\n",
        "        self.norm = nn.BatchNorm2d(in_channels)\n",
        "\n",
        "        # point wise convolution, increase the channel number, expand ratio =1\n",
        "        hidden_dim = in_channels * expand_ratio\n",
        "        self.feature_mixing = nn.Conv2d(in_channels, hidden_dim, kernel_size=1, stride=1, padding=0)\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "        self.bottleneck_channels = nn.Conv2d(hidden_dim, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        input = x\n",
        "        x = self.spatial_mixing(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.feature_mixing(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.bottleneck_channels(x)\n",
        "        x = input + self.drop_path(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvNeXt(nn.Module):\n",
        "    \"\"\"\n",
        "    1. stem + 3 intermediate downsampling layers (stem stage)\n",
        "        Batch norm already shifts the data using its mean. You don't have to make the output affine again. Thats why usually BatchNorm bias is False.\n",
        "    2. feature resolution stage\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=7000, drop_path_rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        \"\"\"\n",
        "        First couple of layers are special, just do them here.\n",
        "        This is called the \"stem\". Usually, methods use it to downsample or twice.\n",
        "        \"\"\"\n",
        "        stem = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=4, stride=4),\n",
        "            nn.BatchNorm2d(96),\n",
        "        )\n",
        "\n",
        "        self.downsample_layers = nn.ModuleList()\n",
        "        self.downsample_layers.append(stem)\n",
        "\n",
        "        downsample_layer = nn.Sequential(nn.BatchNorm2d(96), nn.Conv2d(96, 192, kernel_size=2, stride=2))\n",
        "        self.downsample_layers.append(downsample_layer)\n",
        "\n",
        "        downsample_layer = nn.Sequential(nn.BatchNorm2d(192), nn.Conv2d(192, 384, kernel_size=2, stride=2))\n",
        "        self.downsample_layers.append(downsample_layer)\n",
        "\n",
        "        downsample_layer = nn.Sequential(nn.BatchNorm2d(384), nn.Conv2d(384, 768, kernel_size=2, stride=2))\n",
        "        self.downsample_layers.append(downsample_layer)\n",
        "\n",
        "        # Feature resolution stage\n",
        "        self.stages = nn.ModuleList()\n",
        "\n",
        "        # drop rates (3+9+3+3)\n",
        "        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, (3+9+3+3))]\n",
        "\n",
        "        # Dim 96 combo\n",
        "        block_96 = [Block(in_channels=96, out_channels=96, expand_ratio=4, drop_path=dp_rates[j]) for j in range(3)]\n",
        "        self.stages.append(nn.Sequential(*block_96))\n",
        "\n",
        "        # Dim 192 combo\n",
        "        block_192 = [Block(in_channels=192, out_channels=192, expand_ratio=4, drop_path=dp_rates[j + 3]) for j in\n",
        "                     range(3)]\n",
        "        self.stages.append(nn.Sequential(*block_192))\n",
        "\n",
        "        # Dim 384 combo\n",
        "        block_384 = [Block(in_channels=384, out_channels=384, expand_ratio=4, drop_path=dp_rates[j + 3 + 3]) for j in\n",
        "                     range(9)]\n",
        "        self.stages.append(nn.Sequential(*block_384))\n",
        "\n",
        "        # Dim 192 combo\n",
        "        block_768 = [Block(in_channels=768, out_channels=768, expand_ratio=4, drop_path=dp_rates[j + 3 + 9 + 3]) for j\n",
        "                     in range(3)]\n",
        "        self.stages.append(nn.Sequential(*block_768))\n",
        "\n",
        "        # Some final feature mixing\n",
        "        self.norm = nn.BatchNorm2d(768)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "        self.cls_layer = nn.Sequential(\n",
        "            nn.Linear(768, num_classes)\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"\n",
        "        Usually, I like to use default pytorch initialization for stuff, but\n",
        "        MobileNetV2 made a point of putting in some custom ones, so let's just\n",
        "        use them.\n",
        "        \"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        for i in range(4):\n",
        "            x = self.downsample_layers[i](x)  # torch.Size([2, 3, 224, 224]) -> torch.Size([2, 96, 56, 56])\n",
        "            x = self.stages[i](x)\n",
        "\n",
        "        return self.flatten(self.norm(x).mean([-2, -1]))  # global average pooling, (N, C, H, W) -> (N, C)\n",
        "\n",
        "    def forward(self, x, return_feats=False):\n",
        "        feats = self.forward_features(x)\n",
        "        x = self.cls_layer(feats)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        if return_feats:\n",
        "            return feats\n",
        "        else:\n",
        "            return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwYR-CLwX09u"
      },
      "source": [
        "# Dataset & DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awE5BxlqX2o7"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"/content\"\n",
        "# TRAIN_DIR = osp.join(DATA_DIR, \"train_subset/train_subset\") # This is a smaller subset of the data. Should change this to classification/classification/train\n",
        "TRAIN_DIR = osp.join(DATA_DIR, \"classification/classification/train\") \n",
        "\n",
        "VAL_DIR = osp.join(DATA_DIR, \"classification/classification/dev\")\n",
        "\n",
        "# data augmentation\n",
        "transforms = ttf.Compose([\n",
        "    ttf.RandAugment(),\n",
        "    # ttf.AutoAugment(),\n",
        "    ttf.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.ImageFolder(TRAIN_DIR,\n",
        "                                                 transform=transforms)\n",
        "val_dataset = torchvision.datasets.ImageFolder(VAL_DIR,\n",
        "                                               transform=ttf.Compose([ttf.ToTensor()]))\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                          shuffle=True, drop_last=True, num_workers=8)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                        drop_last=True, num_workers=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZCn0qHuZRKj"
      },
      "source": [
        "# Setup optimizer/schedualr for training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UowI9OcUYPjP",
        "outputId": "9313e2ee-cc98-4751-e2ee-ab7156029aad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Params: 33196504\n"
          ]
        }
      ],
      "source": [
        "# model = ResNet50()\n",
        "model = ConvNeXt()\n",
        "# model = ConvNeXt()\n",
        "# model = MobileNetV2()\n",
        "model.cuda()\n",
        "\n",
        "# For this homework, we're limiting you to 35 million trainable parameters, as\n",
        "# outputted by this. This is to help constrain your search space and maintain\n",
        "# reasonable training times & expectations\n",
        "num_trainable_parameters = 0\n",
        "for p in model.parameters():\n",
        "    num_trainable_parameters += p.numel()\n",
        "print(\"Number of Params: {}\".format(num_trainable_parameters))\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
        "# optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4) # AdamW\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader) * epochs))\n",
        "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5,patience=2, mode='max',threshold=0.01,verbose=True)\n",
        "# T_max is \"how many times will i call scheduler.step() until it reaches 0 lr?\"\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzM11HtcboYv"
      },
      "source": [
        "# Classification Task: Train the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrChwbscbYkj",
        "outputId": "e89a92e2-5fa7-4b0d-f698-1b05c3759716"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150: Train Acc 0.0658%, Train Loss 8.6562, Learning Rate 0.1000 \n",
            "\n",
            " 3.8363085786501565 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/150: Train Acc 0.8893%, Train Loss 7.9146, Learning Rate 0.1000 \n",
            "\n",
            " 3.839955035845439 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/150: Train Acc 5.1461%, Train Loss 7.0125, Learning Rate 0.0999 \n",
            "\n",
            " 3.8620699365933735 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/150: Train Acc 14.9632%, Train Loss 6.1530, Learning Rate 0.0998 \n",
            "\n",
            " 3.852550220489502 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/150: Train Acc 28.6759%, Train Loss 5.3757, Learning Rate 0.0997 \n",
            "\n",
            " 3.8578346172968545 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/150: Train Acc 41.4706%, Train Loss 4.7342, Learning Rate 0.0996 \n",
            "\n",
            " 3.886575663089752 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/150: Train Acc 52.2085%, Train Loss 4.2154, Learning Rate 0.0995 \n",
            "\n",
            " 3.8687500834465025 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/150: Train Acc 60.3923%, Train Loss 3.8295, Learning Rate 0.0993 \n",
            "\n",
            " 3.873291965325673 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/150: Train Acc 66.8026%, Train Loss 3.5179, Learning Rate 0.0991 \n",
            "\n",
            " 3.862355395158132 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/150: Train Acc 71.8972%, Train Loss 3.2181, Learning Rate 0.0989 \n",
            "\n",
            " 3.8440557797749837 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/150: Train Acc 75.1688%, Train Loss 2.9859, Learning Rate 0.0987 \n",
            "\n",
            " 3.8997453848520913 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/150: Train Acc 77.8488%, Train Loss 2.7932, Learning Rate 0.0984 \n",
            "\n",
            " 3.8572275559107463 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/150: Train Acc 80.1003%, Train Loss 2.6705, Learning Rate 0.0982 \n",
            "\n",
            " 3.9173010349273683 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/150: Train Acc 82.4154%, Train Loss 2.5583, Learning Rate 0.0979 \n",
            "\n",
            " 3.9285218636194865 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/150: Train Acc 84.1246%, Train Loss 2.4773, Learning Rate 0.0976 \n",
            "\n",
            " 3.872614896297455 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/150: Train Acc 85.4138%, Train Loss 2.4127, Learning Rate 0.0972 \n",
            "\n",
            " 3.8635461727778115 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/150: Train Acc 85.9518%, Train Loss 2.3831, Learning Rate 0.0969 \n",
            "\n",
            " 3.8548874219258624 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/150: Train Acc 86.6880%, Train Loss 2.3386, Learning Rate 0.0965 \n",
            "\n",
            " 3.888357961177826 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/150: Train Acc 87.0049%, Train Loss 2.3225, Learning Rate 0.0961 \n",
            "\n",
            " 3.8514486153920493 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/150: Train Acc 87.3676%, Train Loss 2.2989, Learning Rate 0.0957 \n",
            "\n",
            " 3.8620041410128274 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/150: Train Acc 87.6767%, Train Loss 2.2792, Learning Rate 0.0952 \n",
            "\n",
            " 3.871598947048187 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/150: Train Acc 87.7146%, Train Loss 2.2732, Learning Rate 0.0948 \n",
            "\n",
            " 3.8575014233589173 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/150: Train Acc 87.7669%, Train Loss 2.2674, Learning Rate 0.0943 \n",
            "\n",
            " 3.86794669230779 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/150: Train Acc 87.8906%, Train Loss 2.2550, Learning Rate 0.0938 \n",
            "\n",
            " 3.8731027086575827 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/150: Train Acc 88.0230%, Train Loss 2.2445, Learning Rate 0.0933 \n",
            "\n",
            " 3.8665797750155133 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/150: Train Acc 88.1668%, Train Loss 2.2343, Learning Rate 0.0928 \n",
            "\n",
            " 3.8703240036964415 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/150: Train Acc 88.2226%, Train Loss 2.2285, Learning Rate 0.0922 \n",
            "\n",
            " 3.8926372647285463 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/150: Train Acc 88.1539%, Train Loss 2.2294, Learning Rate 0.0916 \n",
            "\n",
            " 3.9076795895894367 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/150: Train Acc 88.2991%, Train Loss 2.2181, Learning Rate 0.0911 \n",
            "\n",
            " 3.911422296365102 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/150: Train Acc 88.3850%, Train Loss 2.2117, Learning Rate 0.0905 \n",
            "\n",
            " 3.8445153991381327 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/150: Train Acc 88.4973%, Train Loss 2.2034, Learning Rate 0.0898 \n",
            "\n",
            " 3.856520676612854 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/150: Train Acc 88.5946%, Train Loss 2.1934, Learning Rate 0.0892 \n",
            "\n",
            " 3.8652861595153807 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/150: Train Acc 88.5631%, Train Loss 2.1974, Learning Rate 0.0885 \n",
            "\n",
            " 3.896113467216492 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/150: Train Acc 88.6075%, Train Loss 2.1936, Learning Rate 0.0878 \n",
            "\n",
            " 3.870450969537099 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/150: Train Acc 88.6440%, Train Loss 2.1858, Learning Rate 0.0872 \n",
            "\n",
            " 3.8613805055618284 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/150: Train Acc 88.7985%, Train Loss 2.1780, Learning Rate 0.0864 \n",
            "\n",
            " 3.85339271624883 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/150: Train Acc 88.9323%, Train Loss 2.1694, Learning Rate 0.0857 \n",
            "\n",
            " 3.855262454350789 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/150: Train Acc 88.7534%, Train Loss 2.1726, Learning Rate 0.0850 \n",
            "\n",
            " 3.8704043070475262 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39/150: Train Acc 88.9244%, Train Loss 2.1614, Learning Rate 0.0842 \n",
            "\n",
            " 3.8528399070103965 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/150: Train Acc 88.9022%, Train Loss 2.1613, Learning Rate 0.0835 \n",
            "\n",
            " 3.8829032222429913 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41/150: Train Acc 88.8321%, Train Loss 2.1641, Learning Rate 0.0827 \n",
            "\n",
            " 3.854296290874481 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42/150: Train Acc 89.0017%, Train Loss 2.1510, Learning Rate 0.0819 \n",
            "\n",
            " 3.878300166130066 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43/150: Train Acc 88.9394%, Train Loss 2.1551, Learning Rate 0.0811 \n",
            "\n",
            " 3.857539149125417 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44/150: Train Acc 89.0911%, Train Loss 2.1395, Learning Rate 0.0802 \n",
            "\n",
            " 3.856618344783783 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45/150: Train Acc 89.1856%, Train Loss 2.1337, Learning Rate 0.0794 \n",
            "\n",
            " 3.8670746803283693 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46/150: Train Acc 89.1226%, Train Loss 2.1362, Learning Rate 0.0785 \n",
            "\n",
            " 3.8476856191953024 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47/150: Train Acc 89.1061%, Train Loss 2.1356, Learning Rate 0.0777 \n",
            "\n",
            " 3.8789977351824443 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48/150: Train Acc 89.1455%, Train Loss 2.1317, Learning Rate 0.0768 \n",
            "\n",
            " 3.88303679227829 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49/150: Train Acc 89.1283%, Train Loss 2.1304, Learning Rate 0.0759 \n",
            "\n",
            " 3.8713205377260844 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/150: Train Acc 89.3129%, Train Loss 2.1167, Learning Rate 0.0750 \n",
            "\n",
            " 3.870379829406738 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 51/150: Train Acc 89.2864%, Train Loss 2.1163, Learning Rate 0.0741 \n",
            "\n",
            " 3.9042181889216105 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 52/150: Train Acc 89.2321%, Train Loss 2.1170, Learning Rate 0.0732 \n",
            "\n",
            " 3.868407201766968 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 53/150: Train Acc 89.3258%, Train Loss 2.1084, Learning Rate 0.0722 \n",
            "\n",
            " 3.871434728304545 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 54/150: Train Acc 89.4073%, Train Loss 2.1015, Learning Rate 0.0713 \n",
            "\n",
            " 3.8525020400683085 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 55/150: Train Acc 89.4073%, Train Loss 2.1018, Learning Rate 0.0703 \n",
            "\n",
            " 3.9019830147425334 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 56/150: Train Acc 89.4309%, Train Loss 2.0987, Learning Rate 0.0694 \n",
            "\n",
            " 3.858998143672943 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 57/150: Train Acc 89.3480%, Train Loss 2.0998, Learning Rate 0.0684 \n",
            "\n",
            " 3.858205743630727 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 58/150: Train Acc 89.3329%, Train Loss 2.0976, Learning Rate 0.0674 \n",
            "\n",
            " 3.848817034562429 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 59/150: Train Acc 89.4810%, Train Loss 2.0870, Learning Rate 0.0664 \n",
            "\n",
            " 3.846176552772522 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 60/150: Train Acc 89.5225%, Train Loss 2.0818, Learning Rate 0.0655 \n",
            "\n",
            " 3.8626959760983786 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 61/150: Train Acc 89.4603%, Train Loss 2.0844, Learning Rate 0.0645 \n",
            "\n",
            " 3.877136433124542 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 62/150: Train Acc 89.4302%, Train Loss 2.0831, Learning Rate 0.0634 \n",
            "\n",
            " 3.851140197118123 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 63/150: Train Acc 89.5590%, Train Loss 2.0751, Learning Rate 0.0624 \n",
            "\n",
            " 3.877454936504364 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 64/150: Train Acc 89.6026%, Train Loss 2.0687, Learning Rate 0.0614 \n",
            "\n",
            " 3.8688155810038247 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 65/150: Train Acc 89.6313%, Train Loss 2.0651, Learning Rate 0.0604 \n",
            "\n",
            " 3.8823956648508706 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 66/150: Train Acc 89.6005%, Train Loss 2.0639, Learning Rate 0.0594 \n",
            "\n",
            " 3.8606945276260376 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 67/150: Train Acc 89.5404%, Train Loss 2.0670, Learning Rate 0.0583 \n",
            "\n",
            " 3.8582712610562644 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 68/150: Train Acc 89.6663%, Train Loss 2.0564, Learning Rate 0.0573 \n",
            "\n",
            " 3.8515575369199118 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 69/150: Train Acc 89.6999%, Train Loss 2.0526, Learning Rate 0.0563 \n",
            "\n",
            " 3.8815813024838763 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 70/150: Train Acc 89.6492%, Train Loss 2.0555, Learning Rate 0.0552 \n",
            "\n",
            " 3.849536685148875 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 71/150: Train Acc 89.7078%, Train Loss 2.0487, Learning Rate 0.0542 \n",
            "\n",
            " 3.9628841439882914 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 72/150: Train Acc 89.5654%, Train Loss 2.0545, Learning Rate 0.0531 \n",
            "\n",
            " 3.875212093194326 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 73/150: Train Acc 89.7672%, Train Loss 2.0398, Learning Rate 0.0521 \n",
            "\n",
            " 3.8495607376098633 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 74/150: Train Acc 89.6255%, Train Loss 2.0456, Learning Rate 0.0510 \n",
            "\n",
            " 3.8549267450968423 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 75/150: Train Acc 89.7944%, Train Loss 2.0339, Learning Rate 0.0500 \n",
            "\n",
            " 3.88581413825353 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 76/150: Train Acc 89.8216%, Train Loss 2.0314, Learning Rate 0.0490 \n",
            "\n",
            " 3.855894454320272 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 77/150: Train Acc 89.8974%, Train Loss 2.0242, Learning Rate 0.0479 \n",
            "\n",
            " 3.849430863062541 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 78/150: Train Acc 89.6935%, Train Loss 2.0350, Learning Rate 0.0469 \n",
            "\n",
            " 3.8656876484553018 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 79/150: Train Acc 89.6971%, Train Loss 2.0322, Learning Rate 0.0458 \n",
            "\n",
            " 3.8605401198069256 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 80/150: Train Acc 89.7872%, Train Loss 2.0252, Learning Rate 0.0448 \n",
            "\n",
            " 3.8555984258651734 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 81/150: Train Acc 89.6177%, Train Loss 2.0346, Learning Rate 0.0437 \n",
            "\n",
            " 3.854997007052104 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 82/150: Train Acc 89.8316%, Train Loss 2.0186, Learning Rate 0.0427 \n",
            "\n",
            " 3.878119242191315 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 83/150: Train Acc 89.8695%, Train Loss 2.0137, Learning Rate 0.0417 \n",
            "\n",
            " 3.8606109380722047 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 84/150: Train Acc 89.6964%, Train Loss 2.0244, Learning Rate 0.0406 \n",
            "\n",
            " 3.8404507954915363 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 85/150: Train Acc 89.7915%, Train Loss 2.0171, Learning Rate 0.0396 \n",
            "\n",
            " 3.8593149224917096 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 86/150: Train Acc 89.7457%, Train Loss 2.0172, Learning Rate 0.0386 \n",
            "\n",
            " 3.8658796389897665 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 87/150: Train Acc 89.6785%, Train Loss 2.0186, Learning Rate 0.0376 \n",
            "\n",
            " 3.8670148730278013 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 88/150: Train Acc 89.8609%, Train Loss 2.0049, Learning Rate 0.0366 \n",
            "\n",
            " 3.8457866152127584 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 89/150: Train Acc 89.7915%, Train Loss 2.0071, Learning Rate 0.0355 \n",
            "\n",
            " 3.8844675183296205 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 90/150: Train Acc 89.8280%, Train Loss 2.0030, Learning Rate 0.0345 \n",
            "\n",
            " 3.861160063743591 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 91/150: Train Acc 89.8187%, Train Loss 2.0007, Learning Rate 0.0336 \n",
            "\n",
            " 3.879338812828064 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 92/150: Train Acc 89.8523%, Train Loss 1.9993, Learning Rate 0.0326 \n",
            "\n",
            " 3.855731117725372 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 93/150: Train Acc 89.7550%, Train Loss 2.0025, Learning Rate 0.0316 \n",
            "\n",
            " 3.85532488822937 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 94/150: Train Acc 89.9346%, Train Loss 1.9900, Learning Rate 0.0306 \n",
            "\n",
            " 3.865298541386922 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 95/150: Train Acc 90.0004%, Train Loss 1.9847, Learning Rate 0.0297 \n",
            "\n",
            " 3.859853919347127 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 96/150: Train Acc 89.8581%, Train Loss 1.9911, Learning Rate 0.0287 \n",
            "\n",
            " 3.86423902908961 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 97/150: Train Acc 89.8938%, Train Loss 1.9868, Learning Rate 0.0278 \n",
            "\n",
            " 3.8491111477216085 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 98/150: Train Acc 89.9682%, Train Loss 1.9812, Learning Rate 0.0268 \n",
            "\n",
            " 3.8748903314272565 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 99/150: Train Acc 89.6413%, Train Loss 2.0007, Learning Rate 0.0259 \n",
            "\n",
            " 3.920171594619751 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/150: Train Acc 90.0133%, Train Loss 1.9743, Learning Rate 0.0250 \n",
            "\n",
            " 3.9433886766433717 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 101/150: Train Acc 89.8731%, Train Loss 1.9823, Learning Rate 0.0241 \n",
            "\n",
            " 3.899858272075653 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 102/150: Train Acc 89.9897%, Train Loss 1.9718, Learning Rate 0.0232 \n",
            "\n",
            " 3.872350815931956 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 103/150: Train Acc 90.0698%, Train Loss 1.9666, Learning Rate 0.0223 \n",
            "\n",
            " 3.854131368796031 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 104/150: Train Acc 89.8838%, Train Loss 1.9761, Learning Rate 0.0215 \n",
            "\n",
            " 3.8428102771441144 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 105/150: Train Acc 89.9074%, Train Loss 1.9734, Learning Rate 0.0206 \n",
            "\n",
            " 3.8534873763720197 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 106/150: Train Acc 90.0519%, Train Loss 1.9624, Learning Rate 0.0198 \n",
            "\n",
            " 3.8759323875109355 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 107/150: Train Acc 89.8366%, Train Loss 1.9750, Learning Rate 0.0189 \n",
            "\n",
            " 3.8599671284357706 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 108/150: Train Acc 89.8266%, Train Loss 1.9735, Learning Rate 0.0181 \n",
            "\n",
            " 3.848123296101888 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 109/150: Train Acc 89.9697%, Train Loss 1.9636, Learning Rate 0.0173 \n",
            "\n",
            " 3.91672598918279 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 110/150: Train Acc 89.8967%, Train Loss 1.9671, Learning Rate 0.0165 \n",
            "\n",
            " 3.914385922749837 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 111/150: Train Acc 90.1471%, Train Loss 1.9489, Learning Rate 0.0158 \n",
            "\n",
            " 3.9090270161628724 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 112/150: Train Acc 89.9010%, Train Loss 1.9633, Learning Rate 0.0150 \n",
            "\n",
            " 3.849647255738576 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 113/150: Train Acc 89.8831%, Train Loss 1.9630, Learning Rate 0.0143 \n",
            "\n",
            " 3.868568738301595 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 114/150: Train Acc 89.9511%, Train Loss 1.9571, Learning Rate 0.0136 \n",
            "\n",
            " 3.85196879307429 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 115/150: Train Acc 89.8874%, Train Loss 1.9593, Learning Rate 0.0128 \n",
            "\n",
            " 3.8506410757700604 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 116/150: Train Acc 89.9174%, Train Loss 1.9561, Learning Rate 0.0122 \n",
            "\n",
            " 3.862179704507192 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 117/150: Train Acc 90.0147%, Train Loss 1.9493, Learning Rate 0.0115 \n",
            "\n",
            " 3.8836012721061706 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 118/150: Train Acc 89.8967%, Train Loss 1.9558, Learning Rate 0.0108 \n",
            "\n",
            " 3.8691479563713074 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 119/150: Train Acc 90.0405%, Train Loss 1.9456, Learning Rate 0.0102 \n",
            "\n",
            " 3.892748538653056 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 120/150: Train Acc 89.9196%, Train Loss 1.9518, Learning Rate 0.0095 \n",
            "\n",
            " 3.8801867008209228 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 121/150: Train Acc 89.9711%, Train Loss 1.9478, Learning Rate 0.0089 \n",
            "\n",
            " 3.872723909219106 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 122/150: Train Acc 90.0011%, Train Loss 1.9448, Learning Rate 0.0084 \n",
            "\n",
            " 3.8679453253746034 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 123/150: Train Acc 89.7901%, Train Loss 1.9571, Learning Rate 0.0078 \n",
            "\n",
            " 3.8830745299657186 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 124/150: Train Acc 89.9632%, Train Loss 1.9445, Learning Rate 0.0072 \n",
            "\n",
            " 3.8473328312238055 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 125/150: Train Acc 89.8545%, Train Loss 1.9506, Learning Rate 0.0067 \n",
            "\n",
            " 3.8441006342569985 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 126/150: Train Acc 89.8523%, Train Loss 1.9497, Learning Rate 0.0062 \n",
            "\n",
            " 3.8697543660799663 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 127/150: Train Acc 90.0512%, Train Loss 1.9365, Learning Rate 0.0057 \n",
            "\n",
            " 3.8676225264867146 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 128/150: Train Acc 89.9768%, Train Loss 1.9401, Learning Rate 0.0052 \n",
            "\n",
            " 3.8649959405263266 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 129/150: Train Acc 90.0090%, Train Loss 1.9373, Learning Rate 0.0048 \n",
            "\n",
            " 3.865364933013916 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 130/150: Train Acc 89.9554%, Train Loss 1.9404, Learning Rate 0.0043 \n",
            "\n",
            " 3.885703992843628 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 131/150: Train Acc 89.9625%, Train Loss 1.9387, Learning Rate 0.0039 \n",
            "\n",
            " 3.9602760752042134 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 132/150: Train Acc 89.9818%, Train Loss 1.9362, Learning Rate 0.0035 \n",
            "\n",
            " 3.849486434459686 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 133/150: Train Acc 89.9446%, Train Loss 1.9383, Learning Rate 0.0031 \n",
            "\n",
            " 3.8489320119222006 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 134/150: Train Acc 89.9453%, Train Loss 1.9379, Learning Rate 0.0028 \n",
            "\n",
            " 3.8863594492276508 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 135/150: Train Acc 90.0641%, Train Loss 1.9302, Learning Rate 0.0024 \n",
            "\n",
            " 3.8751012841860453 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 136/150: Train Acc 90.1221%, Train Loss 1.9251, Learning Rate 0.0021 \n",
            "\n",
            " 3.8616149624188743 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 137/150: Train Acc 89.9804%, Train Loss 1.9339, Learning Rate 0.0018 \n",
            "\n",
            " 3.9445790529251097 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 138/150: Train Acc 90.0290%, Train Loss 1.9296, Learning Rate 0.0016 \n",
            "\n",
            " 3.8424040913581847 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 139/150: Train Acc 90.0491%, Train Loss 1.9285, Learning Rate 0.0013 \n",
            "\n",
            " 3.877110409736633 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 140/150: Train Acc 90.0462%, Train Loss 1.9282, Learning Rate 0.0011 \n",
            "\n",
            " 3.9101724068323773 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 141/150: Train Acc 89.9976%, Train Loss 1.9309, Learning Rate 0.0009 \n",
            "\n",
            " 3.865354331334432 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 142/150: Train Acc 89.8867%, Train Loss 1.9374, Learning Rate 0.0007 \n",
            "\n",
            " 3.8843679070472716 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 143/150: Train Acc 90.0662%, Train Loss 1.9262, Learning Rate 0.0005 \n",
            "\n",
            " 3.8635205149650576 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 144/150: Train Acc 89.9368%, Train Loss 1.9339, Learning Rate 0.0004 \n",
            "\n",
            " 3.852676026026408 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 145/150: Train Acc 89.9618%, Train Loss 1.9321, Learning Rate 0.0003 \n",
            "\n",
            " 3.853631826241811 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 146/150: Train Acc 89.9489%, Train Loss 1.9326, Learning Rate 0.0002 \n",
            "\n",
            " 3.8637343049049377 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 147/150: Train Acc 89.9575%, Train Loss 1.9317, Learning Rate 0.0001 \n",
            "\n",
            " 3.86182625691096 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 148/150: Train Acc 89.9625%, Train Loss 1.9319, Learning Rate 0.0000 \n",
            "\n",
            " 3.8533109148343403 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 149/150: Train Acc 90.0527%, Train Loss 1.9256, Learning Rate 0.0000 \n",
            "\n",
            " 3.865573743979136 minutes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 150/150: Train Acc 90.0434%, Train Loss 1.9262, Learning Rate 0.0000 \n",
            "\n",
            " 3.84755463997523 minutes\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "txt_file = data_path +\"/dev_acc.txt\"\n",
        "with open(txt_file,'w') as f :\n",
        "    for epoch in range(1, epochs+1):\n",
        "        start = time.time()\n",
        "        batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "\n",
        "        num_correct = 0\n",
        "        total_loss = 0\n",
        "\n",
        "        # training samples\n",
        "        # model.train()\n",
        "        for i, (x, y) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x = x.cuda()\n",
        "            y = y.cuda()\n",
        "\n",
        "            # Don't be surprised - we just wrap these two lines to make it work for FP16\n",
        "            with torch.cuda.amp.autocast():     \n",
        "                outputs = model(x)\n",
        "                loss = criterion(outputs, y)\n",
        "\n",
        "            # Update # correct & loss as we go\n",
        "            num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n",
        "            total_loss += float(loss)\n",
        "\n",
        "            # tqdm lets you add some details so you can monitor training as you train.\n",
        "            batch_bar.set_postfix(\n",
        "                acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * batch_size)),\n",
        "                loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "                num_correct=num_correct,\n",
        "                lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "            \n",
        "            # Another couple things you need for FP16. \n",
        "            scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "            scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "            scaler.update() # This is something added just for FP16\n",
        "            \n",
        "            scheduler.step() # We told scheduler T_max that we'd call step() (len(train_loader) * epochs) many times.\n",
        "\n",
        "            batch_bar.update() # Update tqdm bar\n",
        "        batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "        path = data_path +\"/model_epoch_{}.txt\".format(epoch)\n",
        "        # print(\"The model is save to \",path)\n",
        "\n",
        "\n",
        "        torch.save({\n",
        "              'epoch': epoch,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(), \n",
        "              'scheduler': scheduler,\n",
        "          }, path)\n",
        "        \n",
        "\n",
        "        print_content =\"Epoch {}/{}: Train Acc {:.04f}%, Train Loss {:.04f}, Learning Rate {:.04f} \\n\".format(\n",
        "        epoch,\n",
        "        epochs,\n",
        "        100 * num_correct / (len(train_loader) * batch_size),\n",
        "        float(total_loss / len(train_loader)),\n",
        "        float(optimizer.param_groups[0]['lr']))\n",
        "        print(print_content)\n",
        "        f.write(print_content)\n",
        "\n",
        "\n",
        "\n",
        "        # # model validation process\n",
        "        # model.eval()\n",
        "        # batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "        # num_correct = 0\n",
        "        # for i, (x, y) in enumerate(val_loader):\n",
        "\n",
        "        #     x = x.cuda()\n",
        "        #     y = y.cuda()\n",
        "\n",
        "        #     with torch.no_grad():\n",
        "        #         outputs = model(x)\n",
        "\n",
        "        #     num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n",
        "        #     batch_bar.set_postfix(acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * batch_size)))\n",
        "\n",
        "        #     batch_bar.update()\n",
        "            \n",
        "        # batch_bar.close()\n",
        "\n",
        "        # dev_acc = 100 * num_correct / (len(val_dataset)*batch_size)\n",
        "        # # scheduler.step(dev_acc) # used for the ReduceOnPlatue \n",
        "\n",
        "        # print_content = \"Epoch {}/{}: Train Acc {:.04f}%, Validation Acc {:.04f}%, Train Loss {:.04f}, Learning Rate {:.04f} \\n\".format(\n",
        "        #     epoch,\n",
        "        #     epochs,\n",
        "        #     100 * num_correct / (len(train_loader) * batch_size),\n",
        "        #     100 * num_correct / (len(val_dataset)*batch_size),\n",
        "        #     float(total_loss / len(train_loader)),\n",
        "        #     float(optimizer.param_groups[0]['lr']))\n",
        "        # print(print_content)\n",
        "        # f.write(print_content)\n",
        "\n",
        "        end = time.time()\n",
        "        print(\"\", (end - start)/60,\"minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPu38MRHiII5"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxVGSLlstYAo"
      },
      "source": [
        "# Classification Task: Resume the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfOB3Vnqtfi-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d3e011f-8a19-4558-8500-f45e3c11ce45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We are working on the directory: => /content/drive/hw2/store_checkpoints/ConvNextSelfImple_dropout_full_RandAug_plus_0.1_label_smoothing_CosineAnnealing_SGD_BatchNorm2d_lr0.1/model_epoch_150.txt\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# set up the resuming training epoch from specified number\n",
        "resume_epoch = 150\n",
        "epochs = 150\n",
        "\n",
        "# model = MobileNetV2()\n",
        "model = ConvNeXt().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader) * epochs))\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "\n",
        "# Reload the checkpoint\n",
        "checkpoint = torch.load(\"/content/drive/hw2/store_checkpoints/{}/model_epoch_{}.txt\".format(model_id,resume_epoch))\n",
        "\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "scheduler=checkpoint['scheduler']\n",
        "epoch = checkpoint['epoch']\n",
        "txt_file = data_path +\"/dev_acc.txt\"\n",
        "\n",
        "print(\"We are working on the directory: => \"+\"/content/drive/hw2/store_checkpoints/{}/model_epoch_{}.txt\".format(model_id,resume_epoch))\n",
        "\n",
        "with open(txt_file,'w') as f :\n",
        "    for epoch in range(resume_epoch+1, epochs+1):\n",
        "        start = time.time()\n",
        "        batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "\n",
        "        num_correct = 0\n",
        "        total_loss = 0\n",
        "\n",
        "        for i, (x, y) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x = x.cuda()\n",
        "            y = y.cuda()\n",
        "\n",
        "            # Don't be surprised - we just wrap these two lines to make it work for FP16\n",
        "            with torch.cuda.amp.autocast():     \n",
        "                outputs = model(x)\n",
        "                loss = criterion(outputs, y)\n",
        "\n",
        "            # Update # correct & loss as we go\n",
        "            num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n",
        "            total_loss += float(loss)\n",
        "\n",
        "            # tqdm lets you add some details so you can monitor training as you train.\n",
        "            batch_bar.set_postfix(\n",
        "                acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * batch_size)),\n",
        "                loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "                num_correct=num_correct,\n",
        "                lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "            \n",
        "            # Another couple things you need for FP16. \n",
        "            scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "            scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "            scaler.update() # This is something added just for FP16\n",
        "\n",
        "            scheduler.step() # We told scheduler T_max that we'd call step() (len(train_loader) * epochs) many times.\n",
        "\n",
        "            batch_bar.update() # Update tqdm bar\n",
        "        batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "        path = data_path +\"/model_epoch_{}.txt\".format(epoch)\n",
        "        # print(\"The model is save to \",path)\n",
        "\n",
        "\n",
        "        torch.save({\n",
        "              'epoch': epoch,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(), \n",
        "              'scheduler': scheduler,\n",
        "          }, path)\n",
        "        \n",
        "\n",
        "        print_content =\"Epoch {}/{}: Train Acc {:.04f}%, Train Loss {:.04f}, Learning Rate {:.04f} \\n\".format(\n",
        "        epoch,\n",
        "        epochs,\n",
        "        100 * num_correct / (len(train_loader) * batch_size),\n",
        "        float(total_loss / len(train_loader)),\n",
        "        float(optimizer.param_groups[0]['lr']))\n",
        "        print(print_content)\n",
        "        f.write(print_content)\n",
        "        end = time.time()\n",
        "        print(\"\", (end - start)/60,\"minutes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKb2iD_9gdpX"
      },
      "source": [
        "# Classification Task: Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "le1o-OVjfeN9",
        "outputId": "c9c079b1-fef3-4d83-847a-48c3b9f9d58c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                    "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: 88.7486%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "num_correct = 0\n",
        "for i, (x, y) in enumerate(val_loader):\n",
        "\n",
        "    x = x.cuda()\n",
        "    y = y.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(x)\n",
        "\n",
        "    num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n",
        "    batch_bar.set_postfix(acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * batch_size)))\n",
        "\n",
        "    batch_bar.update()\n",
        "    \n",
        "batch_bar.close()\n",
        "print(\"Validation: {:.04f}%\".format(100 * num_correct / len(val_dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpgCHImRkYQW"
      },
      "source": [
        "# Classification Task: Submit to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08Zv2AWFrfVP"
      },
      "outputs": [],
      "source": [
        "class ClassificationTestSet(Dataset):\n",
        "    # It's possible to load test set data using ImageFolder without making a custom class.\n",
        "    # See if you can think it through!\n",
        "\n",
        "    def __init__(self, data_dir, transforms):\n",
        "        self.data_dir = data_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # This one-liner basically generates a sorted list of full paths to each image in data_dir\n",
        "        self.img_paths = list(map(lambda fname: osp.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.transforms(Image.open(self.img_paths[idx]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "td_qvGwr16z0"
      },
      "outputs": [],
      "source": [
        "del train_loader\n",
        "del val_loader\n",
        "DATA_DIR = \"/content\"\n",
        "TEST_DIR = osp.join(DATA_DIR, \"classification/classification/test\")\n",
        "batch_size = 32\n",
        "test_dataset = ClassificationTestSet(TEST_DIR, ttf.Compose([ttf.ToTensor()]))\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                         drop_last=False, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-vaYr9hSp7Q",
        "outputId": "8d54285c-a4eb-440b-d249-142db43f88d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are loading files from  /content/drive/hw2/store_checkpoints/ConvNextSelfImple_dropout_full_RandAug_plus_0.1_label_smoothing_CosineAnnealing_SGD_BatchNorm2d_lr0.1/model_epoch_150.txt\n"
          ]
        }
      ],
      "source": [
        "# model = ConvNeXtRef()\n",
        "model = ConvNeXt()\n",
        "# model = ResNet50()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "epochs = 150\n",
        "path = data_path +\"/model_epoch_{}.txt\".format(epochs)\n",
        "print(\"You are loading files from \",path)\n",
        "checkpoint = torch.load(path)\n",
        "# model = ConvNeXtRef()\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "scheduler=checkpoint['scheduler']\n",
        "\n",
        "# epoch = checkpoint['epoch']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2WQEUjXkWvo",
        "outputId": "f27cef1c-3ec5-407d-c642-3ba584110a98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "model.eval()\n",
        "batch_bar = tqdm(total=len(test_loader), dynamic_ncols=True, position=0, leave=False, desc='Test')\n",
        "\n",
        "\n",
        "res = []\n",
        "for i, (x) in enumerate(test_loader):\n",
        "\n",
        "\n",
        "    (x) = (x).cuda()\n",
        "    model = model.cuda()\n",
        "  \n",
        "    output = model((x))\n",
        "    y_pred = torch.argmax(output, axis=1)\n",
        "    res.extend(y_pred.tolist())\n",
        "\n",
        "    batch_bar.update()\n",
        "    \n",
        "batch_bar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vob9a2-HkW_V"
      },
      "outputs": [],
      "source": [
        "with open(\"classification_early_submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,label\\n\")\n",
        "    for i in range(len(test_dataset)):\n",
        "        f.write(\"{},{}\\n\".format(str(i).zfill(6) + \".jpg\", res[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpxatBfT4jSQ",
        "outputId": "617a27aa-de25-4385-ef4e-0a632ec2b4a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r  0% 0.00/541k [00:00<?, ?B/s]\r100% 541k/541k [00:00<00:00, 2.67MB/s]\n",
            "Successfully submitted to Face Recognition"
          ]
        }
      ],
      "source": [
        "!kaggle competitions submit -c 11-785-s22-hw2p2-classification -f classification_early_submission.csv -m \"Message\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsJx1l1T4twC"
      },
      "source": [
        "# Verification Task: Triplet loss implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoBFFF8-Lpvj"
      },
      "source": [
        "There are 6K verification dev images, but 166K \"pairs\" for you to compare. So, it's much more efficient to compute the features for the 6K verification images, and just compare afterwards.\n",
        "\n",
        "This will be done by creating a dictionary mapping the image file names to the features. Then, you'll use this dictionary to compute the similarities for each pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZV-WsTi9LrVz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f651e8e9-09ab-4233-c7cc-f30b7aab8ae4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6000\n",
            "166801\n"
          ]
        }
      ],
      "source": [
        "!ls verification/verification/dev | wc -l\n",
        "!cat verification/verification/verification_dev.csv | wc -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1YtIwxuL7H0"
      },
      "outputs": [],
      "source": [
        "class TripletDataset(torchvision.datasets.VisionDataset):\n",
        "    def __init__(self, root, transform):\n",
        "        # For \"root\", note that you're making this dataset on top of the regular classification dataset.\n",
        "        self.dataset = torchvision.datasets.ImageFolder(root=root, transform=transform)\n",
        "\n",
        "        # map class indices to dataset image indices\n",
        "        self.classes_to_img_indices = [[] for _ in range(len(self.dataset.classes))]\n",
        "        for img_idx, (_, class_id) in enumerate(self.dataset.samples):\n",
        "            self.classes_to_img_indices[class_id].append(img_idx)\n",
        "\n",
        "        # VisionDataset attributes for display\n",
        "        self.root = root\n",
        "        self.length = len(\n",
        "            self.dataset.classes)  # pseudo length! Length of this dataset is 7000, *not* the actual # of images in the dataset. You can just increase the # of epochs you train for.\n",
        "        self.transforms = self.dataset.transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, anchor_class_idx):\n",
        "        \"\"\"Treat the given index as the anchor class and pick a triplet randomly\"\"\"\n",
        "        anchor_class = self.classes_to_img_indices[anchor_class_idx]\n",
        "        # choose positive pair (assuming each class has at least 2 images)\n",
        "        anchor, positive = np.random.choice(a=anchor_class, size=2, replace=False)\n",
        "        # choose negative image\n",
        "        # hint for further exploration: you can choose 2 negative images to make it a Quadruplet Loss\n",
        "\n",
        "        classes_to_choose_negative_class_from = list(range(self.length))\n",
        "        classes_to_choose_negative_class_from.pop(anchor_class_idx)  # TODO: What are we removing?\n",
        "        negative_class = self.classes_to_img_indices[np.random.choice(classes_to_choose_negative_class_from, size=1)[0]]\n",
        "        negative = np.random.choice(a=negative_class, size=1)[0]  # TODO: How do we get a sample from that negative class?\n",
        "\n",
        "\n",
        "        # self.dataset[idx] will return a tuple (image tensor, class label). You can use its outputs to train for classification alongside verification\n",
        "        # If you do not want to train for classification, you can use self.dataset[idx][0] to get the image tensor\n",
        "        return self.dataset[anchor][0], self.dataset[positive][0], self.dataset[negative][0]\n",
        "\n",
        "class TripletWrapper(nn.Module):\n",
        "    def __init__(self, network):\n",
        "        super().__init__()\n",
        "        self.network = network\n",
        "\n",
        "    def forward(self, anchor, posi, nega, imgs=None,return_feats= False):\n",
        "        if return_feats:\n",
        "            return self.network(imgs)\n",
        "        else:\n",
        "            return self.network(anchor), self.network(posi), self.network(nega)\n",
        "\n",
        "\n",
        "# Prepare the verificationDataest\n",
        "class VerificationDataset(Dataset):\n",
        "    def __init__(self, data_dir, transforms):\n",
        "        self.data_dir = data_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # This one-liner basically generates a sorted list of full paths to each image in data_dir\n",
        "        self.img_paths = list(map(lambda fname: osp.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # We return the image, as well as the path to that image (relative path)\n",
        "        return self.transforms(Image.open(self.img_paths[idx])), osp.relpath(self.img_paths[idx], self.data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98lmjm0S4tHR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1e9c87e-3549-436d-dfe1-b317d393c28e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are loading files from  /content/drive/hw2/store_checkpoints/ConvNextSelfImple_dropout_full_RandAug_plus_0.1_label_smoothing_CosineAnnealing_SGD_BatchNorm2d_lr0.1/model_epoch_150.txt\n"
          ]
        }
      ],
      "source": [
        "DATA_DIR = \"/content\"\n",
        "TRAIN_DIR = osp.join(DATA_DIR, \"classification/classification/train\") \n",
        "VAL_DIR = osp.join(DATA_DIR, \"classification/classification/dev\")\n",
        "lr = 1e-4\n",
        "batch_size = 32\n",
        "epochs =150\n",
        "\n",
        "# data augmentation\n",
        "transforms = ttf.Compose([\n",
        "    ttf.RandomHorizontalFlip(),\n",
        "    # ttf.AutoAugment(),\n",
        "    ttf.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset_triplet = TripletDataset(TRAIN_DIR,transforms)\n",
        "\n",
        "val_dataset_triplet = TripletDataset(VAL_DIR,transform=ttf.Compose([ttf.ToTensor()]))\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset_triplet, batch_size=batch_size,\n",
        "                          shuffle=True, drop_last=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset_triplet, batch_size=batch_size, shuffle=False,\n",
        "                        drop_last=True, num_workers=1)\n",
        "\n",
        "model = ConvNeXt().cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "\n",
        "# Reload the checkpoint\n",
        "data_path = \"/content/drive/hw2/store_checkpoints/ConvNextSelfImple_dropout_full_RandAug_plus_0.1_label_smoothing_CosineAnnealing_SGD_BatchNorm2d_lr0.1\"\n",
        "path = data_path +\"/model_epoch_{}.txt\".format(epochs)\n",
        "print(\"You are loading files from \",path)\n",
        "checkpoint = torch.load(path)\n",
        "\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "scheduler=checkpoint['scheduler']\n",
        "\n",
        "\n",
        "# New criterion\n",
        "criterion2 = nn.TripletMarginLoss()\n",
        "# New model\n",
        "model = TripletWrapper(model).cuda()\n",
        "\n",
        "\n",
        "DATA_DIR = \"/content\"\n",
        "val_veri_dataset = VerificationDataset(osp.join(DATA_DIR, \"verification/verification/dev\"),\n",
        "                                       ttf.Compose([ttf.ToTensor()]))\n",
        "val_ver_loader = torch.utils.data.DataLoader(val_veri_dataset, batch_size=batch_size, \n",
        "                                             shuffle=False, num_workers=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    # Quality of life tip: leave=False and position=0 are needed to make tqdm usable in jupyter\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "\n",
        "    num_correct = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    ##########################################################################\n",
        "    # model training\n",
        "    ##########################################################################\n",
        "    model.train()\n",
        "    for i, (x, y, z) in enumerate(train_loader):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "        z = z.cuda()\n",
        "        \n",
        "        # Don't be surprised - we just wrap these two lines to make it work for FP16\n",
        "        with torch.cuda.amp.autocast():     \n",
        "            anchor, pos, neg = model(x,y,z)\n",
        "            \n",
        "            # loss1 = criterion(pos, neg) #crossentropy output/actual\n",
        "            loss2 = criterion2(anchor, pos, neg)\n",
        "            loss1 = 0\n",
        "            loss = loss1 + loss2\n",
        "\n",
        "        # Update # correct & loss as we go\n",
        "        # num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n",
        "        total_loss += float(loss)\n",
        "        \n",
        "        # tqdm lets you add some details so you can monitor training as you train.\n",
        "        batch_bar.set_postfix(\n",
        "            # acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * batch_size)),\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            # num_correct=num_correct,\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "        \n",
        "        # Another couple things you need for FP16. \n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update() # This is something added just for FP16\n",
        "\n",
        "        scheduler.step() # We told scheduler T_max that we'd call step() (len(train_loader) * epochs) many times.\n",
        "\n",
        "        batch_bar.update() # Update tqdm bar\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "    # You can add validation per-epoch here if you would like\n",
        "\n",
        "    print(\"Epoch {}/{}: Train Loss {:.10f}, Learning Rate {:.10f}\".format(\n",
        "        epoch + 1,\n",
        "        epochs,\n",
        "        float(total_loss / len(train_loader)),\n",
        "        float(optimizer.param_groups[0]['lr'])))\n",
        "    \n",
        "    ##########################################################################\n",
        "    # model evaluation\n",
        "    ##########################################################################\n",
        "\n",
        "    model.eval()\n",
        "    feats_dict = dict()\n",
        "    for batch_idx, (imgs, path_names) in tqdm(enumerate(val_ver_loader), total=len(val_ver_loader), position=0, leave=False):\n",
        "        imgs = imgs.cuda()\n",
        "        model.cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "          # obtain the features\n",
        "            feats = model(_,_,_,imgs, return_feats=True) \n",
        "        \n",
        "\n",
        "        for i in range(len(path_names)):\n",
        "          pn = 'dev/'+path_names[i]\n",
        "          ft = feats[i]\n",
        "          feats_dict.update({pn:ft})\n",
        "    \n",
        "    val_veri_csv = osp.join(DATA_DIR, \"verification/verification/verification_dev.csv\")\n",
        "\n",
        "\n",
        "    # Now, loop through the csv and compare each pair, getting the similarity between them\n",
        "    pred_similarities = []\n",
        "    gt_similarities = []\n",
        "    for line in tqdm(open(val_veri_csv).read().splitlines()[1:], position=0, leave=False): # skip header\n",
        "        img_path1, img_path2, gt = line.split(\",\")\n",
        "\n",
        "        img1 = feats_dict[img_path1]\n",
        "        img2 = feats_dict[img_path2]\n",
        "        similarity = F.cosine_similarity(img1,img2,dim=0).to(\"cpu\").numpy()\n",
        "\n",
        "        pred_similarities.append(similarity)\n",
        "        gt_similarities.append(int(gt))\n",
        "\n",
        "    pred_similarities = np.array(pred_similarities).flatten()\n",
        "    gt_similarities = np.array(gt_similarities)\n",
        "    auc =roc_auc_score(gt_similarities, pred_similarities)\n",
        "    print(auc)\n",
        "\n",
        "    path = data_path +f\"/verification_{lr}_margin1_model_epoch_{epoch}_auc{auc}.txt\"\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(), \n",
        "        'scheduler': scheduler,\n",
        "    }, path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRlrm1zG5jQJ",
        "outputId": "f0c1756a-a3db-47fa-d6cb-083d6fe0eff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20: Train Loss 0.2430992990, Learning Rate 0.0001000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.914358905767369\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20: Train Loss 0.1386141033, Learning Rate 0.0001000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9369342838043428\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20: Train Loss 0.0937704865, Learning Rate 0.0001000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9416851006753952\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20: Train Loss 0.1086166364, Learning Rate 0.0001000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9500457032870776\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20: Train Loss 0.0768240964, Learning Rate 0.0001000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9494912846918075\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20: Train Loss 0.0909866040, Learning Rate 0.0001000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9509309137430271\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20: Train Loss 0.0906642161, Learning Rate 0.0001000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9542327033677562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20: Train Loss 0.1026388286, Learning Rate 0.0001000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9476438197293808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20: Train Loss 0.0766102349, Learning Rate 0.0001000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9534510917546448\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/20: Train Loss 0.0647040945, Learning Rate 0.0001000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9556232504264442\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20: Train Loss 0.0654990279, Learning Rate 0.0001000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9576836605619843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20: Train Loss 0.0670144656, Learning Rate 0.0001000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9574625884583468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20: Train Loss 0.0532646278, Learning Rate 0.0001000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.956819850398783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/20: Train Loss 0.0680741750, Learning Rate 0.0001000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9597733135286525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/20: Train Loss 0.0670101457, Learning Rate 0.0001000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9569697887372642\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/20: Train Loss 0.0689347086, Learning Rate 0.0001000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9582438534415195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/20: Train Loss 0.0634206763, Learning Rate 0.0001000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9603718299064128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/20: Train Loss 0.0560014587, Learning Rate 0.0001000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9619034988589739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/20: Train Loss 0.0675913609, Learning Rate 0.0001000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9566596725024203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/20: Train Loss 0.0558926490, Learning Rate 0.0001000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.959605720690609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sakRa8oZOlKr"
      },
      "source": [
        "# Verification Task: Submit to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload the checkpoint\n",
        "# epoch =20\n",
        "# data_path = \"/content/drive/hw2/store_checkpoints/ConvNextRef_full_RandAug_plus_0.1_label_smoothing_CosineAnnealing_SGD_BatchNorm2d_lr0.1\"\n",
        "# path = data_path +f\"/verification_{lr}_margin0.05_model_epoch_{epoch}.txt\"\n",
        "path = \"/content/drive/hw2/store_checkpoints/ConvNextSelfImple_dropout_full_RandAug_plus_0.1_label_smoothing_CosineAnnealing_SGD_BatchNorm2d_lr0.1/verification_0.0001_margin1_model_epoch_17_auc0.9619034988589739.txt\"\n",
        "checkpoint = torch.load(path)\n",
        "\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "scheduler=checkpoint['scheduler']\n",
        "epoch = checkpoint['epoch']\n"
      ],
      "metadata": {
        "id": "VwEEn9K5_Nqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDK3knDcOrOE"
      },
      "outputs": [],
      "source": [
        "val_transforms = [ttf.ToTensor()]\n",
        "test_veri_dataset = VerificationDataset(osp.join(DATA_DIR, \"verification/verification/test\"),\n",
        "                                        ttf.Compose(val_transforms))\n",
        "test_ver_loader = torch.utils.data.DataLoader(test_veri_dataset, batch_size=batch_size, \n",
        "                                              shuffle=False, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igeRT3WxOrB_",
        "outputId": "6542a74c-d7aa-4fde-b0b9-ce325ca854f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "feats_dict = dict()\n",
        "for batch_idx, (imgs, path_names) in tqdm(enumerate(test_ver_loader), total=len(test_ver_loader), position=0, leave=False):\n",
        "    imgs = imgs.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Note that we return the feats here, not the final outputs\n",
        "        # Feel free to try to final outputs too!\n",
        "        feats = model(_,_,_,imgs, return_feats=True) \n",
        "    \n",
        "    # TODO: Now we have features and the image path names. What to do with them?\n",
        "    # Hint: use the feats_dict somehow.\n",
        "    for i in range(len(path_names)):\n",
        "      pn = 'test/'+path_names[i]\n",
        "      ft = feats[i]\n",
        "      feats_dict.update({pn:ft})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We use cosine similarity between feature embeddings.\n",
        "# TODO: Find the relevant function in pytorch and read its documentation.\n",
        "# similarity_metric = \n",
        "val_veri_csv = osp.join(DATA_DIR, \"verification/verification/verification_test.csv\")\n",
        "\n",
        "\n",
        "# Now, loop through the csv and compare each pair, getting the similarity between them\n",
        "pred_similarities = []\n",
        "for line in tqdm(open(val_veri_csv).read().splitlines()[1:], position=0, leave=False): # skip header\n",
        "    img_path1, img_path2 = line.split(\",\")\n",
        "    \n",
        "    img1 = feats_dict[img_path1]\n",
        "    img2 = feats_dict[img_path2]\n",
        "    similarity = F.cosine_similarity(img1,img2,dim=0).to(\"cpu\").numpy().flatten()\n",
        "\n",
        "    pred_similarities.append(similarity)\n",
        "    # TODO: Finish up verification testing.\n",
        "    # How to use these img_paths? What to do with the features?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfuBpwC__vcS",
        "outputId": "1d137598-5978-4c89-ccef-3d403e5d486e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYXiglWkPBDv"
      },
      "outputs": [],
      "source": [
        "with open(\"verification_early_submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,match\\n\")\n",
        "    for i in range(len(pred_similarities)):\n",
        "        f.write(\"{},{}\\n\".format(i, pred_similarities[i][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5zB7P8O687N",
        "outputId": "75900554-b282-4081-c308-1b7de7d0a9eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 16.4M/16.4M [00:00<00:00, 39.7MB/s]\n",
            "Successfully submitted to Face Verification"
          ]
        }
      ],
      "source": [
        "!kaggle competitions submit -c 11-785-s22-hw2p2-verification -f verification_early_submission.csv -m \"Message\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FCemyIldqimj",
        "cBTLCyocZBGS",
        "mIqmojPaWD0H",
        "zZI9Ejh4bOfa"
      ],
      "name": "hw2P2_README.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}