{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykh8vQc0p71n"
      },
      "source": [
        "# 11785 HW3P2: Automatic Speech Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# README How to run this notebook\n"
      ],
      "metadata": {
        "id": "aUFNe-hT6KZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The whole jupyter notebook could be separated into several parts, including necessary set up or configuration, dataset configuration, Levenshtein distance calculation, model training and validation and the final submission towards kaggle. \n",
        "\n",
        "For the Preliminaries and Libraries import section, we check the type of distributed GPU and kaggle data download and set up. The following train configuration section would set up the account information of wandb and hyperparameter initlization for following training.\n",
        "\n",
        "The next main part is to build data loader and possible normalization to prepare for the following training. Specifically, cepstral normalizaiton would be applied in this part. \n",
        "\n",
        "The next chunk is the core of this notebook. It specifies how I constructed the model. I firstly use the block used in ConvNext to do the embedding, but the expansion was reduced to 2 for calculation efficiency. Then followed by the 4 layer bidirectional lstm layer with the following classifcation layer. Layer initilization is also included in this part. \n",
        "\n",
        "Then, it comes to the optimizer set up and Levenshtein distance calculation stuff. With these all set, it finally goes to the training and evaluation part, which would integrate the previous function together and do the training and validating tasks. Additionally, the resume/load model chuncks are used to load the previous saved model for continuous training. \n",
        "\n",
        "Finally, I would make prediction on the test datasets and submit to kaggle to check the results."
      ],
      "metadata": {
        "id": "7FvVmWv56Ren"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_fwJWcpqJDR"
      },
      "source": [
        "# Prelimilaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leymyQ-apwT6"
      },
      "source": [
        "You will need to install packages for decoding and calculating the Levenshtein distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXb-V4AgJvuG",
        "outputId": "85bb0579-8193-4d90-ce83-17d745f5923c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr  6 01:06:06 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    23W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1E1pcIQOSueA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import json\n",
        "\n",
        "TOKEN = {\"username\":\"yuxuanwucmu\",\"key\":\"81ded4babfd327efdc7655be369299b5\"}\n",
        "\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "! mkdir -p .kaggle\n",
        "! mkdir -p /content & mkdir -p /content/.kaggle & mkdir -p /root/.kaggle/\n",
        "\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(TOKEN, file)\n",
        "\n",
        "! pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "! ls \"/content/.kaggle\"\n",
        "! chmod 600 /content/.kaggle/kaggle.json\n",
        "! cp /content/.kaggle/kaggle.json /root/.kaggle/\n",
        "\n",
        "! kaggle config set -n path -v /content/drive/MyDrive/\n",
        "\n",
        "!kaggle competitions download -c 11-785-s22-hw3p2\n",
        "\n",
        "!unzip -q /content/drive/MyDrive/competitions/11-785-s22-hw3p2/11-785-s22-hw3p2.zip\n",
        "!pip install wandb\n",
        "!pip install python-Levenshtein\n",
        "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "!pip install wget\n",
        "!pip install wandb\n",
        "%cd ctcdecode\n",
        "!pip install .\n",
        "%cd ..\n",
        "\n",
        "!pip install torchsummaryX # We also install a summary package to check our model's forward before training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4vZbDmJvMp1"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI4qfx7tiBZt",
        "outputId": "e643f777-238e-447e-a688-57a45eca18fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.5.4)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.5)\n",
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "import csv\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "!pip install timm\n",
        "\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "# imports for decoding and distance calculation\n",
        "import ctcdecode\n",
        "import Levenshtein\n",
        "from ctcdecode import CTCBeamDecoder\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def create_folder(folder_path):\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.mkdir(folder_path)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)\n",
        "# PHONEME_MAP is the list that maps the phoneme to a single character. \n",
        "# The dataset contains a list of phonemes but you need to map them to their corresponding characters to calculate the Levenshtein Distance\n",
        "# You final submission should not have the phonemes but the mapped string\n",
        "# No TODOs in this cell\n",
        "\n",
        "PHONEME_MAP = [\n",
        "    \" \",\n",
        "    \".\", #SIL\n",
        "    \"a\", #AA\n",
        "    \"A\", #AE\n",
        "    \"h\", #AH\n",
        "    \"o\", #AO\n",
        "    \"w\", #AW\n",
        "    \"y\", #AY\n",
        "    \"b\", #B\n",
        "    \"c\", #CH\n",
        "    \"d\", #D\n",
        "    \"D\", #DH\n",
        "    \"e\", #EH\n",
        "    \"r\", #ER\n",
        "    \"E\", #EY\n",
        "    \"f\", #F\n",
        "    \"g\", #G\n",
        "    \"H\", #H\n",
        "    \"i\", #IH \n",
        "    \"I\", #IY\n",
        "    \"j\", #JH\n",
        "    \"k\", #K\n",
        "    \"l\", #L\n",
        "    \"m\", #M\n",
        "    \"n\", #N\n",
        "    \"N\", #NG\n",
        "    \"O\", #OW\n",
        "    \"Y\", #OY\n",
        "    \"p\", #P \n",
        "    \"R\", #R\n",
        "    \"s\", #S\n",
        "    \"S\", #SH\n",
        "    \"t\", #T\n",
        "    \"T\", #TH\n",
        "    \"u\", #UH\n",
        "    \"U\", #UW\n",
        "    \"v\", #V\n",
        "    \"W\", #W\n",
        "    \"?\", #Y\n",
        "    \"z\", #Z\n",
        "    \"Z\" #ZH\n",
        "]\n",
        "\n",
        "PHONEMES = [\"\", 'SIL',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',  \n",
        "            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n",
        "            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n",
        "            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n",
        "            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n",
        "            'V',     'W',     'Y',     'Z',     'ZH']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8QvliqdbEyw"
      },
      "source": [
        "# Train configuration"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size = 256\n",
        "lr = 2e-3\n",
        "epochs = 150\n",
        "batch_size = 64\n",
        "model = \"LSTM_Embedding_layerPlus2_expansion2_block2\"\n",
        "scheduler = \"ReduceOnPlateau\"\n",
        "beam_width = \"50\"\n",
        "drop_out = 0.5\n",
        "stride = \"stride2\"\n",
        "normalization = \"Cepstral\"\n",
        "init =\"kaiming\"\n",
        "embedding = \"ConvNext\"\n",
        "checkpoints_store_path = \"/content/drive/MyDrive/HW3/store_checkpoints/\"\n",
        "create_folder(checkpoints_store_path)\n",
        "\n",
        "masking = \"no\"\n",
        "model_id = f\"{model}_{scheduler}_2Cls_dropout{drop_out}_{stride}_{normalization}_{init}_{embedding}_{masking}\"\n",
        "data_path = checkpoints_store_path + model_id\n",
        "create_folder(data_path)\n",
        "\n",
        "wandb.init(project=\"11785-HW3\", entity=\"saltedfish\",name=model_id)\n",
        "wandb.config = {\n",
        "  \"learning_rate\": lr,\n",
        "  \"epochs\": epochs,\n",
        "  \"batch_size\": batch_size,\n",
        "  \"drop_out\":drop_out\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvGGj8ZTQUbd",
        "outputId": "cf322cc1-e2f9-455a-92c1-c87f24ea65bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m11785_sematicsup\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.12"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220406_173603-31gl6vcb</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/saltedfish/11785-HW3/runs/31gl6vcb\" target=\"_blank\">LSTM_Embedding_layerPlus2_expansion2_block2_ReduceOnPlateau_2Cls_dropout0.5_stride2_Cepstral_kaiming_ConvNext_no</a></strong> to <a href=\"https://wandb.ai/saltedfish/11785-HW3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUCKqm1ST1sU"
      },
      "source": [
        "# Dataset and data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SndiVRVqBMa"
      },
      "outputs": [],
      "source": [
        "# This cell is where your actual TODOs start\n",
        "# You will need to implement the Dataset class by your own. You may also implement it similar to HW1P2 (dont require context)\n",
        "# The steps for implementation given below are how we have implemented it.\n",
        "# However, you are welcomed to do it your own way if it is more comfortable or efficient. \n",
        "\n",
        "class LibriSamples(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data_path, partition=\"train\"):  # You can use partition to specify train or dev\n",
        "\n",
        "        self.X_dir = data_path + \"/\" + partition + \"/mfcc/\"\n",
        "        self.Y_dir = data_path + \"/\" + partition + \"/transcript/\"\n",
        "\n",
        "        self.X_files = os.listdir(self.X_dir)\n",
        "        self.Y_files = os.listdir(self.Y_dir)\n",
        "\n",
        "        # store PHONEMES from phonemes.py inside the class. phonemes.py will be downloaded from kaggle.\n",
        "        # You may wish to store PHONEMES as a class attribute or a global variable as well.\n",
        "        self.PHONEMES = PHONEMES\n",
        "\n",
        "        assert (len(self.X_files) == len(self.Y_files))  # check the length between data and label is the same\n",
        "\n",
        "        pass\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X_files)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        X = np.load(\n",
        "            self.X_dir + self.X_files[ind])  # Each X has a shape of (frames, time step, 13)TODO: Load the mfcc npy file at the specified index ind in the directory\n",
        "        Y = [self.PHONEMES.index(yy) for yy in\n",
        "             np.load(self.Y_dir + self.X_files[ind])[1:-1]]  # TODO: Load the corresponding transcripts\n",
        "        \n",
        "        X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "\n",
        "        # Remember, the transcripts are a sequence of phonemes. Eg. np.array(['<sos>', 'B', 'IH', 'K', 'SH', 'AA', '<eos>'])\n",
        "        # You need to convert these into a sequence of Long tensors\n",
        "        # Tip: You may need to use self.PHONEMES\n",
        "        # Remember, PHONEMES or PHONEME_MAP do not have '<sos>' or '<eos>' but the transcripts have them.\n",
        "        # You need to remove '<sos>' and '<eos>' from the trancripts.\n",
        "        # Inefficient way is to use a for loop for this. Efficient way is to think that '<sos>' occurs at the start and '<eos>' occurs at the end.\n",
        "        Yy = torch.LongTensor(Y) # TODO: Convert sequence of  phonemes into sequence of Long tensors\n",
        "\n",
        "        return X, Yy\n",
        "\n",
        "    def collate_fn(batch):\n",
        "      \n",
        "        descending = sorted(batch, key=lambda x: x[0].shape[0], reverse=True) \n",
        "        batch_x = [x for x, y in descending]\n",
        "        batch_y = [y for x, y in descending]\n",
        "\n",
        "\n",
        "        batch_x_pad = pad_sequence(\n",
        "            [torch.Tensor(x) for x in batch_x])  # TODO: pad the sequence with pad_sequence (already imported)\n",
        "        lengths_x = torch.LongTensor(\n",
        "            [len(x) for x in batch_x])  # TODO: Get original lengths of the sequence before padding\n",
        "\n",
        "        batch_y_pad = pad_sequence(batch_y)  # TODO: pad the sequence with pad_sequence (already imported)\n",
        "        lengths_y = torch.LongTensor(\n",
        "            [len(y) for y in batch_y])   # TODO: Get original lengths of the sequence before padding\n",
        "\n",
        "        return batch_x_pad, batch_y_pad, torch.tensor(lengths_x), torch.tensor(lengths_y)\n",
        "\n",
        "\n",
        "\n",
        "class LibriSamplesTest(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data_path, test_order):  # test_order is the csv similar to what you used in hw1\n",
        "        file_path = data_path + '/test/' + test_order\n",
        "        self.X_dir = data_path + \"/test/mfcc/\"\n",
        "\n",
        "        test_order_list = self.parse_csv(file_path)  # TODO: open test_order.csv as a list\n",
        "        self.X = [i for i in test_order_list]  # TODO: Load the npy files from test_order.csv and append into a list\n",
        "        # You can load the files here or save the paths here and load inside __getitem__ like the previous class\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        # TODOs: Need to return only X because this is the test dataset\n",
        "        X = np.load(self.X_dir+self.X[ind][0])\n",
        "        X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "        return X\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        # descending = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)\n",
        "        # batch_x = [x for x in descending]\n",
        "        batch_x_pad = pad_sequence(\n",
        "            [torch.Tensor(x) for x in batch])  # TODO: pad the sequence with pad_sequence (already imported)\n",
        "        lengths_x = torch.LongTensor(\n",
        "            [len(x) for x in batch])  # TODO: Get original lengths of the sequence before padding\n",
        "\n",
        "        return batch_x_pad, torch.tensor(lengths_x)\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_csv(filepath):\n",
        "        subset = []\n",
        "        with open(filepath) as f:\n",
        "            f_csv = csv.reader(f)\n",
        "            for row in f_csv:\n",
        "                subset.append(row)\n",
        "        return subset[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mzoYfTKu14s",
        "outputId": "b64231cd-5050-4db2-f31b-1a57141db062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size:  64\n",
            "Train dataset samples = 28539, batches = 446\n",
            "Val dataset samples = 2703, batches = 43\n",
            "Test dataset samples = 2620, batches = 41\n"
          ]
        }
      ],
      "source": [
        "root = \"/content/hw3p2_student_data/hw3p2_student_data\"\n",
        "\n",
        "train_data = LibriSamples(root, 'train')\n",
        "val_data = LibriSamples(root, 'dev')\n",
        "test_data = LibriSamplesTest(root, 'test_order.csv')\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True,\n",
        "                                           collate_fn=LibriSamples.collate_fn)  # TODO: Define the train loader. Remember to pass in a parameter (function) for the collate_fn argument\n",
        "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True,\n",
        "                                         collate_fn=LibriSamples.collate_fn)  # TODO: Define the val loader. Remember to pass in a parameter (function) for the collate_fn argument\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False,\n",
        "                                          collate_fn=LibriSamplesTest.collate_fn)  # TODO: Define the test loader. Remember to pass in a parameter (function) for the collate_fn argument\n",
        "\n",
        "print(\"Batch size: \", batch_size)\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9FwVZ9I2da0",
        "outputId": "3ff52bbf-c341-4055-bdd3-07adbe569ff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1746, 64, 13]) torch.Size([209, 64]) torch.Size([64]) torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "# Optional\n",
        "# Test code for checking shapes and return arguments of the train and val loaders\n",
        "for data in val_loader:\n",
        "    x, y, lx, ly = data # if you face an error saying \"Cannot unpack\", then you are not passing the collate_fn argument\n",
        "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nuOCw_nwoMaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly4mjUUUuJhy"
      },
      "source": [
        "# Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CGoiXd70tb5z",
        "outputId": "5ac770b3-b577-4429-aab5-2079528cccb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network(\n",
            "  (embedding): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Conv1d(13, 128, kernel_size=(5,), stride=(2,), padding=(2,))\n",
            "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Block(\n",
            "        (spatial_mixing): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), groups=128)\n",
            "        (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (feature_mixing): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
            "        (activation): GELU()\n",
            "        (bottleneck_channels): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
            "        (drop_path): DropPath()\n",
            "      )\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (lstm): LSTM(256, 512, num_layers=4, dropout=0.4, bidirectional=True)\n",
            "  (classification): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "    (1): GELU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "    (4): GELU()\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=2048, out_features=41, bias=True)\n",
            "  )\n",
            ")\n",
            "==================================================================================================\n",
            "                                             Kernel Shape     Output Shape  \\\n",
            "Layer                                                                        \n",
            "0_embedding.0.Conv1d_0                       [13, 128, 5]   [64, 128, 873]   \n",
            "1_embedding.0.BatchNorm1d_1                         [128]   [64, 128, 873]   \n",
            "2_embedding.1.0.Conv1d_spatial_mixing         [1, 128, 7]   [64, 128, 873]   \n",
            "3_embedding.1.0.BatchNorm1d_norm                    [128]   [64, 128, 873]   \n",
            "4_embedding.1.0.Conv1d_feature_mixing       [128, 256, 1]   [64, 256, 873]   \n",
            "5_embedding.1.0.GELU_activation                         -   [64, 256, 873]   \n",
            "6_embedding.1.0.Conv1d_bottleneck_channels  [256, 128, 1]   [64, 128, 873]   \n",
            "7_embedding.1.0.DropPath_drop_path                      -   [64, 128, 873]   \n",
            "8_embedding.2.Conv1d_0                      [128, 256, 3]   [64, 256, 873]   \n",
            "9_embedding.2.BatchNorm1d_1                         [256]   [64, 256, 873]   \n",
            "10_lstm                                                 -    [23557, 1024]   \n",
            "11_classification.Linear_0                   [1024, 2048]  [873, 64, 2048]   \n",
            "12_classification.GELU_1                                -  [873, 64, 2048]   \n",
            "13_classification.Dropout_2                             -  [873, 64, 2048]   \n",
            "14_classification.Linear_3                   [2048, 2048]  [873, 64, 2048]   \n",
            "15_classification.GELU_4                                -  [873, 64, 2048]   \n",
            "16_classification.Dropout_5                             -  [873, 64, 2048]   \n",
            "17_classification.Linear_6                     [2048, 41]    [873, 64, 41]   \n",
            "\n",
            "                                                Params   Mult-Adds  \n",
            "Layer                                                               \n",
            "0_embedding.0.Conv1d_0                          8.448k    7.26336M  \n",
            "1_embedding.0.BatchNorm1d_1                      256.0       128.0  \n",
            "2_embedding.1.0.Conv1d_spatial_mixing           1.024k    782.208k  \n",
            "3_embedding.1.0.BatchNorm1d_norm                 256.0       128.0  \n",
            "4_embedding.1.0.Conv1d_feature_mixing          33.024k  28.606464M  \n",
            "5_embedding.1.0.GELU_activation                      -           -  \n",
            "6_embedding.1.0.Conv1d_bottleneck_channels     32.896k  28.606464M  \n",
            "7_embedding.1.0.DropPath_drop_path                   -           -  \n",
            "8_embedding.2.Conv1d_0                          98.56k  85.819392M  \n",
            "9_embedding.2.BatchNorm1d_1                      512.0       256.0  \n",
            "10_lstm                                     22.052864M  22.020096M  \n",
            "11_classification.Linear_0                     2.0992M   2.097152M  \n",
            "12_classification.GELU_1                             -           -  \n",
            "13_classification.Dropout_2                          -           -  \n",
            "14_classification.Linear_3                   4.196352M   4.194304M  \n",
            "15_classification.GELU_4                             -           -  \n",
            "16_classification.Dropout_5                          -           -  \n",
            "17_classification.Linear_6                     84.009k     83.968k  \n",
            "--------------------------------------------------------------------------------------------------\n",
            "                          Totals\n",
            "Total params          28.607401M\n",
            "Trainable params      28.607401M\n",
            "Non-trainable params         0.0\n",
            "Mult-Adds             179.47392M\n",
            "==================================================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             Kernel Shape     Output Shape  \\\n",
              "Layer                                                                        \n",
              "0_embedding.0.Conv1d_0                       [13, 128, 5]   [64, 128, 873]   \n",
              "1_embedding.0.BatchNorm1d_1                         [128]   [64, 128, 873]   \n",
              "2_embedding.1.0.Conv1d_spatial_mixing         [1, 128, 7]   [64, 128, 873]   \n",
              "3_embedding.1.0.BatchNorm1d_norm                    [128]   [64, 128, 873]   \n",
              "4_embedding.1.0.Conv1d_feature_mixing       [128, 256, 1]   [64, 256, 873]   \n",
              "5_embedding.1.0.GELU_activation                         -   [64, 256, 873]   \n",
              "6_embedding.1.0.Conv1d_bottleneck_channels  [256, 128, 1]   [64, 128, 873]   \n",
              "7_embedding.1.0.DropPath_drop_path                      -   [64, 128, 873]   \n",
              "8_embedding.2.Conv1d_0                      [128, 256, 3]   [64, 256, 873]   \n",
              "9_embedding.2.BatchNorm1d_1                         [256]   [64, 256, 873]   \n",
              "10_lstm                                                 -    [23557, 1024]   \n",
              "11_classification.Linear_0                   [1024, 2048]  [873, 64, 2048]   \n",
              "12_classification.GELU_1                                -  [873, 64, 2048]   \n",
              "13_classification.Dropout_2                             -  [873, 64, 2048]   \n",
              "14_classification.Linear_3                   [2048, 2048]  [873, 64, 2048]   \n",
              "15_classification.GELU_4                                -  [873, 64, 2048]   \n",
              "16_classification.Dropout_5                             -  [873, 64, 2048]   \n",
              "17_classification.Linear_6                     [2048, 41]    [873, 64, 41]   \n",
              "\n",
              "                                                Params   Mult-Adds  \n",
              "Layer                                                               \n",
              "0_embedding.0.Conv1d_0                          8448.0   7263360.0  \n",
              "1_embedding.0.BatchNorm1d_1                      256.0       128.0  \n",
              "2_embedding.1.0.Conv1d_spatial_mixing           1024.0    782208.0  \n",
              "3_embedding.1.0.BatchNorm1d_norm                 256.0       128.0  \n",
              "4_embedding.1.0.Conv1d_feature_mixing          33024.0  28606464.0  \n",
              "5_embedding.1.0.GELU_activation                    NaN         NaN  \n",
              "6_embedding.1.0.Conv1d_bottleneck_channels     32896.0  28606464.0  \n",
              "7_embedding.1.0.DropPath_drop_path                 NaN         NaN  \n",
              "8_embedding.2.Conv1d_0                         98560.0  85819392.0  \n",
              "9_embedding.2.BatchNorm1d_1                      512.0       256.0  \n",
              "10_lstm                                     22052864.0  22020096.0  \n",
              "11_classification.Linear_0                   2099200.0   2097152.0  \n",
              "12_classification.GELU_1                           NaN         NaN  \n",
              "13_classification.Dropout_2                        NaN         NaN  \n",
              "14_classification.Linear_3                   4196352.0   4194304.0  \n",
              "15_classification.GELU_4                           NaN         NaN  \n",
              "16_classification.Dropout_5                        NaN         NaN  \n",
              "17_classification.Linear_6                     84009.0     83968.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9d94b05f-41eb-453d-abb9-6f4890e4e3cb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_embedding.0.Conv1d_0</th>\n",
              "      <td>[13, 128, 5]</td>\n",
              "      <td>[64, 128, 873]</td>\n",
              "      <td>8448.0</td>\n",
              "      <td>7263360.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_embedding.0.BatchNorm1d_1</th>\n",
              "      <td>[128]</td>\n",
              "      <td>[64, 128, 873]</td>\n",
              "      <td>256.0</td>\n",
              "      <td>128.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_embedding.1.0.Conv1d_spatial_mixing</th>\n",
              "      <td>[1, 128, 7]</td>\n",
              "      <td>[64, 128, 873]</td>\n",
              "      <td>1024.0</td>\n",
              "      <td>782208.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_embedding.1.0.BatchNorm1d_norm</th>\n",
              "      <td>[128]</td>\n",
              "      <td>[64, 128, 873]</td>\n",
              "      <td>256.0</td>\n",
              "      <td>128.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_embedding.1.0.Conv1d_feature_mixing</th>\n",
              "      <td>[128, 256, 1]</td>\n",
              "      <td>[64, 256, 873]</td>\n",
              "      <td>33024.0</td>\n",
              "      <td>28606464.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5_embedding.1.0.GELU_activation</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 256, 873]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6_embedding.1.0.Conv1d_bottleneck_channels</th>\n",
              "      <td>[256, 128, 1]</td>\n",
              "      <td>[64, 128, 873]</td>\n",
              "      <td>32896.0</td>\n",
              "      <td>28606464.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7_embedding.1.0.DropPath_drop_path</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 128, 873]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8_embedding.2.Conv1d_0</th>\n",
              "      <td>[128, 256, 3]</td>\n",
              "      <td>[64, 256, 873]</td>\n",
              "      <td>98560.0</td>\n",
              "      <td>85819392.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_embedding.2.BatchNorm1d_1</th>\n",
              "      <td>[256]</td>\n",
              "      <td>[64, 256, 873]</td>\n",
              "      <td>512.0</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10_lstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[23557, 1024]</td>\n",
              "      <td>22052864.0</td>\n",
              "      <td>22020096.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11_classification.Linear_0</th>\n",
              "      <td>[1024, 2048]</td>\n",
              "      <td>[873, 64, 2048]</td>\n",
              "      <td>2099200.0</td>\n",
              "      <td>2097152.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12_classification.GELU_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[873, 64, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13_classification.Dropout_2</th>\n",
              "      <td>-</td>\n",
              "      <td>[873, 64, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14_classification.Linear_3</th>\n",
              "      <td>[2048, 2048]</td>\n",
              "      <td>[873, 64, 2048]</td>\n",
              "      <td>4196352.0</td>\n",
              "      <td>4194304.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15_classification.GELU_4</th>\n",
              "      <td>-</td>\n",
              "      <td>[873, 64, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16_classification.Dropout_5</th>\n",
              "      <td>-</td>\n",
              "      <td>[873, 64, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17_classification.Linear_6</th>\n",
              "      <td>[2048, 41]</td>\n",
              "      <td>[873, 64, 41]</td>\n",
              "      <td>84009.0</td>\n",
              "      <td>83968.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9d94b05f-41eb-453d-abb9-6f4890e4e3cb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9d94b05f-41eb-453d-abb9-6f4890e4e3cb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9d94b05f-41eb-453d-abb9-6f4890e4e3cb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# from torch.nn.modules import dropout\n",
        "from timm.models.layers import trunc_normal_, DropPath\n",
        "from timm.models.registry import register_model\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 expand_ratio, drop_path=0.):\n",
        "        super().__init__() \n",
        "\n",
        "        # change the kernel size to 7 (spatial mix)\n",
        "        # depth wise convolution\n",
        "        # 3*3 input padding 1, stride ==1\n",
        "        self.spatial_mixing = nn.Conv1d(in_channels, in_channels, kernel_size=7, padding=3, stride=1,\n",
        "                                        groups=in_channels)\n",
        "        self.norm = nn.BatchNorm1d(in_channels)\n",
        "\n",
        "        # point wise convolution, increase the channel number, expand ratio =1\n",
        "        hidden_dim = in_channels * expand_ratio\n",
        "        self.feature_mixing = nn.Conv1d(in_channels, hidden_dim, kernel_size=1, stride=1, padding=0)\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "        self.bottleneck_channels = nn.Conv1d(hidden_dim, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        input = x\n",
        "        x = self.spatial_mixing(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.feature_mixing(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.bottleneck_channels(x)\n",
        "        x = input + self.drop_path(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(self,drop_out = drop_out):  # You can add any extra arguments as you wish\n",
        "\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        # Embedding layer converts the raw input into features which may (or may not) help the LSTM to learn better\n",
        "        # For the very low cut-off you dont require an embedding layer. You can pass the input directly to the  LSTM\n",
        "        self.embedding =nn.ModuleList()\n",
        "        \n",
        "        stem = nn.Sequential(nn.Conv1d(13,128,5,stride=2, padding=2),nn.BatchNorm1d(128))\n",
        "        self.embedding.append(stem)\n",
        "        \n",
        "        block = [Block(in_channels=128, out_channels=128, expand_ratio=2,drop_path=0.1)]\n",
        "        self.embedding.append(nn.Sequential(*block))\n",
        "\n",
        "        stem2 = nn.Sequential(nn.Conv1d(128,256,3,stride=1, padding=1),nn.BatchNorm1d(256))\n",
        "        self.embedding.append(stem2)\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=256, hidden_size= 512, num_layers=4,bidirectional=True,dropout=0.4)  # TODO: # Create a single layer, uni-directional LSTM with hidden_size = 256\n",
        "        # Use nn.LSTM() Make sure that you give in the proper arguments as given in https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "\n",
        "        self.classification = nn.Sequential(\n",
        "            nn.Linear(1024, 2048),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_out),\n",
        "            \n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_out),            \n",
        "\n",
        "            nn.Linear(2048, 41)\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "  \n",
        "\n",
        "    def forward(self, x, lx):  # TODO: You need to pass at least 1 more parameter apart from self and x\n",
        "        x = torch.permute(x,(1,2,0))\n",
        "\n",
        "        # x = T.FrequencyMasking(freq_mask_param=2)(x)\n",
        "\n",
        "        for i in range(len(self.embedding)):\n",
        "          x = self.embedding[i](x)\n",
        "\n",
        "        embedding = x\n",
        "        lx //=2\n",
        "\n",
        "        embedding = torch.permute(embedding,(2,0,1))\n",
        "\n",
        "        # x is returned from the dataloader. So it is assumed to be padded with the help of the collate_fn\n",
        "        packed_input = pack_padded_sequence(embedding,lx,enforce_sorted=False)  # TODO: Pack the input with pack_padded_sequence. Look at the parameters it requires \n",
        "        #enforced_sorted =False\n",
        "\n",
        "        out1, (out2, out3) = self.lstm(packed_input)  # TODO: Pass packed input to self.lstm\n",
        "        # As you may see from the LSTM docs, LSTM returns 3 vectors. Which one do you need to pass to the next function?\n",
        "        # print(out1.shape)\n",
        "        out, lengths = pad_packed_sequence(out1)  # TODO: Need to 'unpack' the LSTM output using pad_packed_sequence\n",
        "\n",
        "        out = self.classification(out)  # TODO: Pass unpacked LSTM output to the classification layer\n",
        "        # out_prob = out.log_softmax(2) # Optional: Do log softmax on the output. Which dimension?\n",
        "\n",
        "        out_prob = F.log_softmax(out,dim=2) # Optional: Do log softmax on the output. Which dimension?\n",
        "        # print(out_prob.shape)\n",
        "\n",
        "        return out_prob, lengths  # TODO: Need to return 2 variables\n",
        "\n",
        "model = Network().to(device)\n",
        "print(model)\n",
        "summary(model, x.to(device), lx) # x and lx are from the previous cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwunYpyugFg"
      },
      "source": [
        "# Optimizer Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGoozH2nd6KB"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CTCLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= lr) # TODO: Adam works well with LSTM (use lr = 2e-3)\n",
        "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader) * epochs))\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=4,factor=0.2, mode=\"min\")\n",
        "\n",
        "decoder = CTCBeamDecoder(PHONEME_MAP,log_probs_input=True, beam_width=1) # TODO: Intialize the CTC beam decoder\n",
        "decoder_test= CTCBeamDecoder(PHONEME_MAP,log_probs_input=True, beam_width=50)\n",
        "scaler = torch.cuda.amp.GradScaler()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxVS0-qSZmAd"
      },
      "source": [
        "# Levenshtein distance calculation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEuvs3Kje47-"
      },
      "outputs": [],
      "source": [
        "def calculate_levenshtein(h, y, ly,lh,decoder, PHONEME_MAP):\n",
        "\n",
        "    # h - ouput from the model. Probability distributions at each time step \n",
        "    # y - target output sequence - sequence of Long tensors\n",
        "    # lh, ly - Lengths of output and target\n",
        "    # decoder - decoder object which was initialized in the previous cell\n",
        "    # PHONEME_MAP - maps output to a character to find the Levenshtein distance\n",
        "\n",
        "    h = h.permute(1,0,2) # potential problems (batch first model) \n",
        "\n",
        "    # TODO: call the decoder's decode method and get beam_results and out_len (Read the docs about the decode method's outputs)\n",
        "    # Input to the decode method will be h and its lengths lh \n",
        "    # You need to pass lh for the 'seq_lens' parameter. This is not explicitly mentioned in the git repo of ctcdecode.\n",
        "    beam_results, _, _, out_lens = decoder.decode(h,lh)\n",
        "\n",
        "    batch_size = len(beam_results)\n",
        "\n",
        "    dist = 0\n",
        "\n",
        "    for i in range(batch_size): # Loop through each element in the batch\n",
        "\n",
        "        h_sliced = beam_results[i][0]# TODO: Get the output as a sequence of numbers from beam_results\n",
        "\n",
        "        # Remember that h is padded to the max sequence length and lh contains lengths of individual sequences\n",
        "        # Same goes for beam_results and out_lens\n",
        "        # You do not require the padded portion of beam_results - you need to slice it with out_lens \n",
        "        # If it is confusing, print out the shapes of all the variables and try to understand\n",
        "\n",
        "        h_string = \"\".join(PHONEME_MAP[x] for x in h_sliced[0:out_lens[i][0]]) # TODO: MAP the sequence of numbers to its corresponding characters with PHONEME_MAP and merge everything as a single string\n",
        "\n",
        "        y_sliced = y[i] # TODO: Do the same for y - slice off the padding with ly\n",
        "\n",
        "        y_string = \"\".join(PHONEME_MAP[i] for i in y_sliced[0:ly[i]]) # TODO: MAP the sequence of numbers to its corresponding characters with PHONEME_MAP and merge everything as a single string\n",
        "        \n",
        "        dist += Levenshtein.distance(h_string, y_string)\n",
        "\n",
        "    dist/=batch_size\n",
        "\n",
        "    return dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2maZ7Z8GZxDy"
      },
      "source": [
        "# Training and Evaluation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MG4F77Nm0Am9"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "for epoch in range(1,epochs+1):\n",
        "    start_time = time.time()\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    totalSampleCnt = 0.0\n",
        "    \n",
        "    for batch_idx, (data, target, data_lens, target_lens) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        target = target.permute(1,0)\n",
        "        data, target= data.cuda(), target.cuda()\n",
        "        \n",
        "        with torch.cuda.amp.autocast():  \n",
        "            output, output_lens = model(data,data_lens)\n",
        "            loss = criterion(output,target, output_lens,target_lens)\n",
        "\n",
        "        running_loss+=loss.item()\n",
        "        totalSampleCnt += len(data)\n",
        "\n",
        "        loss_per_sample = running_loss / len(val_loader)\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "                loss=\"{:.04f}\".format(float(loss_per_sample)),\n",
        "                lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "        lr_wb = round(float(optimizer.param_groups[0]['lr']),8)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        batch_bar.update()\n",
        "    batch_bar.close()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        running_charErr = 0.0\n",
        "        totalSampleCnt = 0\n",
        "        \n",
        "        for batch_idx, (data, target, data_lens, target_lens) in enumerate(val_loader):\n",
        "            target = target.permute(1,0)\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            output, output_lens = model(data, data_lens)    \n",
        "            loss = criterion(output,target, output_lens,target_lens)\n",
        "\n",
        "            running_loss+=loss.item()\n",
        "            totalSampleCnt += len(data)\n",
        "\n",
        "            dist = calculate_levenshtein(output,target,target_lens,output_lens,decoder,PHONEME_MAP)\n",
        "            running_charErr += dist\n",
        "        \n",
        "        loss_per_sample = running_loss / len(val_loader)\n",
        "        dist_per_sample = running_charErr/len(val_loader)\n",
        "        \n",
        "        scheduler.step(dist_per_sample)\n",
        "\n",
        "        print(\"epoch \",epoch,\"loss is: \",loss_per_sample, \" Levenshtein is: \",dist_per_sample)\n",
        "        \n",
        "        wandb.log({\"loss\": loss,\"Levenshtein\":dist_per_sample,\"lr\":lr_wb,\"epoch\":epoch})\n",
        "    \n",
        "    end_time = time.time()\n",
        "    print(\"Whole Epoch takes: \", (end_time - start_time)/60,\"minutes to train\")\n",
        "\n",
        "    path = data_path+f\"/model_epoch_{epoch}.ckpt\"\n",
        "\n",
        "    if epoch % 5==0:\n",
        "        torch.save({\n",
        "              'epoch': epoch,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(), \n",
        "          }, path)\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmoZ6T-ljZOw"
      },
      "source": [
        "# Resume model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNaC7r3mjv9t"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/HW3/store_checkpoints/LSTM_Embedding_layerPlus2_expansion2_block2_ReduceOnPlateau_2Cls_dropout0.5_stride2_Cepstral_kaiming_ConvNext_no/model_epoch_98.ckpt\"\n",
        "checkpoint = torch.load(path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxO2LvHfjY3R"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "for epoch in range(107,epochs+1):\n",
        "    start_time = time.time()\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    totalSampleCnt = 0.0\n",
        "    \n",
        "    for batch_idx, (data, target, data_lens, target_lens) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        target = target.permute(1,0)\n",
        "        data, target= data.cuda(), target.cuda()\n",
        "        \n",
        "        with torch.cuda.amp.autocast():  \n",
        "            output, output_lens = model(data,data_lens)\n",
        "            loss = criterion(output,target, output_lens,target_lens)\n",
        "\n",
        "        running_loss+=loss.item()\n",
        "        totalSampleCnt += len(data)\n",
        "\n",
        "        loss_per_sample = running_loss / len(val_loader)\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "                loss=\"{:.04f}\".format(float(loss_per_sample)),\n",
        "                lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "        lr_wb = round(float(optimizer.param_groups[0]['lr']),8)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        batch_bar.update()\n",
        "    batch_bar.close()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        running_charErr = 0.0\n",
        "        totalSampleCnt = 0\n",
        "        \n",
        "        for batch_idx, (data, target, data_lens, target_lens) in enumerate(val_loader):\n",
        "            target = target.permute(1,0)\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            output, output_lens = model(data, data_lens)    \n",
        "            loss = criterion(output,target, output_lens,target_lens)\n",
        "\n",
        "            running_loss+=loss.item()\n",
        "            totalSampleCnt += len(data)\n",
        "\n",
        "            dist = calculate_levenshtein(output,target,target_lens,output_lens,decoder,PHONEME_MAP)\n",
        "            running_charErr += dist\n",
        "        \n",
        "        loss_per_sample = running_loss / len(val_loader)\n",
        "        dist_per_sample = running_charErr/len(val_loader)\n",
        "        \n",
        "        scheduler.step(dist_per_sample)\n",
        "\n",
        "        print(\"epoch \",epoch,\"loss is: \",loss_per_sample, \" Levenshtein is: \",dist_per_sample)\n",
        "        \n",
        "        wandb.log({\"loss\": loss,\"Levenshtein\":dist_per_sample,\"lr\":lr_wb,\"epoch\":epoch})\n",
        "    \n",
        "    end_time = time.time()\n",
        "    print(\"Whole Epoch takes: \", (end_time - start_time)/60,\"minutes to train\")\n",
        "\n",
        "    path = data_path+f\"/model_epoch_{epoch}.ckpt\"\n",
        "\n",
        "    if epoch % 2==0:\n",
        "        torch.save({\n",
        "              'epoch': epoch,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(), \n",
        "          }, path)\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xD94F70NzfhN"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gulNm5dPzhwJ"
      },
      "outputs": [],
      "source": [
        "decoder_test= CTCBeamDecoder(PHONEME_MAP,log_probs_input=True, beam_width=50)\n",
        "path = \"/content/drive/MyDrive/HW3/store_checkpoints/LSTM_Embedding_layerPlus2_expansion2_block2_ReduceOnPlateau_2Cls_dropout0.5_stride2_Cepstral_kaiming_ConvNext_no/model_epoch_116.ckpt\"\n",
        "checkpoint = torch.load(path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abwKrs1T6ipU"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLuRpwtq6pxz"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "res = []\n",
        "\n",
        "for batch_idx,(data, data_lens) in enumerate(test_loader):\n",
        "    data = data.cuda()\n",
        "    model = model.cuda()\n",
        "    output, output_lens = model(data,data_lens)\n",
        "    output = output.permute(1,0,2)\n",
        "    beam_results, _, _, out_lens = decoder_test.decode(output,output_lens)\n",
        "\n",
        "    batch_size = len(beam_results)\n",
        "\n",
        "    for i in range(batch_size): # Loop through each element in the batch\n",
        "\n",
        "        h_sliced = beam_results[i][0]# TODO: Get the output as a sequence of numbers from beam_results\n",
        "        h_string = \"\".join(PHONEME_MAP[x] for x in h_sliced[0:out_lens[i][0]]) # TODO: MAP the sequence of numbers to its corresponding characters with PHONEME_MAP and merge everything as a single string\n",
        "\n",
        "        # print(h_string)\n",
        "\n",
        "        res.append(h_string)\n",
        "\n",
        "# res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6I0xYmlTVXB",
        "outputId": "417ea4f2-2cab-44cd-e137-880e02c4b830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 214k/214k [00:01<00:00, 117kB/s]\n",
            "Successfully submitted to Automatic Speech Recognition (ASR)"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "id = np.array(range(len(res)))\n",
        "predictions = res\n",
        "df = pd.DataFrame({\"id\":id,\"predictions\":predictions})\n",
        "df.to_csv(\"submission.csv\",index=False)\n",
        "\n",
        "!kaggle competitions submit -c 11-785-s22-hw3p2 -f submission.csv -m \"Message\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROrqXnNqzJSc"
      },
      "source": [
        "# Submit to kaggle (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-SU9fZ3xHtk"
      },
      "outputs": [],
      "source": [
        "# TODO: Write your model evaluation code for the test dataset\n",
        "# You can write your own code or use from the previous homewoks' stater notebooks\n",
        "# You can't calculate loss here. Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZE1hRnvf0bFz"
      },
      "outputs": [],
      "source": [
        "# TODO: Generate the csv file"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "z4vZbDmJvMp1",
        "vUCKqm1ST1sU",
        "Ly4mjUUUuJhy",
        "IBwunYpyugFg",
        "KxVS0-qSZmAd"
      ],
      "machine_shape": "hm",
      "name": "HW3P2_Final_README.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}