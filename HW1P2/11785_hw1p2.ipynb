{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  Instrution on how to run the code\n",
        "\n",
        "In this code, the whole jupyter notebook could be segmented into three majot part, training, predicting and model resuming. \n",
        "\n",
        "The previous four cells are basic configuration, including mount the google drive, download and unzip the kaggle dataset. The forth cell is to check the type of GPU.\n",
        "\n",
        "The first main section is training section. I referred and modified the argument dictionary to allow easy hyperparameter tuning. You could modify the regarding hyperparameter to control the tuning process. \n",
        "\n",
        "The second main code chunk is the model prediction. I would load the saved model to make prediction and submit to the kaggle. You have to maintain the hyperparameter same with loaded model.\n",
        "\n",
        "The third main code chunk is to resume the model training. Therefore, apart from the same hyperparameters in previous two code. I added another `resume_epoch`, which would indicate the last epoch in the resuming model training part and  `epoch` would be the start epoch of the model."
      ],
      "metadata": {
        "id": "5Ur0yMe15rx4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFWHQqVO45l9"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "%cd /content\n",
        "!mkdir drive\n",
        "%cd drive\n",
        "%cd ..\n",
        "!google-drive-ocamlfuse /content/drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaBqm5j15A4B"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "TOKEN = {\"username\":\"yuxuanwucmu\",\"key\":\"81ded4babfd327efdc7655be369299b5\"}\n",
        "\n",
        "! pip install kaggle==1.5.12\n",
        "! mkdir -p .kaggle\n",
        "! mkdir -p /content & mkdir -p /content/.kaggle & mkdir -p /root/.kaggle/\n",
        "\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(TOKEN, file)\n",
        "\n",
        "! pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "! ls \"/content/.kaggle\"\n",
        "! chmod 600 /content/.kaggle/kaggle.json\n",
        "! cp /content/.kaggle/kaggle.json /root/.kaggle/\n",
        "\n",
        "! kaggle config set -n path -v /content/drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uq1fb6ff5C85"
      },
      "outputs": [],
      "source": [
        "! kaggle competitions download -c 11-785-s22-hw1p2\n",
        "! unzip /content/drive/competitions/11-785-s22-hw1p2/11-785-s22-hw1p2.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AzHMtyMGWhA",
        "outputId": "878dbe34-e78f-4b64-c147-9c06ba134493"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Feb 11 12:29:30 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2MjEFBtX5T_0",
        "outputId": "27dec1c1-4cbc-4e5e-e4bd-568fa4465564"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We are going to save to /content/drive/hw1/checkpoints/Pyramid_context_30_lr_0.001_ReduceLROnPlateau_factor0.5_max_patience_2_submission_LeakyReLU/\n",
            "Train Epoch: 1 [0/25340216 (0%)]\tLoss: 4.093301\n",
            "Train Epoch: 1 [409600/25340216 (2%)]\tLoss: 2.097481\n",
            "Train Epoch: 1 [819200/25340216 (3%)]\tLoss: 2.023282\n",
            "Train Epoch: 1 [1228800/25340216 (5%)]\tLoss: 1.911462\n",
            "Train Epoch: 1 [1638400/25340216 (6%)]\tLoss: 1.840209\n",
            "Train Epoch: 1 [2048000/25340216 (8%)]\tLoss: 1.788753\n",
            "Train Epoch: 1 [2457600/25340216 (10%)]\tLoss: 1.773855\n",
            "Train Epoch: 1 [2867200/25340216 (11%)]\tLoss: 1.820575\n",
            "Train Epoch: 1 [3276800/25340216 (13%)]\tLoss: 1.735468\n",
            "Train Epoch: 1 [3686400/25340216 (15%)]\tLoss: 1.685440\n",
            "Train Epoch: 1 [4096000/25340216 (16%)]\tLoss: 1.677622\n",
            "Train Epoch: 1 [4505600/25340216 (18%)]\tLoss: 1.715910\n",
            "Train Epoch: 1 [4915200/25340216 (19%)]\tLoss: 1.711706\n",
            "Train Epoch: 1 [5324800/25340216 (21%)]\tLoss: 1.641915\n",
            "Train Epoch: 1 [5734400/25340216 (23%)]\tLoss: 1.649227\n",
            "Train Epoch: 1 [6144000/25340216 (24%)]\tLoss: 1.712404\n",
            "Train Epoch: 1 [6553600/25340216 (26%)]\tLoss: 1.678475\n",
            "Train Epoch: 1 [6963200/25340216 (27%)]\tLoss: 1.554738\n",
            "Train Epoch: 1 [7372800/25340216 (29%)]\tLoss: 1.645415\n",
            "Train Epoch: 1 [7782400/25340216 (31%)]\tLoss: 1.661597\n",
            "Train Epoch: 1 [8192000/25340216 (32%)]\tLoss: 1.651861\n",
            "Train Epoch: 1 [8601600/25340216 (34%)]\tLoss: 1.571856\n",
            "Train Epoch: 1 [9011200/25340216 (36%)]\tLoss: 1.642339\n",
            "Train Epoch: 1 [9420800/25340216 (37%)]\tLoss: 1.603806\n",
            "Train Epoch: 1 [9830400/25340216 (39%)]\tLoss: 1.643182\n",
            "Train Epoch: 1 [10240000/25340216 (40%)]\tLoss: 1.582935\n",
            "Train Epoch: 1 [10649600/25340216 (42%)]\tLoss: 1.602046\n",
            "Train Epoch: 1 [11059200/25340216 (44%)]\tLoss: 1.600913\n",
            "Train Epoch: 1 [11468800/25340216 (45%)]\tLoss: 1.600889\n",
            "Train Epoch: 1 [11878400/25340216 (47%)]\tLoss: 1.618588\n",
            "Train Epoch: 1 [12288000/25340216 (48%)]\tLoss: 1.536361\n",
            "Train Epoch: 1 [12697600/25340216 (50%)]\tLoss: 1.563701\n",
            "Train Epoch: 1 [13107200/25340216 (52%)]\tLoss: 1.582577\n",
            "Train Epoch: 1 [13516800/25340216 (53%)]\tLoss: 1.561107\n",
            "Train Epoch: 1 [13926400/25340216 (55%)]\tLoss: 1.623182\n",
            "Train Epoch: 1 [14336000/25340216 (57%)]\tLoss: 1.563887\n",
            "Train Epoch: 1 [14745600/25340216 (58%)]\tLoss: 1.562732\n",
            "Train Epoch: 1 [15155200/25340216 (60%)]\tLoss: 1.563534\n",
            "Train Epoch: 1 [15564800/25340216 (61%)]\tLoss: 1.569438\n",
            "Train Epoch: 1 [15974400/25340216 (63%)]\tLoss: 1.561676\n",
            "Train Epoch: 1 [16384000/25340216 (65%)]\tLoss: 1.614600\n",
            "Train Epoch: 1 [16793600/25340216 (66%)]\tLoss: 1.637432\n",
            "Train Epoch: 1 [17203200/25340216 (68%)]\tLoss: 1.578443\n",
            "Train Epoch: 1 [17612800/25340216 (70%)]\tLoss: 1.574802\n",
            "Train Epoch: 1 [18022400/25340216 (71%)]\tLoss: 1.463913\n",
            "Train Epoch: 1 [18432000/25340216 (73%)]\tLoss: 1.610409\n",
            "Train Epoch: 1 [18841600/25340216 (74%)]\tLoss: 1.497971\n",
            "Train Epoch: 1 [19251200/25340216 (76%)]\tLoss: 1.538131\n",
            "Train Epoch: 1 [19660800/25340216 (78%)]\tLoss: 1.584338\n",
            "Train Epoch: 1 [20070400/25340216 (79%)]\tLoss: 1.493325\n",
            "Train Epoch: 1 [20480000/25340216 (81%)]\tLoss: 1.470661\n",
            "Train Epoch: 1 [20889600/25340216 (82%)]\tLoss: 1.588138\n",
            "Train Epoch: 1 [21299200/25340216 (84%)]\tLoss: 1.482728\n",
            "Train Epoch: 1 [21708800/25340216 (86%)]\tLoss: 1.439881\n",
            "Train Epoch: 1 [22118400/25340216 (87%)]\tLoss: 1.578415\n",
            "Train Epoch: 1 [22528000/25340216 (89%)]\tLoss: 1.461362\n",
            "Train Epoch: 1 [22937600/25340216 (91%)]\tLoss: 1.468828\n",
            "Train Epoch: 1 [23347200/25340216 (92%)]\tLoss: 1.576959\n",
            "Train Epoch: 1 [23756800/25340216 (94%)]\tLoss: 1.544444\n",
            "Train Epoch: 1 [24166400/25340216 (95%)]\tLoss: 1.474150\n",
            "Train Epoch: 1 [24576000/25340216 (97%)]\tLoss: 1.591830\n",
            "Train Epoch: 1 [24985600/25340216 (99%)]\tLoss: 1.527827\n",
            "Train Epoch: 1 [0/10850918 (0%)]\tLoss: 1.486120\n",
            "Train Epoch: 1 [409600/10850918 (4%)]\tLoss: 1.531989\n",
            "Train Epoch: 1 [819200/10850918 (8%)]\tLoss: 1.499831\n",
            "Train Epoch: 1 [1228800/10850918 (11%)]\tLoss: 1.487527\n",
            "Train Epoch: 1 [1638400/10850918 (15%)]\tLoss: 1.493373\n",
            "Train Epoch: 1 [2048000/10850918 (19%)]\tLoss: 1.439523\n",
            "Train Epoch: 1 [2457600/10850918 (23%)]\tLoss: 1.478056\n",
            "Train Epoch: 1 [2867200/10850918 (26%)]\tLoss: 1.493414\n",
            "Train Epoch: 1 [3276800/10850918 (30%)]\tLoss: 1.474457\n",
            "Train Epoch: 1 [3686400/10850918 (34%)]\tLoss: 1.509964\n",
            "Train Epoch: 1 [4096000/10850918 (38%)]\tLoss: 1.434232\n",
            "Train Epoch: 1 [4505600/10850918 (42%)]\tLoss: 1.459638\n",
            "Train Epoch: 1 [4915200/10850918 (45%)]\tLoss: 1.503159\n",
            "Train Epoch: 1 [5324800/10850918 (49%)]\tLoss: 1.505635\n",
            "Train Epoch: 1 [5734400/10850918 (53%)]\tLoss: 1.535694\n",
            "Train Epoch: 1 [6144000/10850918 (57%)]\tLoss: 1.492549\n",
            "Train Epoch: 1 [6553600/10850918 (60%)]\tLoss: 1.484733\n",
            "Train Epoch: 1 [6963200/10850918 (64%)]\tLoss: 1.383024\n",
            "Train Epoch: 1 [7372800/10850918 (68%)]\tLoss: 1.478405\n",
            "Train Epoch: 1 [7782400/10850918 (72%)]\tLoss: 1.519504\n",
            "Train Epoch: 1 [8192000/10850918 (75%)]\tLoss: 1.393735\n",
            "Train Epoch: 1 [8601600/10850918 (79%)]\tLoss: 1.420513\n",
            "Train Epoch: 1 [9011200/10850918 (83%)]\tLoss: 1.407273\n",
            "Train Epoch: 1 [9420800/10850918 (87%)]\tLoss: 1.378834\n",
            "Train Epoch: 1 [9830400/10850918 (91%)]\tLoss: 1.513901\n",
            "Train Epoch: 1 [10240000/10850918 (94%)]\tLoss: 1.436208\n",
            "Train Epoch: 1 [10649600/10850918 (98%)]\tLoss: 1.407419\n",
            "Train Epoch: 1 [0/1937496 (0%)]\tLoss: 1.513221\n",
            "Train Epoch: 1 [409600/1937496 (21%)]\tLoss: 1.523296\n",
            "Train Epoch: 1 [819200/1937496 (42%)]\tLoss: 1.384532\n",
            "Train Epoch: 1 [1228800/1937496 (63%)]\tLoss: 1.356502\n",
            "Train Epoch: 1 [1638400/1937496 (84%)]\tLoss: 1.355289\n",
            "Dev accuracy:  0.8900797730679186\n",
            " 50.466245619455975 minutes\n",
            "Train Epoch: 2 [0/25340216 (0%)]\tLoss: 1.481637\n",
            "Train Epoch: 2 [409600/25340216 (2%)]\tLoss: 1.532216\n",
            "Train Epoch: 2 [819200/25340216 (3%)]\tLoss: 1.469836\n",
            "Train Epoch: 2 [1228800/25340216 (5%)]\tLoss: 1.450034\n",
            "Train Epoch: 2 [1638400/25340216 (6%)]\tLoss: 1.433374\n",
            "Train Epoch: 2 [2048000/25340216 (8%)]\tLoss: 1.473267\n",
            "Train Epoch: 2 [2457600/25340216 (10%)]\tLoss: 1.436651\n",
            "Train Epoch: 2 [2867200/25340216 (11%)]\tLoss: 1.493689\n",
            "Train Epoch: 2 [3276800/25340216 (13%)]\tLoss: 1.437629\n",
            "Train Epoch: 2 [3686400/25340216 (15%)]\tLoss: 1.396830\n",
            "Train Epoch: 2 [4096000/25340216 (16%)]\tLoss: 1.459081\n",
            "Train Epoch: 2 [4505600/25340216 (18%)]\tLoss: 1.434504\n",
            "Train Epoch: 2 [4915200/25340216 (19%)]\tLoss: 1.391286\n",
            "Train Epoch: 2 [5324800/25340216 (21%)]\tLoss: 1.482133\n",
            "Train Epoch: 2 [5734400/25340216 (23%)]\tLoss: 1.470065\n",
            "Train Epoch: 2 [6144000/25340216 (24%)]\tLoss: 1.482144\n",
            "Train Epoch: 2 [6553600/25340216 (26%)]\tLoss: 1.327106\n",
            "Train Epoch: 2 [6963200/25340216 (27%)]\tLoss: 1.424857\n",
            "Train Epoch: 2 [7372800/25340216 (29%)]\tLoss: 1.441067\n",
            "Train Epoch: 2 [7782400/25340216 (31%)]\tLoss: 1.468592\n",
            "Train Epoch: 2 [8192000/25340216 (32%)]\tLoss: 1.405825\n",
            "Train Epoch: 2 [8601600/25340216 (34%)]\tLoss: 1.420732\n",
            "Train Epoch: 2 [9011200/25340216 (36%)]\tLoss: 1.461078\n",
            "Train Epoch: 2 [9420800/25340216 (37%)]\tLoss: 1.401723\n",
            "Train Epoch: 2 [9830400/25340216 (39%)]\tLoss: 1.398046\n",
            "Train Epoch: 2 [10240000/25340216 (40%)]\tLoss: 1.418911\n",
            "Train Epoch: 2 [10649600/25340216 (42%)]\tLoss: 1.402919\n",
            "Train Epoch: 2 [11059200/25340216 (44%)]\tLoss: 1.428584\n",
            "Train Epoch: 2 [11468800/25340216 (45%)]\tLoss: 1.390793\n",
            "Train Epoch: 2 [11878400/25340216 (47%)]\tLoss: 1.379744\n",
            "Train Epoch: 2 [12288000/25340216 (48%)]\tLoss: 1.416832\n",
            "Train Epoch: 2 [12697600/25340216 (50%)]\tLoss: 1.488829\n",
            "Train Epoch: 2 [13107200/25340216 (52%)]\tLoss: 1.423413\n",
            "Train Epoch: 2 [13516800/25340216 (53%)]\tLoss: 1.380869\n",
            "Train Epoch: 2 [13926400/25340216 (55%)]\tLoss: 1.362882\n",
            "Train Epoch: 2 [14336000/25340216 (57%)]\tLoss: 1.440226\n",
            "Train Epoch: 2 [14745600/25340216 (58%)]\tLoss: 1.442919\n",
            "Train Epoch: 2 [15155200/25340216 (60%)]\tLoss: 1.440657\n",
            "Train Epoch: 2 [15564800/25340216 (61%)]\tLoss: 1.385097\n",
            "Train Epoch: 2 [15974400/25340216 (63%)]\tLoss: 1.382613\n",
            "Train Epoch: 2 [16384000/25340216 (65%)]\tLoss: 1.400987\n",
            "Train Epoch: 2 [16793600/25340216 (66%)]\tLoss: 1.478864\n",
            "Train Epoch: 2 [17203200/25340216 (68%)]\tLoss: 1.463364\n",
            "Train Epoch: 2 [17612800/25340216 (70%)]\tLoss: 1.413015\n",
            "Train Epoch: 2 [18022400/25340216 (71%)]\tLoss: 1.403477\n",
            "Train Epoch: 2 [18432000/25340216 (73%)]\tLoss: 1.433175\n",
            "Train Epoch: 2 [18841600/25340216 (74%)]\tLoss: 1.415195\n",
            "Train Epoch: 2 [19251200/25340216 (76%)]\tLoss: 1.363450\n",
            "Train Epoch: 2 [19660800/25340216 (78%)]\tLoss: 1.423090\n",
            "Train Epoch: 2 [20070400/25340216 (79%)]\tLoss: 1.308784\n",
            "Train Epoch: 2 [20480000/25340216 (81%)]\tLoss: 1.455836\n",
            "Train Epoch: 2 [20889600/25340216 (82%)]\tLoss: 1.477577\n",
            "Train Epoch: 2 [21299200/25340216 (84%)]\tLoss: 1.448380\n",
            "Train Epoch: 2 [21708800/25340216 (86%)]\tLoss: 1.440787\n",
            "Train Epoch: 2 [22118400/25340216 (87%)]\tLoss: 1.385092\n",
            "Train Epoch: 2 [22528000/25340216 (89%)]\tLoss: 1.414455\n",
            "Train Epoch: 2 [22937600/25340216 (91%)]\tLoss: 1.376766\n",
            "Train Epoch: 2 [23347200/25340216 (92%)]\tLoss: 1.369823\n",
            "Train Epoch: 2 [23756800/25340216 (94%)]\tLoss: 1.398624\n",
            "Train Epoch: 2 [24166400/25340216 (95%)]\tLoss: 1.366111\n",
            "Train Epoch: 2 [24576000/25340216 (97%)]\tLoss: 1.388115\n",
            "Train Epoch: 2 [24985600/25340216 (99%)]\tLoss: 1.334877\n",
            "Train Epoch: 2 [0/10850918 (0%)]\tLoss: 1.505831\n",
            "Train Epoch: 2 [409600/10850918 (4%)]\tLoss: 1.378600\n",
            "Train Epoch: 2 [819200/10850918 (8%)]\tLoss: 1.433574\n",
            "Train Epoch: 2 [1228800/10850918 (11%)]\tLoss: 1.378526\n",
            "Train Epoch: 2 [1638400/10850918 (15%)]\tLoss: 1.354574\n",
            "Train Epoch: 2 [2048000/10850918 (19%)]\tLoss: 1.415640\n",
            "Train Epoch: 2 [2457600/10850918 (23%)]\tLoss: 1.353582\n",
            "Train Epoch: 2 [2867200/10850918 (26%)]\tLoss: 1.377681\n",
            "Train Epoch: 2 [3276800/10850918 (30%)]\tLoss: 1.523944\n",
            "Train Epoch: 2 [3686400/10850918 (34%)]\tLoss: 1.351681\n",
            "Train Epoch: 2 [4096000/10850918 (38%)]\tLoss: 1.379034\n",
            "Train Epoch: 2 [4505600/10850918 (42%)]\tLoss: 1.311413\n",
            "Train Epoch: 2 [4915200/10850918 (45%)]\tLoss: 1.409100\n",
            "Train Epoch: 2 [5324800/10850918 (49%)]\tLoss: 1.437490\n",
            "Train Epoch: 2 [5734400/10850918 (53%)]\tLoss: 1.386481\n",
            "Train Epoch: 2 [6144000/10850918 (57%)]\tLoss: 1.292019\n",
            "Train Epoch: 2 [6553600/10850918 (60%)]\tLoss: 1.334627\n",
            "Train Epoch: 2 [6963200/10850918 (64%)]\tLoss: 1.397940\n",
            "Train Epoch: 2 [7372800/10850918 (68%)]\tLoss: 1.376379\n",
            "Train Epoch: 2 [7782400/10850918 (72%)]\tLoss: 1.315838\n",
            "Train Epoch: 2 [8192000/10850918 (75%)]\tLoss: 1.366748\n",
            "Train Epoch: 2 [8601600/10850918 (79%)]\tLoss: 1.384955\n",
            "Train Epoch: 2 [9011200/10850918 (83%)]\tLoss: 1.397539\n",
            "Train Epoch: 2 [9420800/10850918 (87%)]\tLoss: 1.347742\n",
            "Train Epoch: 2 [9830400/10850918 (91%)]\tLoss: 1.312541\n",
            "Train Epoch: 2 [10240000/10850918 (94%)]\tLoss: 1.333477\n",
            "Train Epoch: 2 [10649600/10850918 (98%)]\tLoss: 1.363177\n",
            "Train Epoch: 2 [0/1937496 (0%)]\tLoss: 1.418250\n",
            "Train Epoch: 2 [409600/1937496 (21%)]\tLoss: 1.340766\n",
            "Train Epoch: 2 [819200/1937496 (42%)]\tLoss: 1.350744\n",
            "Train Epoch: 2 [1228800/1937496 (63%)]\tLoss: 1.333482\n",
            "Train Epoch: 2 [1638400/1937496 (84%)]\tLoss: 1.367249\n",
            "Dev accuracy:  0.9110516873325157\n",
            " 50.71858041683833 minutes\n",
            "Train Epoch: 3 [0/25340216 (0%)]\tLoss: 1.428050\n",
            "Train Epoch: 3 [409600/25340216 (2%)]\tLoss: 1.407020\n",
            "Train Epoch: 3 [819200/25340216 (3%)]\tLoss: 1.392207\n",
            "Train Epoch: 3 [1228800/25340216 (5%)]\tLoss: 1.368941\n",
            "Train Epoch: 3 [1638400/25340216 (6%)]\tLoss: 1.363927\n",
            "Train Epoch: 3 [2048000/25340216 (8%)]\tLoss: 1.472231\n",
            "Train Epoch: 3 [2457600/25340216 (10%)]\tLoss: 1.381130\n",
            "Train Epoch: 3 [2867200/25340216 (11%)]\tLoss: 1.434799\n",
            "Train Epoch: 3 [3276800/25340216 (13%)]\tLoss: 1.342507\n",
            "Train Epoch: 3 [3686400/25340216 (15%)]\tLoss: 1.388461\n",
            "Train Epoch: 3 [4096000/25340216 (16%)]\tLoss: 1.420750\n",
            "Train Epoch: 3 [4505600/25340216 (18%)]\tLoss: 1.401022\n",
            "Train Epoch: 3 [4915200/25340216 (19%)]\tLoss: 1.393496\n",
            "Train Epoch: 3 [5324800/25340216 (21%)]\tLoss: 1.368785\n",
            "Train Epoch: 3 [5734400/25340216 (23%)]\tLoss: 1.369566\n",
            "Train Epoch: 3 [6144000/25340216 (24%)]\tLoss: 1.333483\n",
            "Train Epoch: 3 [6553600/25340216 (26%)]\tLoss: 1.347866\n",
            "Train Epoch: 3 [6963200/25340216 (27%)]\tLoss: 1.380737\n",
            "Train Epoch: 3 [7372800/25340216 (29%)]\tLoss: 1.396791\n",
            "Train Epoch: 3 [7782400/25340216 (31%)]\tLoss: 1.348050\n",
            "Train Epoch: 3 [8192000/25340216 (32%)]\tLoss: 1.316167\n",
            "Train Epoch: 3 [8601600/25340216 (34%)]\tLoss: 1.341621\n",
            "Train Epoch: 3 [9011200/25340216 (36%)]\tLoss: 1.293996\n",
            "Train Epoch: 3 [9420800/25340216 (37%)]\tLoss: 1.332992\n",
            "Train Epoch: 3 [9830400/25340216 (39%)]\tLoss: 1.392278\n",
            "Train Epoch: 3 [10240000/25340216 (40%)]\tLoss: 1.396402\n",
            "Train Epoch: 3 [10649600/25340216 (42%)]\tLoss: 1.433851\n",
            "Train Epoch: 3 [11059200/25340216 (44%)]\tLoss: 1.369289\n",
            "Train Epoch: 3 [11468800/25340216 (45%)]\tLoss: 1.338852\n",
            "Train Epoch: 3 [11878400/25340216 (47%)]\tLoss: 1.367114\n",
            "Train Epoch: 3 [12288000/25340216 (48%)]\tLoss: 1.331377\n",
            "Train Epoch: 3 [12697600/25340216 (50%)]\tLoss: 1.299070\n",
            "Train Epoch: 3 [13107200/25340216 (52%)]\tLoss: 1.355042\n",
            "Train Epoch: 3 [13516800/25340216 (53%)]\tLoss: 1.439778\n",
            "Train Epoch: 3 [13926400/25340216 (55%)]\tLoss: 1.371053\n",
            "Train Epoch: 3 [14336000/25340216 (57%)]\tLoss: 1.400696\n",
            "Train Epoch: 3 [14745600/25340216 (58%)]\tLoss: 1.376477\n",
            "Train Epoch: 3 [15155200/25340216 (60%)]\tLoss: 1.344064\n",
            "Train Epoch: 3 [15564800/25340216 (61%)]\tLoss: 1.393931\n",
            "Train Epoch: 3 [15974400/25340216 (63%)]\tLoss: 1.353631\n",
            "Train Epoch: 3 [16384000/25340216 (65%)]\tLoss: 1.387715\n",
            "Train Epoch: 3 [16793600/25340216 (66%)]\tLoss: 1.364231\n",
            "Train Epoch: 3 [17203200/25340216 (68%)]\tLoss: 1.399629\n",
            "Train Epoch: 3 [17612800/25340216 (70%)]\tLoss: 1.393000\n",
            "Train Epoch: 3 [18022400/25340216 (71%)]\tLoss: 1.400438\n",
            "Train Epoch: 3 [18432000/25340216 (73%)]\tLoss: 1.317364\n",
            "Train Epoch: 3 [18841600/25340216 (74%)]\tLoss: 1.367280\n",
            "Train Epoch: 3 [19251200/25340216 (76%)]\tLoss: 1.289129\n",
            "Train Epoch: 3 [19660800/25340216 (78%)]\tLoss: 1.391902\n",
            "Train Epoch: 3 [20070400/25340216 (79%)]\tLoss: 1.272971\n",
            "Train Epoch: 3 [20480000/25340216 (81%)]\tLoss: 1.406079\n",
            "Train Epoch: 3 [20889600/25340216 (82%)]\tLoss: 1.304086\n",
            "Train Epoch: 3 [21299200/25340216 (84%)]\tLoss: 1.371794\n",
            "Train Epoch: 3 [21708800/25340216 (86%)]\tLoss: 1.331304\n",
            "Train Epoch: 3 [22118400/25340216 (87%)]\tLoss: 1.369191\n",
            "Train Epoch: 3 [22528000/25340216 (89%)]\tLoss: 1.382213\n",
            "Train Epoch: 3 [22937600/25340216 (91%)]\tLoss: 1.375261\n",
            "Train Epoch: 3 [23347200/25340216 (92%)]\tLoss: 1.284082\n",
            "Train Epoch: 3 [23756800/25340216 (94%)]\tLoss: 1.401554\n",
            "Train Epoch: 3 [24166400/25340216 (95%)]\tLoss: 1.369677\n",
            "Train Epoch: 3 [24576000/25340216 (97%)]\tLoss: 1.379798\n",
            "Train Epoch: 3 [24985600/25340216 (99%)]\tLoss: 1.290470\n",
            "Train Epoch: 3 [0/10850918 (0%)]\tLoss: 1.387142\n",
            "Train Epoch: 3 [409600/10850918 (4%)]\tLoss: 1.314837\n",
            "Train Epoch: 3 [819200/10850918 (8%)]\tLoss: 1.356021\n",
            "Train Epoch: 3 [1228800/10850918 (11%)]\tLoss: 1.314476\n",
            "Train Epoch: 3 [1638400/10850918 (15%)]\tLoss: 1.258033\n",
            "Train Epoch: 3 [2048000/10850918 (19%)]\tLoss: 1.358150\n",
            "Train Epoch: 3 [2457600/10850918 (23%)]\tLoss: 1.359314\n",
            "Train Epoch: 3 [2867200/10850918 (26%)]\tLoss: 1.311440\n",
            "Train Epoch: 3 [3276800/10850918 (30%)]\tLoss: 1.351823\n",
            "Train Epoch: 3 [3686400/10850918 (34%)]\tLoss: 1.359033\n",
            "Train Epoch: 3 [4096000/10850918 (38%)]\tLoss: 1.328660\n",
            "Train Epoch: 3 [4505600/10850918 (42%)]\tLoss: 1.338521\n",
            "Train Epoch: 3 [4915200/10850918 (45%)]\tLoss: 1.348525\n",
            "Train Epoch: 3 [5324800/10850918 (49%)]\tLoss: 1.305186\n",
            "Train Epoch: 3 [5734400/10850918 (53%)]\tLoss: 1.363267\n",
            "Train Epoch: 3 [6144000/10850918 (57%)]\tLoss: 1.314035\n",
            "Train Epoch: 3 [6553600/10850918 (60%)]\tLoss: 1.352673\n",
            "Train Epoch: 3 [6963200/10850918 (64%)]\tLoss: 1.310171\n",
            "Train Epoch: 3 [7372800/10850918 (68%)]\tLoss: 1.290598\n",
            "Train Epoch: 3 [7782400/10850918 (72%)]\tLoss: 1.274833\n",
            "Train Epoch: 3 [8192000/10850918 (75%)]\tLoss: 1.286076\n",
            "Train Epoch: 3 [8601600/10850918 (79%)]\tLoss: 1.326727\n",
            "Train Epoch: 3 [9011200/10850918 (83%)]\tLoss: 1.327110\n",
            "Train Epoch: 3 [9420800/10850918 (87%)]\tLoss: 1.313203\n",
            "Train Epoch: 3 [9830400/10850918 (91%)]\tLoss: 1.380181\n",
            "Train Epoch: 3 [10240000/10850918 (94%)]\tLoss: 1.297417\n",
            "Train Epoch: 3 [10649600/10850918 (98%)]\tLoss: 1.337598\n",
            "Train Epoch: 3 [0/1937496 (0%)]\tLoss: 1.435609\n",
            "Train Epoch: 3 [409600/1937496 (21%)]\tLoss: 1.392066\n",
            "Train Epoch: 3 [819200/1937496 (42%)]\tLoss: 1.343658\n",
            "Train Epoch: 3 [1228800/1937496 (63%)]\tLoss: 1.302645\n",
            "Train Epoch: 3 [1638400/1937496 (84%)]\tLoss: 1.295730\n",
            "Dev accuracy:  0.9205639650352827\n",
            " 50.69762250185013 minutes\n",
            "Train Epoch: 4 [0/25340216 (0%)]\tLoss: 1.394313\n",
            "Train Epoch: 4 [409600/25340216 (2%)]\tLoss: 1.322414\n",
            "Train Epoch: 4 [819200/25340216 (3%)]\tLoss: 1.427138\n",
            "Train Epoch: 4 [1228800/25340216 (5%)]\tLoss: 1.341854\n",
            "Train Epoch: 4 [1638400/25340216 (6%)]\tLoss: 1.377523\n",
            "Train Epoch: 4 [2048000/25340216 (8%)]\tLoss: 1.350434\n",
            "Train Epoch: 4 [2457600/25340216 (10%)]\tLoss: 1.377430\n",
            "Train Epoch: 4 [2867200/25340216 (11%)]\tLoss: 1.326415\n",
            "Train Epoch: 4 [3276800/25340216 (13%)]\tLoss: 1.296443\n",
            "Train Epoch: 4 [3686400/25340216 (15%)]\tLoss: 1.315476\n",
            "Train Epoch: 4 [4096000/25340216 (16%)]\tLoss: 1.353675\n",
            "Train Epoch: 4 [4505600/25340216 (18%)]\tLoss: 1.367528\n",
            "Train Epoch: 4 [4915200/25340216 (19%)]\tLoss: 1.303284\n",
            "Train Epoch: 4 [5324800/25340216 (21%)]\tLoss: 1.333396\n",
            "Train Epoch: 4 [5734400/25340216 (23%)]\tLoss: 1.365780\n",
            "Train Epoch: 4 [6144000/25340216 (24%)]\tLoss: 1.312720\n",
            "Train Epoch: 4 [6553600/25340216 (26%)]\tLoss: 1.389544\n",
            "Train Epoch: 4 [6963200/25340216 (27%)]\tLoss: 1.289663\n",
            "Train Epoch: 4 [7372800/25340216 (29%)]\tLoss: 1.405771\n",
            "Train Epoch: 4 [7782400/25340216 (31%)]\tLoss: 1.355439\n",
            "Train Epoch: 4 [8192000/25340216 (32%)]\tLoss: 1.320153\n",
            "Train Epoch: 4 [8601600/25340216 (34%)]\tLoss: 1.365360\n",
            "Train Epoch: 4 [9011200/25340216 (36%)]\tLoss: 1.331394\n",
            "Train Epoch: 4 [9420800/25340216 (37%)]\tLoss: 1.311089\n",
            "Train Epoch: 4 [9830400/25340216 (39%)]\tLoss: 1.286131\n",
            "Train Epoch: 4 [10240000/25340216 (40%)]\tLoss: 1.387172\n",
            "Train Epoch: 4 [10649600/25340216 (42%)]\tLoss: 1.314093\n",
            "Train Epoch: 4 [11059200/25340216 (44%)]\tLoss: 1.361391\n",
            "Train Epoch: 4 [11468800/25340216 (45%)]\tLoss: 1.291319\n",
            "Train Epoch: 4 [11878400/25340216 (47%)]\tLoss: 1.346482\n",
            "Train Epoch: 4 [12288000/25340216 (48%)]\tLoss: 1.346699\n",
            "Train Epoch: 4 [12697600/25340216 (50%)]\tLoss: 1.301207\n",
            "Train Epoch: 4 [13107200/25340216 (52%)]\tLoss: 1.305709\n",
            "Train Epoch: 4 [13516800/25340216 (53%)]\tLoss: 1.297917\n",
            "Train Epoch: 4 [13926400/25340216 (55%)]\tLoss: 1.324019\n",
            "Train Epoch: 4 [14336000/25340216 (57%)]\tLoss: 1.311475\n",
            "Train Epoch: 4 [14745600/25340216 (58%)]\tLoss: 1.319460\n",
            "Train Epoch: 4 [15155200/25340216 (60%)]\tLoss: 1.354206\n",
            "Train Epoch: 4 [15564800/25340216 (61%)]\tLoss: 1.248396\n",
            "Train Epoch: 4 [15974400/25340216 (63%)]\tLoss: 1.308545\n",
            "Train Epoch: 4 [16384000/25340216 (65%)]\tLoss: 1.285457\n",
            "Train Epoch: 4 [16793600/25340216 (66%)]\tLoss: 1.321052\n",
            "Train Epoch: 4 [17203200/25340216 (68%)]\tLoss: 1.271260\n",
            "Train Epoch: 4 [17612800/25340216 (70%)]\tLoss: 1.400307\n",
            "Train Epoch: 4 [18022400/25340216 (71%)]\tLoss: 1.365359\n",
            "Train Epoch: 4 [18432000/25340216 (73%)]\tLoss: 1.322300\n",
            "Train Epoch: 4 [18841600/25340216 (74%)]\tLoss: 1.346094\n",
            "Train Epoch: 4 [19251200/25340216 (76%)]\tLoss: 1.306945\n",
            "Train Epoch: 4 [19660800/25340216 (78%)]\tLoss: 1.359023\n",
            "Train Epoch: 4 [20070400/25340216 (79%)]\tLoss: 1.310973\n",
            "Train Epoch: 4 [20480000/25340216 (81%)]\tLoss: 1.346960\n",
            "Train Epoch: 4 [20889600/25340216 (82%)]\tLoss: 1.337605\n",
            "Train Epoch: 4 [21299200/25340216 (84%)]\tLoss: 1.346574\n",
            "Train Epoch: 4 [21708800/25340216 (86%)]\tLoss: 1.349457\n",
            "Train Epoch: 4 [22118400/25340216 (87%)]\tLoss: 1.275309\n",
            "Train Epoch: 4 [22528000/25340216 (89%)]\tLoss: 1.354139\n",
            "Train Epoch: 4 [22937600/25340216 (91%)]\tLoss: 1.360233\n",
            "Train Epoch: 4 [23347200/25340216 (92%)]\tLoss: 1.322176\n",
            "Train Epoch: 4 [23756800/25340216 (94%)]\tLoss: 1.335791\n",
            "Train Epoch: 4 [24166400/25340216 (95%)]\tLoss: 1.304312\n",
            "Train Epoch: 4 [24576000/25340216 (97%)]\tLoss: 1.352181\n",
            "Train Epoch: 4 [24985600/25340216 (99%)]\tLoss: 1.301529\n",
            "Train Epoch: 4 [0/10850918 (0%)]\tLoss: 1.375894\n",
            "Train Epoch: 4 [409600/10850918 (4%)]\tLoss: 1.318108\n",
            "Train Epoch: 4 [819200/10850918 (8%)]\tLoss: 1.306853\n",
            "Train Epoch: 4 [1228800/10850918 (11%)]\tLoss: 1.316229\n",
            "Train Epoch: 4 [1638400/10850918 (15%)]\tLoss: 1.276378\n",
            "Train Epoch: 4 [2048000/10850918 (19%)]\tLoss: 1.310858\n",
            "Train Epoch: 4 [2457600/10850918 (23%)]\tLoss: 1.409106\n",
            "Train Epoch: 4 [2867200/10850918 (26%)]\tLoss: 1.312675\n",
            "Train Epoch: 4 [3276800/10850918 (30%)]\tLoss: 1.280696\n",
            "Train Epoch: 4 [3686400/10850918 (34%)]\tLoss: 1.377248\n",
            "Train Epoch: 4 [4096000/10850918 (38%)]\tLoss: 1.339644\n",
            "Train Epoch: 4 [4505600/10850918 (42%)]\tLoss: 1.340006\n",
            "Train Epoch: 4 [4915200/10850918 (45%)]\tLoss: 1.315579\n",
            "Train Epoch: 4 [5324800/10850918 (49%)]\tLoss: 1.356926\n",
            "Train Epoch: 4 [5734400/10850918 (53%)]\tLoss: 1.255019\n",
            "Train Epoch: 4 [6144000/10850918 (57%)]\tLoss: 1.281634\n",
            "Train Epoch: 4 [6553600/10850918 (60%)]\tLoss: 1.254454\n",
            "Train Epoch: 4 [6963200/10850918 (64%)]\tLoss: 1.302713\n",
            "Train Epoch: 4 [7372800/10850918 (68%)]\tLoss: 1.234715\n",
            "Train Epoch: 4 [7782400/10850918 (72%)]\tLoss: 1.299106\n",
            "Train Epoch: 4 [8192000/10850918 (75%)]\tLoss: 1.335742\n",
            "Train Epoch: 4 [8601600/10850918 (79%)]\tLoss: 1.293077\n",
            "Train Epoch: 4 [9011200/10850918 (83%)]\tLoss: 1.289234\n",
            "Train Epoch: 4 [9420800/10850918 (87%)]\tLoss: 1.309241\n",
            "Train Epoch: 4 [9830400/10850918 (91%)]\tLoss: 1.273166\n",
            "Train Epoch: 4 [10240000/10850918 (94%)]\tLoss: 1.285814\n",
            "Train Epoch: 4 [10649600/10850918 (98%)]\tLoss: 1.311773\n",
            "Train Epoch: 4 [0/1937496 (0%)]\tLoss: 1.436548\n",
            "Train Epoch: 4 [409600/1937496 (21%)]\tLoss: 1.271131\n",
            "Train Epoch: 4 [819200/1937496 (42%)]\tLoss: 1.324580\n",
            "Train Epoch: 4 [1228800/1937496 (63%)]\tLoss: 1.247705\n",
            "Train Epoch: 4 [1638400/1937496 (84%)]\tLoss: 1.256126\n",
            "Dev accuracy:  0.9273226886661959\n",
            " 50.44524863958359 minutes\n",
            "Train Epoch: 5 [0/25340216 (0%)]\tLoss: 1.335184\n",
            "Train Epoch: 5 [409600/25340216 (2%)]\tLoss: 1.307995\n",
            "Train Epoch: 5 [819200/25340216 (3%)]\tLoss: 1.307839\n",
            "Train Epoch: 5 [1228800/25340216 (5%)]\tLoss: 1.298902\n",
            "Train Epoch: 5 [1638400/25340216 (6%)]\tLoss: 1.297317\n",
            "Train Epoch: 5 [2048000/25340216 (8%)]\tLoss: 1.301716\n",
            "Train Epoch: 5 [2457600/25340216 (10%)]\tLoss: 1.368098\n",
            "Train Epoch: 5 [2867200/25340216 (11%)]\tLoss: 1.273167\n",
            "Train Epoch: 5 [3276800/25340216 (13%)]\tLoss: 1.359054\n",
            "Train Epoch: 5 [3686400/25340216 (15%)]\tLoss: 1.278516\n",
            "Train Epoch: 5 [4096000/25340216 (16%)]\tLoss: 1.256288\n",
            "Train Epoch: 5 [4505600/25340216 (18%)]\tLoss: 1.377033\n",
            "Train Epoch: 5 [4915200/25340216 (19%)]\tLoss: 1.287713\n",
            "Train Epoch: 5 [5324800/25340216 (21%)]\tLoss: 1.279207\n",
            "Train Epoch: 5 [5734400/25340216 (23%)]\tLoss: 1.343758\n",
            "Train Epoch: 5 [6144000/25340216 (24%)]\tLoss: 1.329629\n",
            "Train Epoch: 5 [6553600/25340216 (26%)]\tLoss: 1.277797\n",
            "Train Epoch: 5 [6963200/25340216 (27%)]\tLoss: 1.314902\n",
            "Train Epoch: 5 [7372800/25340216 (29%)]\tLoss: 1.260658\n",
            "Train Epoch: 5 [7782400/25340216 (31%)]\tLoss: 1.262957\n",
            "Train Epoch: 5 [8192000/25340216 (32%)]\tLoss: 1.349597\n",
            "Train Epoch: 5 [8601600/25340216 (34%)]\tLoss: 1.300015\n",
            "Train Epoch: 5 [9011200/25340216 (36%)]\tLoss: 1.326793\n",
            "Train Epoch: 5 [9420800/25340216 (37%)]\tLoss: 1.255363\n",
            "Train Epoch: 5 [9830400/25340216 (39%)]\tLoss: 1.291327\n",
            "Train Epoch: 5 [10240000/25340216 (40%)]\tLoss: 1.342621\n",
            "Train Epoch: 5 [10649600/25340216 (42%)]\tLoss: 1.243999\n",
            "Train Epoch: 5 [11059200/25340216 (44%)]\tLoss: 1.310397\n",
            "Train Epoch: 5 [11468800/25340216 (45%)]\tLoss: 1.330637\n",
            "Train Epoch: 5 [11878400/25340216 (47%)]\tLoss: 1.284852\n",
            "Train Epoch: 5 [12288000/25340216 (48%)]\tLoss: 1.299264\n",
            "Train Epoch: 5 [12697600/25340216 (50%)]\tLoss: 1.241446\n",
            "Train Epoch: 5 [13107200/25340216 (52%)]\tLoss: 1.327434\n",
            "Train Epoch: 5 [13516800/25340216 (53%)]\tLoss: 1.399344\n",
            "Train Epoch: 5 [13926400/25340216 (55%)]\tLoss: 1.370368\n",
            "Train Epoch: 5 [14336000/25340216 (57%)]\tLoss: 1.319933\n",
            "Train Epoch: 5 [14745600/25340216 (58%)]\tLoss: 1.229499\n",
            "Train Epoch: 5 [15155200/25340216 (60%)]\tLoss: 1.273762\n",
            "Train Epoch: 5 [15564800/25340216 (61%)]\tLoss: 1.319177\n",
            "Train Epoch: 5 [15974400/25340216 (63%)]\tLoss: 1.227896\n",
            "Train Epoch: 5 [16384000/25340216 (65%)]\tLoss: 1.279721\n",
            "Train Epoch: 5 [16793600/25340216 (66%)]\tLoss: 1.323116\n",
            "Train Epoch: 5 [17203200/25340216 (68%)]\tLoss: 1.262019\n",
            "Train Epoch: 5 [17612800/25340216 (70%)]\tLoss: 1.328400\n",
            "Train Epoch: 5 [18022400/25340216 (71%)]\tLoss: 1.278449\n",
            "Train Epoch: 5 [18432000/25340216 (73%)]\tLoss: 1.320172\n",
            "Train Epoch: 5 [18841600/25340216 (74%)]\tLoss: 1.270767\n",
            "Train Epoch: 5 [19251200/25340216 (76%)]\tLoss: 1.297483\n",
            "Train Epoch: 5 [19660800/25340216 (78%)]\tLoss: 1.282110\n",
            "Train Epoch: 5 [20070400/25340216 (79%)]\tLoss: 1.357359\n",
            "Train Epoch: 5 [20480000/25340216 (81%)]\tLoss: 1.375301\n",
            "Train Epoch: 5 [20889600/25340216 (82%)]\tLoss: 1.282826\n",
            "Train Epoch: 5 [21299200/25340216 (84%)]\tLoss: 1.247509\n",
            "Train Epoch: 5 [21708800/25340216 (86%)]\tLoss: 1.260720\n",
            "Train Epoch: 5 [22118400/25340216 (87%)]\tLoss: 1.303131\n",
            "Train Epoch: 5 [22528000/25340216 (89%)]\tLoss: 1.308030\n",
            "Train Epoch: 5 [22937600/25340216 (91%)]\tLoss: 1.258991\n",
            "Train Epoch: 5 [23347200/25340216 (92%)]\tLoss: 1.290595\n",
            "Train Epoch: 5 [23756800/25340216 (94%)]\tLoss: 1.323019\n",
            "Train Epoch: 5 [24166400/25340216 (95%)]\tLoss: 1.247222\n",
            "Train Epoch: 5 [24576000/25340216 (97%)]\tLoss: 1.262513\n",
            "Train Epoch: 5 [24985600/25340216 (99%)]\tLoss: 1.350831\n",
            "Train Epoch: 5 [0/10850918 (0%)]\tLoss: 1.300318\n",
            "Train Epoch: 5 [409600/10850918 (4%)]\tLoss: 1.339287\n",
            "Train Epoch: 5 [819200/10850918 (8%)]\tLoss: 1.367807\n",
            "Train Epoch: 5 [1228800/10850918 (11%)]\tLoss: 1.332791\n",
            "Train Epoch: 5 [1638400/10850918 (15%)]\tLoss: 1.260596\n",
            "Train Epoch: 5 [2048000/10850918 (19%)]\tLoss: 1.363609\n",
            "Train Epoch: 5 [2457600/10850918 (23%)]\tLoss: 1.219915\n",
            "Train Epoch: 5 [2867200/10850918 (26%)]\tLoss: 1.273680\n",
            "Train Epoch: 5 [3276800/10850918 (30%)]\tLoss: 1.245079\n",
            "Train Epoch: 5 [3686400/10850918 (34%)]\tLoss: 1.274743\n",
            "Train Epoch: 5 [4096000/10850918 (38%)]\tLoss: 1.219000\n",
            "Train Epoch: 5 [4505600/10850918 (42%)]\tLoss: 1.284139\n",
            "Train Epoch: 5 [4915200/10850918 (45%)]\tLoss: 1.268092\n",
            "Train Epoch: 5 [5324800/10850918 (49%)]\tLoss: 1.327056\n",
            "Train Epoch: 5 [5734400/10850918 (53%)]\tLoss: 1.326181\n",
            "Train Epoch: 5 [6144000/10850918 (57%)]\tLoss: 1.294948\n",
            "Train Epoch: 5 [6553600/10850918 (60%)]\tLoss: 1.264577\n",
            "Train Epoch: 5 [6963200/10850918 (64%)]\tLoss: 1.269006\n",
            "Train Epoch: 5 [7372800/10850918 (68%)]\tLoss: 1.262570\n",
            "Train Epoch: 5 [7782400/10850918 (72%)]\tLoss: 1.333984\n",
            "Train Epoch: 5 [8192000/10850918 (75%)]\tLoss: 1.284992\n",
            "Train Epoch: 5 [8601600/10850918 (79%)]\tLoss: 1.308354\n",
            "Train Epoch: 5 [9011200/10850918 (83%)]\tLoss: 1.258877\n",
            "Train Epoch: 5 [9420800/10850918 (87%)]\tLoss: 1.274027\n",
            "Train Epoch: 5 [9830400/10850918 (91%)]\tLoss: 1.270993\n",
            "Train Epoch: 5 [10240000/10850918 (94%)]\tLoss: 1.259392\n",
            "Train Epoch: 5 [10649600/10850918 (98%)]\tLoss: 1.282406\n",
            "Train Epoch: 5 [0/1937496 (0%)]\tLoss: 1.381478\n",
            "Train Epoch: 5 [409600/1937496 (21%)]\tLoss: 1.295119\n",
            "Train Epoch: 5 [819200/1937496 (42%)]\tLoss: 1.239331\n",
            "Train Epoch: 5 [1228800/1937496 (63%)]\tLoss: 1.329236\n",
            "Train Epoch: 5 [1638400/1937496 (84%)]\tLoss: 1.263739\n",
            "Dev accuracy:  0.9319678595465487\n",
            " 50.211284546057385 minutes\n",
            "Train Epoch: 6 [0/25340216 (0%)]\tLoss: 1.300671\n",
            "Train Epoch: 6 [409600/25340216 (2%)]\tLoss: 1.342699\n",
            "Train Epoch: 6 [819200/25340216 (3%)]\tLoss: 1.310436\n",
            "Train Epoch: 6 [1228800/25340216 (5%)]\tLoss: 1.334168\n",
            "Train Epoch: 6 [1638400/25340216 (6%)]\tLoss: 1.268466\n",
            "Train Epoch: 6 [2048000/25340216 (8%)]\tLoss: 1.240516\n",
            "Train Epoch: 6 [2457600/25340216 (10%)]\tLoss: 1.300393\n",
            "Train Epoch: 6 [2867200/25340216 (11%)]\tLoss: 1.293810\n",
            "Train Epoch: 6 [3276800/25340216 (13%)]\tLoss: 1.362856\n",
            "Train Epoch: 6 [3686400/25340216 (15%)]\tLoss: 1.282794\n",
            "Train Epoch: 6 [4096000/25340216 (16%)]\tLoss: 1.240443\n",
            "Train Epoch: 6 [4505600/25340216 (18%)]\tLoss: 1.260907\n",
            "Train Epoch: 6 [4915200/25340216 (19%)]\tLoss: 1.336661\n",
            "Train Epoch: 6 [5324800/25340216 (21%)]\tLoss: 1.240186\n",
            "Train Epoch: 6 [5734400/25340216 (23%)]\tLoss: 1.258778\n",
            "Train Epoch: 6 [6144000/25340216 (24%)]\tLoss: 1.353031\n",
            "Train Epoch: 6 [6553600/25340216 (26%)]\tLoss: 1.283057\n",
            "Train Epoch: 6 [6963200/25340216 (27%)]\tLoss: 1.308447\n",
            "Train Epoch: 6 [7372800/25340216 (29%)]\tLoss: 1.297472\n",
            "Train Epoch: 6 [7782400/25340216 (31%)]\tLoss: 1.316399\n",
            "Train Epoch: 6 [8192000/25340216 (32%)]\tLoss: 1.257988\n",
            "Train Epoch: 6 [8601600/25340216 (34%)]\tLoss: 1.279243\n",
            "Train Epoch: 6 [9011200/25340216 (36%)]\tLoss: 1.286998\n",
            "Train Epoch: 6 [9420800/25340216 (37%)]\tLoss: 1.246852\n",
            "Train Epoch: 6 [9830400/25340216 (39%)]\tLoss: 1.260732\n",
            "Train Epoch: 6 [10240000/25340216 (40%)]\tLoss: 1.309154\n",
            "Train Epoch: 6 [10649600/25340216 (42%)]\tLoss: 1.279327\n",
            "Train Epoch: 6 [11059200/25340216 (44%)]\tLoss: 1.217800\n",
            "Train Epoch: 6 [11468800/25340216 (45%)]\tLoss: 1.316034\n",
            "Train Epoch: 6 [11878400/25340216 (47%)]\tLoss: 1.354290\n",
            "Train Epoch: 6 [12288000/25340216 (48%)]\tLoss: 1.313971\n",
            "Train Epoch: 6 [12697600/25340216 (50%)]\tLoss: 1.307369\n",
            "Train Epoch: 6 [13107200/25340216 (52%)]\tLoss: 1.323833\n",
            "Train Epoch: 6 [13516800/25340216 (53%)]\tLoss: 1.242859\n",
            "Train Epoch: 6 [13926400/25340216 (55%)]\tLoss: 1.283232\n",
            "Train Epoch: 6 [14336000/25340216 (57%)]\tLoss: 1.376164\n",
            "Train Epoch: 6 [14745600/25340216 (58%)]\tLoss: 1.264343\n",
            "Train Epoch: 6 [15155200/25340216 (60%)]\tLoss: 1.253224\n",
            "Train Epoch: 6 [15564800/25340216 (61%)]\tLoss: 1.269873\n",
            "Train Epoch: 6 [15974400/25340216 (63%)]\tLoss: 1.249053\n",
            "Train Epoch: 6 [16384000/25340216 (65%)]\tLoss: 1.272215\n",
            "Train Epoch: 6 [16793600/25340216 (66%)]\tLoss: 1.241778\n",
            "Train Epoch: 6 [17203200/25340216 (68%)]\tLoss: 1.260548\n",
            "Train Epoch: 6 [17612800/25340216 (70%)]\tLoss: 1.273384\n",
            "Train Epoch: 6 [18022400/25340216 (71%)]\tLoss: 1.306755\n",
            "Train Epoch: 6 [18432000/25340216 (73%)]\tLoss: 1.322607\n",
            "Train Epoch: 6 [18841600/25340216 (74%)]\tLoss: 1.260927\n",
            "Train Epoch: 6 [19251200/25340216 (76%)]\tLoss: 1.242060\n",
            "Train Epoch: 6 [19660800/25340216 (78%)]\tLoss: 1.264934\n",
            "Train Epoch: 6 [20070400/25340216 (79%)]\tLoss: 1.297185\n",
            "Train Epoch: 6 [20480000/25340216 (81%)]\tLoss: 1.370111\n",
            "Train Epoch: 6 [20889600/25340216 (82%)]\tLoss: 1.285206\n",
            "Train Epoch: 6 [21299200/25340216 (84%)]\tLoss: 1.307481\n",
            "Train Epoch: 6 [21708800/25340216 (86%)]\tLoss: 1.247527\n",
            "Train Epoch: 6 [22118400/25340216 (87%)]\tLoss: 1.255526\n",
            "Train Epoch: 6 [22528000/25340216 (89%)]\tLoss: 1.313825\n",
            "Train Epoch: 6 [22937600/25340216 (91%)]\tLoss: 1.347200\n",
            "Train Epoch: 6 [23347200/25340216 (92%)]\tLoss: 1.362455\n",
            "Train Epoch: 6 [23756800/25340216 (94%)]\tLoss: 1.302788\n",
            "Train Epoch: 6 [24166400/25340216 (95%)]\tLoss: 1.237400\n",
            "Train Epoch: 6 [24576000/25340216 (97%)]\tLoss: 1.255086\n",
            "Train Epoch: 6 [24985600/25340216 (99%)]\tLoss: 1.225549\n",
            "Train Epoch: 6 [0/10850918 (0%)]\tLoss: 1.259045\n",
            "Train Epoch: 6 [409600/10850918 (4%)]\tLoss: 1.220471\n",
            "Train Epoch: 6 [819200/10850918 (8%)]\tLoss: 1.310867\n",
            "Train Epoch: 6 [1228800/10850918 (11%)]\tLoss: 1.272455\n",
            "Train Epoch: 6 [1638400/10850918 (15%)]\tLoss: 1.248393\n",
            "Train Epoch: 6 [2048000/10850918 (19%)]\tLoss: 1.300805\n",
            "Train Epoch: 6 [2457600/10850918 (23%)]\tLoss: 1.301518\n",
            "Train Epoch: 6 [2867200/10850918 (26%)]\tLoss: 1.252216\n",
            "Train Epoch: 6 [3276800/10850918 (30%)]\tLoss: 1.294700\n",
            "Train Epoch: 6 [3686400/10850918 (34%)]\tLoss: 1.289363\n",
            "Train Epoch: 6 [4096000/10850918 (38%)]\tLoss: 1.297568\n",
            "Train Epoch: 6 [4505600/10850918 (42%)]\tLoss: 1.227297\n",
            "Train Epoch: 6 [4915200/10850918 (45%)]\tLoss: 1.320545\n",
            "Train Epoch: 6 [5324800/10850918 (49%)]\tLoss: 1.250814\n",
            "Train Epoch: 6 [5734400/10850918 (53%)]\tLoss: 1.251804\n",
            "Train Epoch: 6 [6144000/10850918 (57%)]\tLoss: 1.287199\n",
            "Train Epoch: 6 [6553600/10850918 (60%)]\tLoss: 1.199318\n",
            "Train Epoch: 6 [6963200/10850918 (64%)]\tLoss: 1.252330\n",
            "Train Epoch: 6 [7372800/10850918 (68%)]\tLoss: 1.314802\n",
            "Train Epoch: 6 [7782400/10850918 (72%)]\tLoss: 1.269814\n",
            "Train Epoch: 6 [8192000/10850918 (75%)]\tLoss: 1.239573\n",
            "Train Epoch: 6 [8601600/10850918 (79%)]\tLoss: 1.274243\n",
            "Train Epoch: 6 [9011200/10850918 (83%)]\tLoss: 1.272467\n",
            "Train Epoch: 6 [9420800/10850918 (87%)]\tLoss: 1.262841\n",
            "Train Epoch: 6 [9830400/10850918 (91%)]\tLoss: 1.267075\n",
            "Train Epoch: 6 [10240000/10850918 (94%)]\tLoss: 1.342512\n",
            "Train Epoch: 6 [10649600/10850918 (98%)]\tLoss: 1.140425\n",
            "Train Epoch: 6 [0/1937496 (0%)]\tLoss: 1.348209\n",
            "Train Epoch: 6 [409600/1937496 (21%)]\tLoss: 1.303048\n",
            "Train Epoch: 6 [819200/1937496 (42%)]\tLoss: 1.247353\n",
            "Train Epoch: 6 [1228800/1937496 (63%)]\tLoss: 1.257676\n",
            "Train Epoch: 6 [1638400/1937496 (84%)]\tLoss: 1.252756\n",
            "Dev accuracy:  0.935669544608092\n",
            " 50.30734889904658 minutes\n",
            "Train Epoch: 7 [0/25340216 (0%)]\tLoss: 1.317169\n",
            "Train Epoch: 7 [409600/25340216 (2%)]\tLoss: 1.290192\n",
            "Train Epoch: 7 [819200/25340216 (3%)]\tLoss: 1.290241\n",
            "Train Epoch: 7 [1228800/25340216 (5%)]\tLoss: 1.338951\n",
            "Train Epoch: 7 [1638400/25340216 (6%)]\tLoss: 1.280972\n",
            "Train Epoch: 7 [2048000/25340216 (8%)]\tLoss: 1.218472\n",
            "Train Epoch: 7 [2457600/25340216 (10%)]\tLoss: 1.303841\n",
            "Train Epoch: 7 [2867200/25340216 (11%)]\tLoss: 1.271890\n",
            "Train Epoch: 7 [3276800/25340216 (13%)]\tLoss: 1.288568\n",
            "Train Epoch: 7 [3686400/25340216 (15%)]\tLoss: 1.238811\n",
            "Train Epoch: 7 [4096000/25340216 (16%)]\tLoss: 1.321160\n",
            "Train Epoch: 7 [4505600/25340216 (18%)]\tLoss: 1.248907\n",
            "Train Epoch: 7 [4915200/25340216 (19%)]\tLoss: 1.380648\n",
            "Train Epoch: 7 [5324800/25340216 (21%)]\tLoss: 1.308923\n",
            "Train Epoch: 7 [5734400/25340216 (23%)]\tLoss: 1.293105\n",
            "Train Epoch: 7 [6144000/25340216 (24%)]\tLoss: 1.263799\n",
            "Train Epoch: 7 [6553600/25340216 (26%)]\tLoss: 1.298549\n",
            "Train Epoch: 7 [6963200/25340216 (27%)]\tLoss: 1.351093\n",
            "Train Epoch: 7 [7372800/25340216 (29%)]\tLoss: 1.299263\n",
            "Train Epoch: 7 [7782400/25340216 (31%)]\tLoss: 1.247388\n",
            "Train Epoch: 7 [8192000/25340216 (32%)]\tLoss: 1.243147\n",
            "Train Epoch: 7 [8601600/25340216 (34%)]\tLoss: 1.190325\n",
            "Train Epoch: 7 [9011200/25340216 (36%)]\tLoss: 1.259548\n",
            "Train Epoch: 7 [9420800/25340216 (37%)]\tLoss: 1.295008\n",
            "Train Epoch: 7 [9830400/25340216 (39%)]\tLoss: 1.255176\n",
            "Train Epoch: 7 [10240000/25340216 (40%)]\tLoss: 1.201486\n",
            "Train Epoch: 7 [10649600/25340216 (42%)]\tLoss: 1.285479\n",
            "Train Epoch: 7 [11059200/25340216 (44%)]\tLoss: 1.212456\n",
            "Train Epoch: 7 [11468800/25340216 (45%)]\tLoss: 1.281072\n",
            "Train Epoch: 7 [11878400/25340216 (47%)]\tLoss: 1.356309\n",
            "Train Epoch: 7 [12288000/25340216 (48%)]\tLoss: 1.207392\n",
            "Train Epoch: 7 [12697600/25340216 (50%)]\tLoss: 1.305680\n",
            "Train Epoch: 7 [13107200/25340216 (52%)]\tLoss: 1.279410\n",
            "Train Epoch: 7 [13516800/25340216 (53%)]\tLoss: 1.299730\n",
            "Train Epoch: 7 [13926400/25340216 (55%)]\tLoss: 1.258286\n",
            "Train Epoch: 7 [14336000/25340216 (57%)]\tLoss: 1.315225\n",
            "Train Epoch: 7 [14745600/25340216 (58%)]\tLoss: 1.294329\n",
            "Train Epoch: 7 [15155200/25340216 (60%)]\tLoss: 1.323241\n",
            "Train Epoch: 7 [15564800/25340216 (61%)]\tLoss: 1.359341\n",
            "Train Epoch: 7 [15974400/25340216 (63%)]\tLoss: 1.296660\n",
            "Train Epoch: 7 [16384000/25340216 (65%)]\tLoss: 1.307636\n",
            "Train Epoch: 7 [16793600/25340216 (66%)]\tLoss: 1.306166\n",
            "Train Epoch: 7 [17203200/25340216 (68%)]\tLoss: 1.255987\n",
            "Train Epoch: 7 [17612800/25340216 (70%)]\tLoss: 1.251626\n",
            "Train Epoch: 7 [18022400/25340216 (71%)]\tLoss: 1.187242\n",
            "Train Epoch: 7 [18432000/25340216 (73%)]\tLoss: 1.325817\n",
            "Train Epoch: 7 [18841600/25340216 (74%)]\tLoss: 1.269924\n",
            "Train Epoch: 7 [19251200/25340216 (76%)]\tLoss: 1.238427\n",
            "Train Epoch: 7 [19660800/25340216 (78%)]\tLoss: 1.246880\n",
            "Train Epoch: 7 [20070400/25340216 (79%)]\tLoss: 1.255832\n",
            "Train Epoch: 7 [20480000/25340216 (81%)]\tLoss: 1.295155\n",
            "Train Epoch: 7 [20889600/25340216 (82%)]\tLoss: 1.250703\n",
            "Train Epoch: 7 [21299200/25340216 (84%)]\tLoss: 1.217907\n",
            "Train Epoch: 7 [21708800/25340216 (86%)]\tLoss: 1.276997\n",
            "Train Epoch: 7 [22118400/25340216 (87%)]\tLoss: 1.350899\n",
            "Train Epoch: 7 [22528000/25340216 (89%)]\tLoss: 1.278154\n",
            "Train Epoch: 7 [22937600/25340216 (91%)]\tLoss: 1.326449\n",
            "Train Epoch: 7 [23347200/25340216 (92%)]\tLoss: 1.308897\n",
            "Train Epoch: 7 [23756800/25340216 (94%)]\tLoss: 1.337346\n",
            "Train Epoch: 7 [24166400/25340216 (95%)]\tLoss: 1.249936\n",
            "Train Epoch: 7 [24576000/25340216 (97%)]\tLoss: 1.267833\n",
            "Train Epoch: 7 [24985600/25340216 (99%)]\tLoss: 1.304484\n",
            "Train Epoch: 7 [0/10850918 (0%)]\tLoss: 1.331507\n",
            "Train Epoch: 7 [409600/10850918 (4%)]\tLoss: 1.295940\n",
            "Train Epoch: 7 [819200/10850918 (8%)]\tLoss: 1.299148\n",
            "Train Epoch: 7 [1228800/10850918 (11%)]\tLoss: 1.236685\n",
            "Train Epoch: 7 [1638400/10850918 (15%)]\tLoss: 1.247433\n",
            "Train Epoch: 7 [2048000/10850918 (19%)]\tLoss: 1.245355\n",
            "Train Epoch: 7 [2457600/10850918 (23%)]\tLoss: 1.384240\n",
            "Train Epoch: 7 [2867200/10850918 (26%)]\tLoss: 1.291095\n",
            "Train Epoch: 7 [3276800/10850918 (30%)]\tLoss: 1.169180\n",
            "Train Epoch: 7 [3686400/10850918 (34%)]\tLoss: 1.329150\n",
            "Train Epoch: 7 [4096000/10850918 (38%)]\tLoss: 1.274175\n",
            "Train Epoch: 7 [4505600/10850918 (42%)]\tLoss: 1.307432\n",
            "Train Epoch: 7 [4915200/10850918 (45%)]\tLoss: 1.273426\n",
            "Train Epoch: 7 [5324800/10850918 (49%)]\tLoss: 1.214967\n",
            "Train Epoch: 7 [5734400/10850918 (53%)]\tLoss: 1.249272\n",
            "Train Epoch: 7 [6144000/10850918 (57%)]\tLoss: 1.292800\n",
            "Train Epoch: 7 [6553600/10850918 (60%)]\tLoss: 1.262981\n",
            "Train Epoch: 7 [6963200/10850918 (64%)]\tLoss: 1.279328\n",
            "Train Epoch: 7 [7372800/10850918 (68%)]\tLoss: 1.200317\n",
            "Train Epoch: 7 [7782400/10850918 (72%)]\tLoss: 1.252159\n",
            "Train Epoch: 7 [8192000/10850918 (75%)]\tLoss: 1.249815\n",
            "Train Epoch: 7 [8601600/10850918 (79%)]\tLoss: 1.215466\n",
            "Train Epoch: 7 [9011200/10850918 (83%)]\tLoss: 1.300520\n",
            "Train Epoch: 7 [9420800/10850918 (87%)]\tLoss: 1.314005\n",
            "Train Epoch: 7 [9830400/10850918 (91%)]\tLoss: 1.273248\n",
            "Train Epoch: 7 [10240000/10850918 (94%)]\tLoss: 1.211191\n",
            "Train Epoch: 7 [10649600/10850918 (98%)]\tLoss: 1.210243\n",
            "Train Epoch: 7 [0/1937496 (0%)]\tLoss: 1.339846\n",
            "Train Epoch: 7 [409600/1937496 (21%)]\tLoss: 1.320639\n",
            "Train Epoch: 7 [819200/1937496 (42%)]\tLoss: 1.276093\n",
            "Train Epoch: 7 [1228800/1937496 (63%)]\tLoss: 1.142051\n",
            "Train Epoch: 7 [1638400/1937496 (84%)]\tLoss: 1.285387\n",
            "Dev accuracy:  0.9395255525688827\n",
            " 50.382195774714155 minutes\n",
            "Train Epoch: 8 [0/25340216 (0%)]\tLoss: 1.240339\n",
            "Train Epoch: 8 [409600/25340216 (2%)]\tLoss: 1.283134\n",
            "Train Epoch: 8 [819200/25340216 (3%)]\tLoss: 1.270396\n",
            "Train Epoch: 8 [1228800/25340216 (5%)]\tLoss: 1.295724\n",
            "Train Epoch: 8 [1638400/25340216 (6%)]\tLoss: 1.268407\n",
            "Train Epoch: 8 [2048000/25340216 (8%)]\tLoss: 1.230038\n",
            "Train Epoch: 8 [2457600/25340216 (10%)]\tLoss: 1.243618\n",
            "Train Epoch: 8 [2867200/25340216 (11%)]\tLoss: 1.262077\n",
            "Train Epoch: 8 [3276800/25340216 (13%)]\tLoss: 1.276412\n",
            "Train Epoch: 8 [3686400/25340216 (15%)]\tLoss: 1.234354\n",
            "Train Epoch: 8 [4096000/25340216 (16%)]\tLoss: 1.245201\n",
            "Train Epoch: 8 [4505600/25340216 (18%)]\tLoss: 1.263127\n",
            "Train Epoch: 8 [4915200/25340216 (19%)]\tLoss: 1.244520\n",
            "Train Epoch: 8 [5324800/25340216 (21%)]\tLoss: 1.233107\n",
            "Train Epoch: 8 [5734400/25340216 (23%)]\tLoss: 1.241189\n",
            "Train Epoch: 8 [6144000/25340216 (24%)]\tLoss: 1.270566\n",
            "Train Epoch: 8 [6553600/25340216 (26%)]\tLoss: 1.287225\n",
            "Train Epoch: 8 [6963200/25340216 (27%)]\tLoss: 1.273103\n",
            "Train Epoch: 8 [7372800/25340216 (29%)]\tLoss: 1.242786\n",
            "Train Epoch: 8 [7782400/25340216 (31%)]\tLoss: 1.242777\n",
            "Train Epoch: 8 [8192000/25340216 (32%)]\tLoss: 1.289962\n",
            "Train Epoch: 8 [8601600/25340216 (34%)]\tLoss: 1.200747\n",
            "Train Epoch: 8 [9011200/25340216 (36%)]\tLoss: 1.241774\n",
            "Train Epoch: 8 [9420800/25340216 (37%)]\tLoss: 1.266381\n",
            "Train Epoch: 8 [9830400/25340216 (39%)]\tLoss: 1.229570\n",
            "Train Epoch: 8 [10240000/25340216 (40%)]\tLoss: 1.295916\n",
            "Train Epoch: 8 [10649600/25340216 (42%)]\tLoss: 1.248346\n",
            "Train Epoch: 8 [11059200/25340216 (44%)]\tLoss: 1.275690\n",
            "Train Epoch: 8 [11468800/25340216 (45%)]\tLoss: 1.224065\n",
            "Train Epoch: 8 [11878400/25340216 (47%)]\tLoss: 1.277352\n",
            "Train Epoch: 8 [12288000/25340216 (48%)]\tLoss: 1.242903\n",
            "Train Epoch: 8 [12697600/25340216 (50%)]\tLoss: 1.356162\n",
            "Train Epoch: 8 [13107200/25340216 (52%)]\tLoss: 1.309893\n",
            "Train Epoch: 8 [13516800/25340216 (53%)]\tLoss: 1.231536\n",
            "Train Epoch: 8 [13926400/25340216 (55%)]\tLoss: 1.306778\n",
            "Train Epoch: 8 [14336000/25340216 (57%)]\tLoss: 1.254405\n",
            "Train Epoch: 8 [14745600/25340216 (58%)]\tLoss: 1.266587\n",
            "Train Epoch: 8 [15155200/25340216 (60%)]\tLoss: 1.207932\n",
            "Train Epoch: 8 [15564800/25340216 (61%)]\tLoss: 1.215457\n",
            "Train Epoch: 8 [15974400/25340216 (63%)]\tLoss: 1.262645\n",
            "Train Epoch: 8 [16384000/25340216 (65%)]\tLoss: 1.217912\n",
            "Train Epoch: 8 [16793600/25340216 (66%)]\tLoss: 1.212727\n",
            "Train Epoch: 8 [17203200/25340216 (68%)]\tLoss: 1.232481\n",
            "Train Epoch: 8 [17612800/25340216 (70%)]\tLoss: 1.292694\n",
            "Train Epoch: 8 [18022400/25340216 (71%)]\tLoss: 1.212056\n",
            "Train Epoch: 8 [18432000/25340216 (73%)]\tLoss: 1.224665\n",
            "Train Epoch: 8 [18841600/25340216 (74%)]\tLoss: 1.252541\n",
            "Train Epoch: 8 [19251200/25340216 (76%)]\tLoss: 1.279960\n",
            "Train Epoch: 8 [19660800/25340216 (78%)]\tLoss: 1.244494\n",
            "Train Epoch: 8 [20070400/25340216 (79%)]\tLoss: 1.260846\n",
            "Train Epoch: 8 [20480000/25340216 (81%)]\tLoss: 1.216805\n",
            "Train Epoch: 8 [20889600/25340216 (82%)]\tLoss: 1.273414\n",
            "Train Epoch: 8 [21299200/25340216 (84%)]\tLoss: 1.307930\n",
            "Train Epoch: 8 [21708800/25340216 (86%)]\tLoss: 1.274618\n",
            "Train Epoch: 8 [22118400/25340216 (87%)]\tLoss: 1.217267\n",
            "Train Epoch: 8 [22528000/25340216 (89%)]\tLoss: 1.222199\n",
            "Train Epoch: 8 [22937600/25340216 (91%)]\tLoss: 1.225069\n",
            "Train Epoch: 8 [23347200/25340216 (92%)]\tLoss: 1.156665\n",
            "Train Epoch: 8 [23756800/25340216 (94%)]\tLoss: 1.228275\n",
            "Train Epoch: 8 [24166400/25340216 (95%)]\tLoss: 1.289905\n",
            "Train Epoch: 8 [24576000/25340216 (97%)]\tLoss: 1.231654\n",
            "Train Epoch: 8 [24985600/25340216 (99%)]\tLoss: 1.258666\n",
            "Train Epoch: 8 [0/10850918 (0%)]\tLoss: 1.225115\n",
            "Train Epoch: 8 [409600/10850918 (4%)]\tLoss: 1.284296\n",
            "Train Epoch: 8 [819200/10850918 (8%)]\tLoss: 1.216117\n",
            "Train Epoch: 8 [1228800/10850918 (11%)]\tLoss: 1.202506\n",
            "Train Epoch: 8 [1638400/10850918 (15%)]\tLoss: 1.355946\n",
            "Train Epoch: 8 [2048000/10850918 (19%)]\tLoss: 1.317593\n",
            "Train Epoch: 8 [2457600/10850918 (23%)]\tLoss: 1.241751\n",
            "Train Epoch: 8 [2867200/10850918 (26%)]\tLoss: 1.212730\n",
            "Train Epoch: 8 [3276800/10850918 (30%)]\tLoss: 1.239359\n",
            "Train Epoch: 8 [3686400/10850918 (34%)]\tLoss: 1.207935\n",
            "Train Epoch: 8 [4096000/10850918 (38%)]\tLoss: 1.246246\n",
            "Train Epoch: 8 [4505600/10850918 (42%)]\tLoss: 1.284992\n",
            "Train Epoch: 8 [4915200/10850918 (45%)]\tLoss: 1.292956\n",
            "Train Epoch: 8 [5324800/10850918 (49%)]\tLoss: 1.275784\n",
            "Train Epoch: 8 [5734400/10850918 (53%)]\tLoss: 1.310891\n",
            "Train Epoch: 8 [6144000/10850918 (57%)]\tLoss: 1.233457\n",
            "Train Epoch: 8 [6553600/10850918 (60%)]\tLoss: 1.338373\n",
            "Train Epoch: 8 [6963200/10850918 (64%)]\tLoss: 1.261347\n",
            "Train Epoch: 8 [7372800/10850918 (68%)]\tLoss: 1.332516\n",
            "Train Epoch: 8 [7782400/10850918 (72%)]\tLoss: 1.230325\n",
            "Train Epoch: 8 [8192000/10850918 (75%)]\tLoss: 1.228129\n",
            "Train Epoch: 8 [8601600/10850918 (79%)]\tLoss: 1.259840\n",
            "Train Epoch: 8 [9011200/10850918 (83%)]\tLoss: 1.251856\n",
            "Train Epoch: 8 [9420800/10850918 (87%)]\tLoss: 1.268859\n",
            "Train Epoch: 8 [9830400/10850918 (91%)]\tLoss: 1.162371\n",
            "Train Epoch: 8 [10240000/10850918 (94%)]\tLoss: 1.187879\n",
            "Train Epoch: 8 [10649600/10850918 (98%)]\tLoss: 1.206312\n",
            "Train Epoch: 8 [0/1937496 (0%)]\tLoss: 1.292671\n",
            "Train Epoch: 8 [409600/1937496 (21%)]\tLoss: 1.235252\n",
            "Train Epoch: 8 [819200/1937496 (42%)]\tLoss: 1.226852\n",
            "Train Epoch: 8 [1228800/1937496 (63%)]\tLoss: 1.222032\n",
            "Train Epoch: 8 [1638400/1937496 (84%)]\tLoss: 1.278794\n",
            "Dev accuracy:  0.9419157510518731\n",
            " 50.48984555800756 minutes\n",
            "Train Epoch: 9 [0/25340216 (0%)]\tLoss: 1.302190\n",
            "Train Epoch: 9 [409600/25340216 (2%)]\tLoss: 1.245607\n",
            "Train Epoch: 9 [819200/25340216 (3%)]\tLoss: 1.212357\n",
            "Train Epoch: 9 [1228800/25340216 (5%)]\tLoss: 1.302956\n",
            "Train Epoch: 9 [1638400/25340216 (6%)]\tLoss: 1.205976\n",
            "Train Epoch: 9 [2048000/25340216 (8%)]\tLoss: 1.251650\n",
            "Train Epoch: 9 [2457600/25340216 (10%)]\tLoss: 1.264159\n",
            "Train Epoch: 9 [2867200/25340216 (11%)]\tLoss: 1.280065\n",
            "Train Epoch: 9 [3276800/25340216 (13%)]\tLoss: 1.297074\n",
            "Train Epoch: 9 [3686400/25340216 (15%)]\tLoss: 1.334981\n",
            "Train Epoch: 9 [4096000/25340216 (16%)]\tLoss: 1.251602\n",
            "Train Epoch: 9 [4505600/25340216 (18%)]\tLoss: 1.272235\n",
            "Train Epoch: 9 [4915200/25340216 (19%)]\tLoss: 1.297580\n",
            "Train Epoch: 9 [5324800/25340216 (21%)]\tLoss: 1.224934\n",
            "Train Epoch: 9 [5734400/25340216 (23%)]\tLoss: 1.292939\n",
            "Train Epoch: 9 [6144000/25340216 (24%)]\tLoss: 1.264798\n",
            "Train Epoch: 9 [6553600/25340216 (26%)]\tLoss: 1.220690\n",
            "Train Epoch: 9 [6963200/25340216 (27%)]\tLoss: 1.245316\n",
            "Train Epoch: 9 [7372800/25340216 (29%)]\tLoss: 1.266265\n",
            "Train Epoch: 9 [7782400/25340216 (31%)]\tLoss: 1.268346\n",
            "Train Epoch: 9 [8192000/25340216 (32%)]\tLoss: 1.243296\n",
            "Train Epoch: 9 [8601600/25340216 (34%)]\tLoss: 1.300476\n",
            "Train Epoch: 9 [9011200/25340216 (36%)]\tLoss: 1.246981\n",
            "Train Epoch: 9 [9420800/25340216 (37%)]\tLoss: 1.279165\n",
            "Train Epoch: 9 [9830400/25340216 (39%)]\tLoss: 1.267955\n",
            "Train Epoch: 9 [10240000/25340216 (40%)]\tLoss: 1.248585\n",
            "Train Epoch: 9 [10649600/25340216 (42%)]\tLoss: 1.302605\n",
            "Train Epoch: 9 [11059200/25340216 (44%)]\tLoss: 1.300891\n",
            "Train Epoch: 9 [11468800/25340216 (45%)]\tLoss: 1.256418\n",
            "Train Epoch: 9 [11878400/25340216 (47%)]\tLoss: 1.241080\n",
            "Train Epoch: 9 [12288000/25340216 (48%)]\tLoss: 1.228238\n",
            "Train Epoch: 9 [12697600/25340216 (50%)]\tLoss: 1.278049\n",
            "Train Epoch: 9 [13107200/25340216 (52%)]\tLoss: 1.195096\n",
            "Train Epoch: 9 [13516800/25340216 (53%)]\tLoss: 1.190965\n",
            "Train Epoch: 9 [13926400/25340216 (55%)]\tLoss: 1.328200\n",
            "Train Epoch: 9 [14336000/25340216 (57%)]\tLoss: 1.240390\n",
            "Train Epoch: 9 [14745600/25340216 (58%)]\tLoss: 1.226280\n",
            "Train Epoch: 9 [15155200/25340216 (60%)]\tLoss: 1.187820\n",
            "Train Epoch: 9 [15564800/25340216 (61%)]\tLoss: 1.234261\n",
            "Train Epoch: 9 [15974400/25340216 (63%)]\tLoss: 1.241209\n",
            "Train Epoch: 9 [16384000/25340216 (65%)]\tLoss: 1.193171\n",
            "Train Epoch: 9 [16793600/25340216 (66%)]\tLoss: 1.210224\n",
            "Train Epoch: 9 [17203200/25340216 (68%)]\tLoss: 1.256272\n",
            "Train Epoch: 9 [17612800/25340216 (70%)]\tLoss: 1.216193\n",
            "Train Epoch: 9 [18022400/25340216 (71%)]\tLoss: 1.260105\n",
            "Train Epoch: 9 [18432000/25340216 (73%)]\tLoss: 1.234444\n",
            "Train Epoch: 9 [18841600/25340216 (74%)]\tLoss: 1.197816\n",
            "Train Epoch: 9 [19251200/25340216 (76%)]\tLoss: 1.271386\n",
            "Train Epoch: 9 [19660800/25340216 (78%)]\tLoss: 1.267460\n",
            "Train Epoch: 9 [20070400/25340216 (79%)]\tLoss: 1.266741\n",
            "Train Epoch: 9 [20480000/25340216 (81%)]\tLoss: 1.211409\n",
            "Train Epoch: 9 [20889600/25340216 (82%)]\tLoss: 1.219040\n",
            "Train Epoch: 9 [21299200/25340216 (84%)]\tLoss: 1.201088\n",
            "Train Epoch: 9 [21708800/25340216 (86%)]\tLoss: 1.273278\n",
            "Train Epoch: 9 [22118400/25340216 (87%)]\tLoss: 1.188923\n",
            "Train Epoch: 9 [22528000/25340216 (89%)]\tLoss: 1.175648\n",
            "Train Epoch: 9 [22937600/25340216 (91%)]\tLoss: 1.275357\n",
            "Train Epoch: 9 [23347200/25340216 (92%)]\tLoss: 1.259748\n",
            "Train Epoch: 9 [23756800/25340216 (94%)]\tLoss: 1.245605\n",
            "Train Epoch: 9 [24166400/25340216 (95%)]\tLoss: 1.285371\n",
            "Train Epoch: 9 [24576000/25340216 (97%)]\tLoss: 1.163339\n",
            "Train Epoch: 9 [24985600/25340216 (99%)]\tLoss: 1.202591\n",
            "Train Epoch: 9 [0/10850918 (0%)]\tLoss: 1.290502\n",
            "Train Epoch: 9 [409600/10850918 (4%)]\tLoss: 1.299737\n",
            "Train Epoch: 9 [819200/10850918 (8%)]\tLoss: 1.261098\n",
            "Train Epoch: 9 [1228800/10850918 (11%)]\tLoss: 1.260154\n",
            "Train Epoch: 9 [1638400/10850918 (15%)]\tLoss: 1.201166\n",
            "Train Epoch: 9 [2048000/10850918 (19%)]\tLoss: 1.268442\n",
            "Train Epoch: 9 [2457600/10850918 (23%)]\tLoss: 1.260007\n",
            "Train Epoch: 9 [2867200/10850918 (26%)]\tLoss: 1.250910\n",
            "Train Epoch: 9 [3276800/10850918 (30%)]\tLoss: 1.200660\n",
            "Train Epoch: 9 [3686400/10850918 (34%)]\tLoss: 1.239525\n",
            "Train Epoch: 9 [4096000/10850918 (38%)]\tLoss: 1.240914\n",
            "Train Epoch: 9 [4505600/10850918 (42%)]\tLoss: 1.186351\n",
            "Train Epoch: 9 [4915200/10850918 (45%)]\tLoss: 1.246093\n",
            "Train Epoch: 9 [5324800/10850918 (49%)]\tLoss: 1.225024\n",
            "Train Epoch: 9 [5734400/10850918 (53%)]\tLoss: 1.327603\n",
            "Train Epoch: 9 [6144000/10850918 (57%)]\tLoss: 1.207336\n",
            "Train Epoch: 9 [6553600/10850918 (60%)]\tLoss: 1.145696\n",
            "Train Epoch: 9 [6963200/10850918 (64%)]\tLoss: 1.271422\n",
            "Train Epoch: 9 [7372800/10850918 (68%)]\tLoss: 1.240118\n",
            "Train Epoch: 9 [7782400/10850918 (72%)]\tLoss: 1.253166\n",
            "Train Epoch: 9 [8192000/10850918 (75%)]\tLoss: 1.229249\n",
            "Train Epoch: 9 [8601600/10850918 (79%)]\tLoss: 1.262754\n",
            "Train Epoch: 9 [9011200/10850918 (83%)]\tLoss: 1.276143\n",
            "Train Epoch: 9 [9420800/10850918 (87%)]\tLoss: 1.164837\n",
            "Train Epoch: 9 [9830400/10850918 (91%)]\tLoss: 1.152669\n",
            "Train Epoch: 9 [10240000/10850918 (94%)]\tLoss: 1.199969\n",
            "Train Epoch: 9 [10649600/10850918 (98%)]\tLoss: 1.240477\n",
            "Train Epoch: 9 [0/1937496 (0%)]\tLoss: 1.351371\n",
            "Train Epoch: 9 [409600/1937496 (21%)]\tLoss: 1.291205\n",
            "Train Epoch: 9 [819200/1937496 (42%)]\tLoss: 1.267173\n",
            "Train Epoch: 9 [1228800/1937496 (63%)]\tLoss: 1.254118\n",
            "Train Epoch: 9 [1638400/1937496 (84%)]\tLoss: 1.219565\n",
            "Dev accuracy:  0.9445872404381738\n",
            " 50.90144649744034 minutes\n",
            "Train Epoch: 10 [0/25340216 (0%)]\tLoss: 1.266297\n",
            "Train Epoch: 10 [409600/25340216 (2%)]\tLoss: 1.207380\n",
            "Train Epoch: 10 [819200/25340216 (3%)]\tLoss: 1.236878\n",
            "Train Epoch: 10 [1228800/25340216 (5%)]\tLoss: 1.287849\n",
            "Train Epoch: 10 [1638400/25340216 (6%)]\tLoss: 1.273366\n",
            "Train Epoch: 10 [2048000/25340216 (8%)]\tLoss: 1.344237\n",
            "Train Epoch: 10 [2457600/25340216 (10%)]\tLoss: 1.221865\n",
            "Train Epoch: 10 [2867200/25340216 (11%)]\tLoss: 1.238256\n",
            "Train Epoch: 10 [3276800/25340216 (13%)]\tLoss: 1.283218\n",
            "Train Epoch: 10 [3686400/25340216 (15%)]\tLoss: 1.253899\n",
            "Train Epoch: 10 [4096000/25340216 (16%)]\tLoss: 1.290689\n",
            "Train Epoch: 10 [4505600/25340216 (18%)]\tLoss: 1.148961\n",
            "Train Epoch: 10 [4915200/25340216 (19%)]\tLoss: 1.336180\n",
            "Train Epoch: 10 [5324800/25340216 (21%)]\tLoss: 1.242045\n",
            "Train Epoch: 10 [5734400/25340216 (23%)]\tLoss: 1.292584\n",
            "Train Epoch: 10 [6144000/25340216 (24%)]\tLoss: 1.157399\n",
            "Train Epoch: 10 [6553600/25340216 (26%)]\tLoss: 1.166478\n",
            "Train Epoch: 10 [6963200/25340216 (27%)]\tLoss: 1.183124\n",
            "Train Epoch: 10 [7372800/25340216 (29%)]\tLoss: 1.125498\n",
            "Train Epoch: 10 [7782400/25340216 (31%)]\tLoss: 1.262165\n",
            "Train Epoch: 10 [8192000/25340216 (32%)]\tLoss: 1.228097\n",
            "Train Epoch: 10 [8601600/25340216 (34%)]\tLoss: 1.177470\n",
            "Train Epoch: 10 [9011200/25340216 (36%)]\tLoss: 1.232889\n",
            "Train Epoch: 10 [9420800/25340216 (37%)]\tLoss: 1.253031\n",
            "Train Epoch: 10 [9830400/25340216 (39%)]\tLoss: 1.196315\n",
            "Train Epoch: 10 [10240000/25340216 (40%)]\tLoss: 1.276027\n",
            "Train Epoch: 10 [10649600/25340216 (42%)]\tLoss: 1.213800\n",
            "Train Epoch: 10 [11059200/25340216 (44%)]\tLoss: 1.187768\n",
            "Train Epoch: 10 [11468800/25340216 (45%)]\tLoss: 1.205793\n",
            "Train Epoch: 10 [11878400/25340216 (47%)]\tLoss: 1.258440\n",
            "Train Epoch: 10 [12288000/25340216 (48%)]\tLoss: 1.183883\n",
            "Train Epoch: 10 [12697600/25340216 (50%)]\tLoss: 1.305355\n",
            "Train Epoch: 10 [13107200/25340216 (52%)]\tLoss: 1.213175\n",
            "Train Epoch: 10 [13516800/25340216 (53%)]\tLoss: 1.276342\n",
            "Train Epoch: 10 [13926400/25340216 (55%)]\tLoss: 1.300265\n",
            "Train Epoch: 10 [14336000/25340216 (57%)]\tLoss: 1.264953\n",
            "Train Epoch: 10 [14745600/25340216 (58%)]\tLoss: 1.224923\n",
            "Train Epoch: 10 [15155200/25340216 (60%)]\tLoss: 1.296600\n",
            "Train Epoch: 10 [15564800/25340216 (61%)]\tLoss: 1.226672\n",
            "Train Epoch: 10 [15974400/25340216 (63%)]\tLoss: 1.232216\n",
            "Train Epoch: 10 [16384000/25340216 (65%)]\tLoss: 1.238034\n",
            "Train Epoch: 10 [16793600/25340216 (66%)]\tLoss: 1.207897\n",
            "Train Epoch: 10 [17203200/25340216 (68%)]\tLoss: 1.188406\n",
            "Train Epoch: 10 [17612800/25340216 (70%)]\tLoss: 1.218276\n",
            "Train Epoch: 10 [18022400/25340216 (71%)]\tLoss: 1.290892\n",
            "Train Epoch: 10 [18432000/25340216 (73%)]\tLoss: 1.137352\n",
            "Train Epoch: 10 [18841600/25340216 (74%)]\tLoss: 1.206843\n",
            "Train Epoch: 10 [19251200/25340216 (76%)]\tLoss: 1.250058\n",
            "Train Epoch: 10 [19660800/25340216 (78%)]\tLoss: 1.165007\n",
            "Train Epoch: 10 [20070400/25340216 (79%)]\tLoss: 1.230416\n",
            "Train Epoch: 10 [20480000/25340216 (81%)]\tLoss: 1.220550\n",
            "Train Epoch: 10 [20889600/25340216 (82%)]\tLoss: 1.193994\n",
            "Train Epoch: 10 [21299200/25340216 (84%)]\tLoss: 1.150704\n",
            "Train Epoch: 10 [21708800/25340216 (86%)]\tLoss: 1.262141\n",
            "Train Epoch: 10 [22118400/25340216 (87%)]\tLoss: 1.243117\n",
            "Train Epoch: 10 [22528000/25340216 (89%)]\tLoss: 1.213300\n",
            "Train Epoch: 10 [22937600/25340216 (91%)]\tLoss: 1.236545\n",
            "Train Epoch: 10 [23347200/25340216 (92%)]\tLoss: 1.221326\n",
            "Train Epoch: 10 [23756800/25340216 (94%)]\tLoss: 1.265044\n",
            "Train Epoch: 10 [24166400/25340216 (95%)]\tLoss: 1.139490\n",
            "Train Epoch: 10 [24576000/25340216 (97%)]\tLoss: 1.284159\n",
            "Train Epoch: 10 [24985600/25340216 (99%)]\tLoss: 1.243644\n",
            "Train Epoch: 10 [0/10850918 (0%)]\tLoss: 1.220979\n",
            "Train Epoch: 10 [409600/10850918 (4%)]\tLoss: 1.285720\n",
            "Train Epoch: 10 [819200/10850918 (8%)]\tLoss: 1.193127\n",
            "Train Epoch: 10 [1228800/10850918 (11%)]\tLoss: 1.257073\n",
            "Train Epoch: 10 [1638400/10850918 (15%)]\tLoss: 1.230330\n",
            "Train Epoch: 10 [2048000/10850918 (19%)]\tLoss: 1.280565\n",
            "Train Epoch: 10 [2457600/10850918 (23%)]\tLoss: 1.226225\n",
            "Train Epoch: 10 [2867200/10850918 (26%)]\tLoss: 1.204676\n",
            "Train Epoch: 10 [3276800/10850918 (30%)]\tLoss: 1.244298\n",
            "Train Epoch: 10 [3686400/10850918 (34%)]\tLoss: 1.185749\n",
            "Train Epoch: 10 [4096000/10850918 (38%)]\tLoss: 1.271727\n",
            "Train Epoch: 10 [4505600/10850918 (42%)]\tLoss: 1.263863\n",
            "Train Epoch: 10 [4915200/10850918 (45%)]\tLoss: 1.175517\n",
            "Train Epoch: 10 [5324800/10850918 (49%)]\tLoss: 1.230155\n",
            "Train Epoch: 10 [5734400/10850918 (53%)]\tLoss: 1.150679\n",
            "Train Epoch: 10 [6144000/10850918 (57%)]\tLoss: 1.246774\n",
            "Train Epoch: 10 [6553600/10850918 (60%)]\tLoss: 1.205833\n",
            "Train Epoch: 10 [6963200/10850918 (64%)]\tLoss: 1.168893\n",
            "Train Epoch: 10 [7372800/10850918 (68%)]\tLoss: 1.263105\n",
            "Train Epoch: 10 [7782400/10850918 (72%)]\tLoss: 1.202336\n",
            "Train Epoch: 10 [8192000/10850918 (75%)]\tLoss: 1.162022\n",
            "Train Epoch: 10 [8601600/10850918 (79%)]\tLoss: 1.193433\n",
            "Train Epoch: 10 [9011200/10850918 (83%)]\tLoss: 1.188902\n",
            "Train Epoch: 10 [9420800/10850918 (87%)]\tLoss: 1.149315\n",
            "Train Epoch: 10 [9830400/10850918 (91%)]\tLoss: 1.186219\n",
            "Train Epoch: 10 [10240000/10850918 (94%)]\tLoss: 1.190005\n",
            "Train Epoch: 10 [10649600/10850918 (98%)]\tLoss: 1.250681\n",
            "Train Epoch: 10 [0/1937496 (0%)]\tLoss: 1.294290\n",
            "Train Epoch: 10 [409600/1937496 (21%)]\tLoss: 1.185771\n",
            "Train Epoch: 10 [819200/1937496 (42%)]\tLoss: 1.238250\n",
            "Train Epoch: 10 [1228800/1937496 (63%)]\tLoss: 1.251779\n",
            "Train Epoch: 10 [1638400/1937496 (84%)]\tLoss: 1.239002\n",
            "Dev accuracy:  0.9467157609615711\n",
            " 51.31224897305171 minutes\n",
            "Train Epoch: 11 [0/25340216 (0%)]\tLoss: 1.276393\n",
            "Train Epoch: 11 [409600/25340216 (2%)]\tLoss: 1.248504\n",
            "Train Epoch: 11 [819200/25340216 (3%)]\tLoss: 1.219330\n",
            "Train Epoch: 11 [1228800/25340216 (5%)]\tLoss: 1.270876\n",
            "Train Epoch: 11 [1638400/25340216 (6%)]\tLoss: 1.203557\n",
            "Train Epoch: 11 [2048000/25340216 (8%)]\tLoss: 1.216835\n",
            "Train Epoch: 11 [2457600/25340216 (10%)]\tLoss: 1.213387\n",
            "Train Epoch: 11 [2867200/25340216 (11%)]\tLoss: 1.197739\n",
            "Train Epoch: 11 [3276800/25340216 (13%)]\tLoss: 1.160920\n",
            "Train Epoch: 11 [3686400/25340216 (15%)]\tLoss: 1.125989\n",
            "Train Epoch: 11 [4096000/25340216 (16%)]\tLoss: 1.202324\n",
            "Train Epoch: 11 [4505600/25340216 (18%)]\tLoss: 1.243421\n",
            "Train Epoch: 11 [4915200/25340216 (19%)]\tLoss: 1.213854\n",
            "Train Epoch: 11 [5324800/25340216 (21%)]\tLoss: 1.219048\n",
            "Train Epoch: 11 [5734400/25340216 (23%)]\tLoss: 1.220375\n",
            "Train Epoch: 11 [6144000/25340216 (24%)]\tLoss: 1.248628\n",
            "Train Epoch: 11 [6553600/25340216 (26%)]\tLoss: 1.177570\n",
            "Train Epoch: 11 [6963200/25340216 (27%)]\tLoss: 1.244033\n",
            "Train Epoch: 11 [7372800/25340216 (29%)]\tLoss: 1.231986\n",
            "Train Epoch: 11 [7782400/25340216 (31%)]\tLoss: 1.298637\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1f370a5f3a3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;34m'submission'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     }\n\u001b[0;32m--> 300\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-1f370a5f3a3f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;31m# train the model, with parameters changed in the train step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;31m# validation step, with validation accuracy generated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-1f370a5f3a3f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model, device, train_samples, optimizer, criterion, epoch, scaler)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# get gradients w.r.t the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# update the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No inf checks were recorded for this optimizer.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import Counter\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
        "from torch.utils.data import ConcatDataset\n",
        "\n",
        "import time\n",
        "import sys\n",
        "\n",
        "class Layer(nn.Module):\n",
        "    def __init__(self, in_size, out_size, dropout=0.25):\n",
        "        super(Layer, self).__init__()\n",
        "\n",
        "        if args['activation'].lower() == \"Tanh\".lower():\n",
        "            nn_activation = nn.Tanh()\n",
        "        elif args['activation'].lower() == \"LeakyReLU\".lower():\n",
        "            nn_activation = nn.LeakyReLU()\n",
        "        elif args['activation'].lower() == \"ELU\".lower():\n",
        "            nn_activation = nn.ELU()\n",
        "        else:\n",
        "            nn_activation = nn.ReLU()\n",
        "\n",
        "        standard_layer = [\n",
        "            nn.Linear(in_features=in_size, out_features=out_size),\n",
        "            nn.BatchNorm1d(out_size),\n",
        "            nn_activation,\n",
        "            nn.Dropout(dropout)\n",
        "        ]\n",
        "        self.layer = nn.Sequential(*standard_layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "        \n",
        "class Network(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        # each is 13 dim letter\n",
        "        context = args['context']\n",
        "        in_size = 13 * (2 * context + 1)\n",
        "        layers = [\n",
        "           Layer(in_size, 4096),\n",
        "            Layer(4096, 4096),\n",
        "            Layer(4096, 4096),\n",
        "            Layer(4096, 4096),\n",
        "            Layer(4096, 4096),\n",
        "            Layer(4096, 2048),            \n",
        "            Layer(2048, 1024),\n",
        "            Layer(1024, 40),\n",
        "        ]\n",
        "        self.laysers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, A0):\n",
        "        x = self.laysers(A0)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LibriSamples(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_path, sample=20000, shuffle=True, partition=\"dev-clean\", csvpath=None, submission=None):\n",
        "        # sample represent how many npy files will be preloaded for one __getitem__ call\n",
        "        self.sample = sample\n",
        "\n",
        "        self.X_dir = data_path + \"/\" + partition + \"/mfcc/\"\n",
        "        self.Y_dir = data_path + \"/\" + partition + \"/transcript/\"\n",
        "\n",
        "        self.X_names = os.listdir(self.X_dir)\n",
        "        self.Y_names = os.listdir(self.Y_dir)\n",
        "\n",
        "        # using a small part of the dataset to debug\n",
        "        if csvpath:\n",
        "            subset = self.parse_csv(csvpath)\n",
        "            self.X_names = [i for i in self.X_names if i in subset]\n",
        "            self.Y_names = [i for i in self.Y_names if i in subset]\n",
        "\n",
        "        if shuffle == True:\n",
        "            XY_names = list(zip(self.X_names, self.Y_names))\n",
        "            random.shuffle(XY_names)\n",
        "            self.X_names, self.Y_names = zip(*XY_names)\n",
        "\n",
        "        assert (len(self.X_names) == len(self.Y_names))\n",
        "        self.length = len(self.X_names)\n",
        "\n",
        "        self.PHONEMES = [\n",
        "            'SIL', 'AA', 'AE', 'AH', 'AO', 'AW', 'AY',\n",
        "            'B', 'CH', 'D', 'DH', 'EH', 'ER', 'EY',\n",
        "            'F', 'G', 'HH', 'IH', 'IY', 'JH', 'K',\n",
        "            'L', 'M', 'N', 'NG', 'OW', 'OY', 'P',\n",
        "            'R', 'S', 'SH', 'T', 'TH', 'UH', 'UW',\n",
        "            'V', 'W', 'Y', 'Z', 'ZH', '<sos>', '<eos>']\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_csv(filepath):\n",
        "        subset = []\n",
        "        with open(filepath) as f:\n",
        "            f_csv = csv.reader(f)\n",
        "            for row in f_csv:\n",
        "                subset.append(row[1])\n",
        "        return subset[1:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(self.length / self.sample))\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        sample_range = range(i * self.sample, min((i + 1) * self.sample, self.length))\n",
        "\n",
        "        X, Y = [], []\n",
        "        for j in sample_range:\n",
        "            X_path = self.X_dir + self.X_names[j]\n",
        "            Y_path = self.Y_dir + self.Y_names[j]\n",
        "\n",
        "            label = [self.PHONEMES.index(yy) for yy in np.load(Y_path)][1:-1]\n",
        "\n",
        "            X_data = np.load(X_path)\n",
        "            X_data = (X_data - X_data.mean(axis=0)) / X_data.std(axis=0)\n",
        "            X.append(X_data)\n",
        "            Y.append(np.array(label))\n",
        "\n",
        "        X, Y = np.concatenate(X), np.concatenate(Y)\n",
        "        return X, Y\n",
        "\n",
        "\n",
        "\n",
        "class LibriItems(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, Y, context=0):\n",
        "        assert (X.shape[0] == Y.shape[0])\n",
        "\n",
        "        self.length = X.shape[0]\n",
        "        self.context = context\n",
        "\n",
        "        if context == 0:\n",
        "            self.X, self.Y = X, Y\n",
        "        else:\n",
        "            self.X = np.pad(X, ((context, context), (0, 0)), 'constant', constant_values=0)\n",
        "            self.Y = Y\n",
        "            pass\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        if self.context == 0:\n",
        "            xx = self.X[i].flatten()\n",
        "            yy = self.Y[i]\n",
        "        else:\n",
        "            start_i = i\n",
        "            end_i = i + 2 * self.context + 1\n",
        "            xx = self.X[start_i:end_i].flatten()\n",
        "            yy = self.Y[i]\n",
        "\n",
        "            pass\n",
        "        return xx, yy\n",
        "\n",
        "\n",
        "def train(args, model, device, train_samples, optimizer, criterion, epoch, scaler):\n",
        "    model.train()\n",
        "    running_loss = 0.0      \n",
        "    for i in range(len(train_samples)):\n",
        "        X, Y = train_samples[i]\n",
        "        train_items = LibriItems(X, Y, context=args['context'])\n",
        "        train_loader = torch.utils.data.DataLoader(train_items, batch_size=args['batch_size'], shuffle=True)\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data = data.float().to(device)\n",
        "            target = target.long().to(device)\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                optimizer.zero_grad()\n",
        "                output = model(data)\n",
        "                loss = criterion(output, target)\n",
        "\n",
        "                # loss.backward()\n",
        "\n",
        "                scaler.scale(loss).backward()  # get gradients w.r.t the loss\n",
        "                scaler.step(optimizer)  # update the weights\n",
        "\n",
        "                scaler.update()\n",
        "                if batch_idx % args['log_interval'] == 0:\n",
        "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                               100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "    running_loss = running_loss / len(train_samples)\n",
        "    # print(\"Training loss is: \",running_loss)\n",
        "    return running_loss\n",
        "\n",
        "\n",
        "def test(args, model, device, dev_samples, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0    \n",
        "    true_y_list = []\n",
        "    pred_y_list = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(dev_samples)):\n",
        "            X, Y = dev_samples[i]\n",
        "\n",
        "            test_items = LibriItems(X, Y, context=args['context'])\n",
        "            test_loader = torch.utils.data.DataLoader(test_items, batch_size=args['batch_size'], shuffle=False)\n",
        "\n",
        "            for data, true_y in test_loader:\n",
        "                data = data.float().to(device)\n",
        "                true_y = true_y.long().to(device)\n",
        "\n",
        "                output = model(data)\n",
        "                pred_y = torch.argmax(output, axis=1)\n",
        "                loss = criterion(output,true_y)\n",
        "                running_loss+=loss.item()\n",
        "\n",
        "                pred_y_list.extend(pred_y.tolist())\n",
        "                true_y_list.extend(true_y.tolist())\n",
        "    \n",
        "    running_loss = running_loss / len(dev_samples)\n",
        "    # print(\"Dev loss is: \",running_loss)\n",
        "    train_accuracy = accuracy_score(true_y_list, pred_y_list)\n",
        "    return train_accuracy, running_loss\n",
        "\n",
        "\n",
        "def create_folder(folder_path):\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.mkdir(folder_path)\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = Network().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    # scheduler1 = StepLR(optimizer, step_size=4, gamma=0.1)\n",
        "    scheduler1 = ReduceLROnPlateau(optimizer, factor=0.5,patience=2, mode='max',threshold=0.01,verbose=True)\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "\n",
        "    # If you want to use full Dataset, please pass None to csvpath\n",
        "    train_samples = LibriSamples(data_path=args['LIBRI_PATH'], shuffle=True, partition=\"train-clean-100\",\n",
        "                                 csvpath= None,submission=args['submission'])\n",
        "    dev_samples = LibriSamples(data_path=args['LIBRI_PATH'], shuffle=True, partition=\"dev-clean\")\n",
        "    if args['submission']:\n",
        "        train_samples = ConcatDataset([train_samples,dev_samples])\n",
        "\n",
        "    create_folder(\"/content/drive/hw1/checkpoints\")\n",
        "\n",
        "    save_path_dir = args['model_type']+ \"_context_\"+str(args['context']) + \"_lr_\"+str(args['lr'])+\"_\"+args['schedular']+\"_\"+args['activation']\n",
        "    model_path= \"/content/drive/hw1/checkpoints/\"+save_path_dir+\"/\"\n",
        "    args['save_path_dir'] = model_path\n",
        "    create_folder(model_path)\n",
        "    print(\"We are going to save to \"+ model_path)\n",
        "    \n",
        "    txt_file = args['save_path_dir']+\"dev_acc.txt\"\n",
        "    with open(txt_file,'w') as f :\n",
        "\n",
        "        for epoch in range(1, args['epoch'] + 1):\n",
        "            start = time.time()\n",
        "            # train the model, with parameters changed in the train step\n",
        "            train(args, model, device, train_samples, optimizer, criterion, epoch, scaler )\n",
        "\n",
        "            # validation step, with validation accuracy generated\n",
        "            test_acc, dev_loss = test(args, model, device, dev_samples,criterion)\n",
        "            scheduler1.step(test_acc)\n",
        "\n",
        "            print('Dev accuracy: ', test_acc)\n",
        "            print_content = \"Dev accuracy in epoch {} is {}\\n\".format(epoch,test_acc)\n",
        "            f.write(print_content)\n",
        "\n",
        "            end = time.time()\n",
        "            print(\"\", (end - start)/60,\"minutes\")\n",
        "\n",
        "            # specify the path\n",
        "            path = \"/content/drive/hw1/checkpoints/{}/model_epoch_{}.txt\".format(save_path_dir,epoch)\n",
        "\n",
        "            # save the checkpoint\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(), \n",
        "                'scheduler': scheduler1,\n",
        "            }, path)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = {\n",
        "        'batch_size': 2048,\n",
        "        'context': 30,\n",
        "        'log_interval': 200,\n",
        "        'LIBRI_PATH': '/content/hw1p2_student_data',\n",
        "        'lr': 1e-3,\n",
        "        'epoch': 30,\n",
        "        'save_path_dir':'',\n",
        "        'model_type':'Pyramid',\n",
        "        'schedular':'ReduceLROnPlateau_factor0.5_max_patience_2',\n",
        "        'activation':'LeakyReLU',\n",
        "        'submission':False,\n",
        "    }\n",
        "    main(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr-9dS-wyYO7"
      },
      "outputs": [],
      "source": [
        "# Model prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzulRW3rUs1-",
        "outputId": "6df77521-9418-4754-e3de-59ce9c08fb80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We are predicting files from Pyramid_context_30_lr_0.001_ReduceLROnPlateau_factor0.5_max_patience_2_LeakyReLU\n",
            "100% 18.6M/18.6M [00:03<00:00, 4.99MB/s]\n",
            "Successfully submitted to Frame-Level Speech Recognition"
          ]
        }
      ],
      "source": [
        "from numpy.lib.npyio import save\n",
        "import os\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.optim.lr_scheduler import StepLR,ReduceLROnPlateau\n",
        "\n",
        "class Layer(nn.Module):\n",
        "    def __init__(self, in_size, out_size, dropout=0.25):\n",
        "        super(Layer, self).__init__()\n",
        "\n",
        "        if args['activation'].lower() == \"Tanh\".lower():\n",
        "            nn_activation = nn.Tanh()\n",
        "        elif args['activation'].lower() == \"LeakyReLU\".lower():\n",
        "            nn_activation = nn.LeakyReLU()\n",
        "        elif args['activation'].lower() == \"ELU\".lower():\n",
        "            nn_activation = nn.ELU()\n",
        "        else:\n",
        "            nn_activation = nn.ReLU()\n",
        "\n",
        "        standard_layer = [\n",
        "            nn.Linear(in_features=in_size, out_features=out_size),\n",
        "            nn.BatchNorm1d(out_size),\n",
        "            nn_activation,\n",
        "            nn.Dropout(dropout)\n",
        "        ]\n",
        "        self.layer = nn.Sequential(*standard_layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "        \n",
        "class Network(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        # each is 13 dim letter\n",
        "        context = args['context']\n",
        "        in_size = 13 * (2 * context + 1)\n",
        "        layers = [\n",
        "           Layer(in_size, 4096),\n",
        "            Layer(4096, 4096),\n",
        "            Layer(4096, 4096),\n",
        "            Layer(4096, 4096),\n",
        "            Layer(4096, 4096),\n",
        "            Layer(4096, 2048),            \n",
        "            Layer(2048, 1024),\n",
        "            Layer(1024, 40),\n",
        "        ]\n",
        "        self.laysers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, A0):\n",
        "        x = self.laysers(A0)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LibriSamplesTest(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_path, sample=20000, partition=\"test-clean\", csvpath=None):\n",
        "        # sample represent how many npy files will be preloaded for one __getitem__ call\n",
        "        self.sample = sample\n",
        "\n",
        "        self.X_dir = data_path + \"/\" + partition + \"/mfcc/\"\n",
        "\n",
        "        # using a small part of the dataset to debug\n",
        "        if csvpath:\n",
        "            subset = self.parse_csv(csvpath)\n",
        "            # self.X_names = [i for i in self.X_names if i in subset]\n",
        "            self.X_names = [i for i in subset]\n",
        "\n",
        "        self.length = len(self.X_names)\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_csv(filepath):\n",
        "        subset = []\n",
        "        with open(filepath) as f:\n",
        "            f_csv = csv.reader(f)\n",
        "            for row in f_csv:\n",
        "                subset.append(row[0])\n",
        "        return subset[1:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(self.length / self.sample))\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        sample_range = range(i * self.sample, min((i + 1) * self.sample, self.length))\n",
        "\n",
        "        X = []\n",
        "        for j in sample_range:\n",
        "            X_path = self.X_dir + self.X_names[j]\n",
        "            X_data = np.load(X_path)\n",
        "            X_data = (X_data - X_data.mean(axis=0)) / X_data.std(axis=0)\n",
        "            X.append(X_data)\n",
        "\n",
        "        X = np.concatenate(X)\n",
        "        return X\n",
        "\n",
        "\n",
        "class LibriItemsTest(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, context=0):\n",
        "\n",
        "        self.length = X.shape[0]\n",
        "        self.context = context\n",
        "\n",
        "        if context == 0:\n",
        "            self.X = X\n",
        "        else:\n",
        "            self.X = np.pad(X, ((context, context), (0, 0)), 'constant', constant_values=0)\n",
        "            pass\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        if self.context == 0:\n",
        "            xx = self.X[i].flatten()\n",
        "        else:\n",
        "            start_i = i\n",
        "            end_i = i + 2 * self.context + 1\n",
        "            xx = self.X[start_i:end_i].flatten()\n",
        "            pass\n",
        "        return xx\n",
        "\n",
        "\n",
        "def predict(args, model, device, test_samples):\n",
        "    model.eval()\n",
        "    res = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(test_samples)):\n",
        "            X = test_samples[i]\n",
        "\n",
        "            test_items = LibriItemsTest(X, context=args['context'])\n",
        "            test_loader = torch.utils.data.DataLoader(test_items, batch_size=args['batch_size'], shuffle=False)\n",
        "\n",
        "            for data in test_loader:\n",
        "                data = data.float().to(device)\n",
        "\n",
        "                output = model(data)\n",
        "                pred_y = torch.argmax(output, axis=1)\n",
        "                res.extend(pred_y.tolist())\n",
        "    return res\n",
        "\n",
        "\n",
        "def create_folder(folder_path):\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.mkdir(folder_path)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "args = {\n",
        "    'batch_size': 2048,\n",
        "    'context': 30,\n",
        "    'log_interval': 200,\n",
        "    'LIBRI_PATH': '/content/hw1p2_student_data',\n",
        "    'lr': 1e-3,\n",
        "    'epoch': 100,\n",
        "    'save_path_dir':'',\n",
        "    'model_type':'Pyramid',\n",
        "    'schedular':'ReduceLROnPlateau_factor0.5_max_patience_2',\n",
        "    'activation':'LeakyReLU',\n",
        "    'submission':False,\n",
        "}\n",
        "save_path_dir = args['model_type']+ \"_context_\"+str(args['context']) + \"_lr_\"+str(args['lr'])+\"_\"+args['schedular']+\"_\"+args['activation']\n",
        "# save_path_dir = args['model_type']+ \"_context_\"+str(args['context']) + \"_lr_\"+str(args['lr'])+\"_\"+args['schedular']\n",
        "\n",
        "print(\"We are predicting files from\",save_path_dir)\n",
        "\n",
        "model_path= \"/content/drive/hw1/checkpoints/\"+save_path_dir+\"/\"\n",
        "args['save_path_dir'] = model_path\n",
        "\n",
        "results_path = \"/content/drive/hw1/results/{}\".format(save_path_dir)\n",
        "create_folder(results_path)\n",
        "\n",
        "# Reload the model and necessary architecture\n",
        "model = Network().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "# scheduler1 = StepLR(optimizer, step_size=4, gamma=0.1)\n",
        "scheduler1 = ReduceLROnPlateau(optimizer, factor=0.6,patience=2, mode='max',threshold=0.01,verbose=True)\n",
        "\n",
        "# Reload the checkpoint\n",
        "checkpoint = torch.load(\"/content/drive/hw1/checkpoints/{}/model_epoch_{}.txt\".format(save_path_dir,args['epoch']))\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "scheduler1=checkpoint['scheduler']\n",
        "epoch = checkpoint['epoch']\n",
        "\n",
        "test_samples = LibriSamplesTest(data_path=args['LIBRI_PATH'], partition=\"test-clean\",\n",
        "                                csvpath=\"/content/test_order.csv\")\n",
        "\n",
        "pred_Y = predict(args, model, device, test_samples)\n",
        "npy_path = '/content/drive/hw1/results/{}/predict_labels_{}.npy'.format(save_path_dir,args['epoch'])\n",
        "np.save(npy_path, pred_Y)\n",
        "label = np.load(npy_path)\n",
        "\n",
        "id = np.array(range(len(label)))\n",
        "df = pd.DataFrame({\"id\": id, \"label\": label})\n",
        "create_folder(\"/content/drive/hw1/results/kaggle/\")\n",
        "df.to_csv(\"/content/drive/hw1/results/kaggle/submission.csv\", index=False)\n",
        "! kaggle competitions submit -c 11-785-s22-hw1p2 -f /content/drive/hw1/results/kaggle/submission.csv -m \"Message\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bja9FS0uHFjC",
        "outputId": "610a0204-465b-4340-fec0-53df09586e79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Feb  9 19:04:16 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIQ0ZhDD7Rpe",
        "outputId": "26db4ca0-36d5-4d99-a7f2-09e38d7bd568"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We are reloading files from Pyramid_context_30_lr_0.001_ReduceLROnPlateau_factor0.5_max_patience_2_LeakyReLU\n",
            "Train Epoch: 101 [0/25334306 (0%)]\tLoss: 1.024348\n",
            "Train Epoch: 101 [409600/25334306 (2%)]\tLoss: 1.006572\n",
            "Train Epoch: 101 [819200/25334306 (3%)]\tLoss: 1.027281\n",
            "Train Epoch: 101 [1228800/25334306 (5%)]\tLoss: 1.005733\n",
            "Train Epoch: 101 [1638400/25334306 (6%)]\tLoss: 0.972232\n",
            "Train Epoch: 101 [2048000/25334306 (8%)]\tLoss: 0.988977\n",
            "Train Epoch: 101 [2457600/25334306 (10%)]\tLoss: 1.029442\n",
            "Train Epoch: 101 [2867200/25334306 (11%)]\tLoss: 1.025268\n",
            "Train Epoch: 101 [3276800/25334306 (13%)]\tLoss: 1.090567\n",
            "Train Epoch: 101 [3686400/25334306 (15%)]\tLoss: 1.011129\n",
            "Train Epoch: 101 [4096000/25334306 (16%)]\tLoss: 0.991853\n",
            "Train Epoch: 101 [4505600/25334306 (18%)]\tLoss: 1.017848\n",
            "Train Epoch: 101 [4915200/25334306 (19%)]\tLoss: 1.032083\n",
            "Train Epoch: 101 [5324800/25334306 (21%)]\tLoss: 1.064490\n",
            "Train Epoch: 101 [5734400/25334306 (23%)]\tLoss: 1.018716\n",
            "Train Epoch: 101 [6144000/25334306 (24%)]\tLoss: 1.009477\n",
            "Train Epoch: 101 [6553600/25334306 (26%)]\tLoss: 1.000584\n",
            "Train Epoch: 101 [6963200/25334306 (27%)]\tLoss: 0.980870\n",
            "Train Epoch: 101 [7372800/25334306 (29%)]\tLoss: 0.984378\n",
            "Train Epoch: 101 [7782400/25334306 (31%)]\tLoss: 1.017015\n",
            "Train Epoch: 101 [8192000/25334306 (32%)]\tLoss: 1.002087\n",
            "Train Epoch: 101 [8601600/25334306 (34%)]\tLoss: 1.029167\n",
            "Train Epoch: 101 [9011200/25334306 (36%)]\tLoss: 1.032260\n",
            "Train Epoch: 101 [9420800/25334306 (37%)]\tLoss: 1.000418\n",
            "Train Epoch: 101 [9830400/25334306 (39%)]\tLoss: 1.021613\n",
            "Train Epoch: 101 [10240000/25334306 (40%)]\tLoss: 0.982183\n",
            "Train Epoch: 101 [10649600/25334306 (42%)]\tLoss: 1.011617\n",
            "Train Epoch: 101 [11059200/25334306 (44%)]\tLoss: 1.030742\n",
            "Train Epoch: 101 [11468800/25334306 (45%)]\tLoss: 0.969622\n",
            "Train Epoch: 101 [11878400/25334306 (47%)]\tLoss: 1.024148\n",
            "Train Epoch: 101 [12288000/25334306 (49%)]\tLoss: 1.006630\n",
            "Train Epoch: 101 [12697600/25334306 (50%)]\tLoss: 1.041536\n",
            "Train Epoch: 101 [13107200/25334306 (52%)]\tLoss: 1.038070\n",
            "Train Epoch: 101 [13516800/25334306 (53%)]\tLoss: 0.979908\n",
            "Train Epoch: 101 [13926400/25334306 (55%)]\tLoss: 1.033490\n",
            "Train Epoch: 101 [14336000/25334306 (57%)]\tLoss: 1.021279\n",
            "Train Epoch: 101 [14745600/25334306 (58%)]\tLoss: 0.989990\n",
            "Train Epoch: 101 [15155200/25334306 (60%)]\tLoss: 1.028416\n",
            "Train Epoch: 101 [15564800/25334306 (61%)]\tLoss: 0.933687\n",
            "Train Epoch: 101 [15974400/25334306 (63%)]\tLoss: 0.981524\n",
            "Train Epoch: 101 [16384000/25334306 (65%)]\tLoss: 0.975047\n",
            "Train Epoch: 101 [16793600/25334306 (66%)]\tLoss: 0.941888\n",
            "Train Epoch: 101 [17203200/25334306 (68%)]\tLoss: 0.983169\n",
            "Train Epoch: 101 [17612800/25334306 (70%)]\tLoss: 1.059486\n",
            "Train Epoch: 101 [18022400/25334306 (71%)]\tLoss: 1.032381\n",
            "Train Epoch: 101 [18432000/25334306 (73%)]\tLoss: 1.013571\n",
            "Train Epoch: 101 [18841600/25334306 (74%)]\tLoss: 0.980216\n",
            "Train Epoch: 101 [19251200/25334306 (76%)]\tLoss: 0.998090\n",
            "Train Epoch: 101 [19660800/25334306 (78%)]\tLoss: 1.011917\n",
            "Train Epoch: 101 [20070400/25334306 (79%)]\tLoss: 0.992202\n",
            "Train Epoch: 101 [20480000/25334306 (81%)]\tLoss: 1.024972\n",
            "Train Epoch: 101 [20889600/25334306 (82%)]\tLoss: 0.949988\n",
            "Train Epoch: 101 [21299200/25334306 (84%)]\tLoss: 1.015482\n",
            "Train Epoch: 101 [21708800/25334306 (86%)]\tLoss: 0.990087\n",
            "Train Epoch: 101 [22118400/25334306 (87%)]\tLoss: 0.975829\n",
            "Train Epoch: 101 [22528000/25334306 (89%)]\tLoss: 1.040550\n",
            "Train Epoch: 101 [22937600/25334306 (91%)]\tLoss: 1.051734\n",
            "Train Epoch: 101 [23347200/25334306 (92%)]\tLoss: 1.034553\n",
            "Train Epoch: 101 [23756800/25334306 (94%)]\tLoss: 0.989833\n",
            "Train Epoch: 101 [24166400/25334306 (95%)]\tLoss: 0.987383\n",
            "Train Epoch: 101 [24576000/25334306 (97%)]\tLoss: 0.970519\n",
            "Train Epoch: 101 [24985600/25334306 (99%)]\tLoss: 1.010558\n",
            "Train Epoch: 101 [0/10856828 (0%)]\tLoss: 1.023892\n",
            "Train Epoch: 101 [409600/10856828 (4%)]\tLoss: 0.991895\n",
            "Train Epoch: 101 [819200/10856828 (8%)]\tLoss: 1.026567\n",
            "Train Epoch: 101 [1228800/10856828 (11%)]\tLoss: 1.000262\n",
            "Train Epoch: 101 [1638400/10856828 (15%)]\tLoss: 0.973394\n",
            "Train Epoch: 101 [2048000/10856828 (19%)]\tLoss: 1.026899\n",
            "Train Epoch: 101 [2457600/10856828 (23%)]\tLoss: 0.962607\n",
            "Train Epoch: 101 [2867200/10856828 (26%)]\tLoss: 0.980849\n",
            "Train Epoch: 101 [3276800/10856828 (30%)]\tLoss: 1.036799\n",
            "Train Epoch: 101 [3686400/10856828 (34%)]\tLoss: 0.963661\n",
            "Train Epoch: 101 [4096000/10856828 (38%)]\tLoss: 1.005981\n",
            "Train Epoch: 101 [4505600/10856828 (41%)]\tLoss: 1.020959\n",
            "Train Epoch: 101 [4915200/10856828 (45%)]\tLoss: 0.944098\n",
            "Train Epoch: 101 [5324800/10856828 (49%)]\tLoss: 0.981045\n",
            "Train Epoch: 101 [5734400/10856828 (53%)]\tLoss: 0.919130\n",
            "Train Epoch: 101 [6144000/10856828 (57%)]\tLoss: 1.002699\n",
            "Train Epoch: 101 [6553600/10856828 (60%)]\tLoss: 1.011967\n",
            "Train Epoch: 101 [6963200/10856828 (64%)]\tLoss: 1.040613\n",
            "Train Epoch: 101 [7372800/10856828 (68%)]\tLoss: 0.943743\n",
            "Train Epoch: 101 [7782400/10856828 (72%)]\tLoss: 1.015759\n",
            "Train Epoch: 101 [8192000/10856828 (75%)]\tLoss: 1.077654\n",
            "Train Epoch: 101 [8601600/10856828 (79%)]\tLoss: 1.015332\n",
            "Train Epoch: 101 [9011200/10856828 (83%)]\tLoss: 0.991090\n",
            "Train Epoch: 101 [9420800/10856828 (87%)]\tLoss: 0.956780\n",
            "Train Epoch: 101 [9830400/10856828 (91%)]\tLoss: 1.041327\n",
            "Train Epoch: 101 [10240000/10856828 (94%)]\tLoss: 1.018610\n",
            "Train Epoch: 101 [10649600/10856828 (98%)]\tLoss: 1.011900\n",
            "Dev accuracy:  0.8698779249092643\n",
            " 31.356229356924693 minutes\n",
            "Train Epoch: 102 [0/25334306 (0%)]\tLoss: 0.945127\n",
            "Train Epoch: 102 [409600/25334306 (2%)]\tLoss: 1.001313\n",
            "Train Epoch: 102 [819200/25334306 (3%)]\tLoss: 0.978994\n",
            "Train Epoch: 102 [1228800/25334306 (5%)]\tLoss: 0.977692\n",
            "Train Epoch: 102 [1638400/25334306 (6%)]\tLoss: 1.007930\n",
            "Train Epoch: 102 [2048000/25334306 (8%)]\tLoss: 0.971800\n",
            "Train Epoch: 102 [2457600/25334306 (10%)]\tLoss: 1.008596\n",
            "Train Epoch: 102 [2867200/25334306 (11%)]\tLoss: 1.038999\n",
            "Train Epoch: 102 [3276800/25334306 (13%)]\tLoss: 0.971683\n",
            "Train Epoch: 102 [3686400/25334306 (15%)]\tLoss: 1.023505\n",
            "Train Epoch: 102 [4096000/25334306 (16%)]\tLoss: 1.013886\n",
            "Train Epoch: 102 [4505600/25334306 (18%)]\tLoss: 0.966230\n",
            "Train Epoch: 102 [4915200/25334306 (19%)]\tLoss: 0.929391\n",
            "Train Epoch: 102 [5324800/25334306 (21%)]\tLoss: 0.972173\n",
            "Train Epoch: 102 [5734400/25334306 (23%)]\tLoss: 1.027053\n",
            "Train Epoch: 102 [6144000/25334306 (24%)]\tLoss: 1.034484\n",
            "Train Epoch: 102 [6553600/25334306 (26%)]\tLoss: 1.046606\n",
            "Train Epoch: 102 [6963200/25334306 (27%)]\tLoss: 0.975331\n",
            "Train Epoch: 102 [7372800/25334306 (29%)]\tLoss: 1.065180\n",
            "Train Epoch: 102 [7782400/25334306 (31%)]\tLoss: 1.077318\n",
            "Train Epoch: 102 [8192000/25334306 (32%)]\tLoss: 1.016121\n",
            "Train Epoch: 102 [8601600/25334306 (34%)]\tLoss: 1.029768\n",
            "Train Epoch: 102 [9011200/25334306 (36%)]\tLoss: 1.033401\n",
            "Train Epoch: 102 [9420800/25334306 (37%)]\tLoss: 0.993117\n",
            "Train Epoch: 102 [9830400/25334306 (39%)]\tLoss: 1.053052\n",
            "Train Epoch: 102 [10240000/25334306 (40%)]\tLoss: 1.026997\n",
            "Train Epoch: 102 [10649600/25334306 (42%)]\tLoss: 1.028240\n",
            "Train Epoch: 102 [11059200/25334306 (44%)]\tLoss: 1.015454\n",
            "Train Epoch: 102 [11468800/25334306 (45%)]\tLoss: 0.995478\n",
            "Train Epoch: 102 [11878400/25334306 (47%)]\tLoss: 1.042920\n",
            "Train Epoch: 102 [12288000/25334306 (49%)]\tLoss: 0.995575\n",
            "Train Epoch: 102 [12697600/25334306 (50%)]\tLoss: 1.056458\n",
            "Train Epoch: 102 [13107200/25334306 (52%)]\tLoss: 1.013736\n",
            "Train Epoch: 102 [13516800/25334306 (53%)]\tLoss: 0.985140\n",
            "Train Epoch: 102 [13926400/25334306 (55%)]\tLoss: 1.002538\n",
            "Train Epoch: 102 [14336000/25334306 (57%)]\tLoss: 0.993550\n",
            "Train Epoch: 102 [14745600/25334306 (58%)]\tLoss: 0.991128\n",
            "Train Epoch: 102 [15155200/25334306 (60%)]\tLoss: 0.923380\n",
            "Train Epoch: 102 [15564800/25334306 (61%)]\tLoss: 1.037840\n",
            "Train Epoch: 102 [15974400/25334306 (63%)]\tLoss: 1.010144\n",
            "Train Epoch: 102 [16384000/25334306 (65%)]\tLoss: 0.925362\n",
            "Train Epoch: 102 [16793600/25334306 (66%)]\tLoss: 1.011629\n",
            "Train Epoch: 102 [17203200/25334306 (68%)]\tLoss: 0.996445\n",
            "Train Epoch: 102 [17612800/25334306 (70%)]\tLoss: 1.018275\n",
            "Train Epoch: 102 [18022400/25334306 (71%)]\tLoss: 1.008918\n",
            "Train Epoch: 102 [18432000/25334306 (73%)]\tLoss: 0.956232\n",
            "Train Epoch: 102 [18841600/25334306 (74%)]\tLoss: 1.001706\n",
            "Train Epoch: 102 [19251200/25334306 (76%)]\tLoss: 1.011369\n",
            "Train Epoch: 102 [19660800/25334306 (78%)]\tLoss: 0.959951\n",
            "Train Epoch: 102 [20070400/25334306 (79%)]\tLoss: 1.029883\n",
            "Train Epoch: 102 [20480000/25334306 (81%)]\tLoss: 0.966140\n",
            "Train Epoch: 102 [20889600/25334306 (82%)]\tLoss: 1.024286\n",
            "Train Epoch: 102 [21299200/25334306 (84%)]\tLoss: 0.998139\n",
            "Train Epoch: 102 [21708800/25334306 (86%)]\tLoss: 1.011937\n",
            "Train Epoch: 102 [22118400/25334306 (87%)]\tLoss: 0.956803\n",
            "Train Epoch: 102 [22528000/25334306 (89%)]\tLoss: 0.981323\n",
            "Train Epoch: 102 [22937600/25334306 (91%)]\tLoss: 0.984067\n",
            "Train Epoch: 102 [23347200/25334306 (92%)]\tLoss: 1.103504\n",
            "Train Epoch: 102 [23756800/25334306 (94%)]\tLoss: 0.942890\n",
            "Train Epoch: 102 [24166400/25334306 (95%)]\tLoss: 0.907034\n",
            "Train Epoch: 102 [24576000/25334306 (97%)]\tLoss: 1.032592\n",
            "Train Epoch: 102 [24985600/25334306 (99%)]\tLoss: 0.980989\n",
            "Train Epoch: 102 [0/10856828 (0%)]\tLoss: 1.009936\n",
            "Train Epoch: 102 [409600/10856828 (4%)]\tLoss: 1.106045\n",
            "Train Epoch: 102 [819200/10856828 (8%)]\tLoss: 1.018568\n",
            "Train Epoch: 102 [1228800/10856828 (11%)]\tLoss: 0.999062\n",
            "Train Epoch: 102 [1638400/10856828 (15%)]\tLoss: 1.050058\n",
            "Train Epoch: 102 [2048000/10856828 (19%)]\tLoss: 0.998988\n",
            "Train Epoch: 102 [2457600/10856828 (23%)]\tLoss: 0.988983\n",
            "Train Epoch: 102 [2867200/10856828 (26%)]\tLoss: 0.993653\n",
            "Train Epoch: 102 [3276800/10856828 (30%)]\tLoss: 0.978167\n",
            "Train Epoch: 102 [3686400/10856828 (34%)]\tLoss: 1.025076\n",
            "Train Epoch: 102 [4096000/10856828 (38%)]\tLoss: 1.000825\n",
            "Train Epoch: 102 [4505600/10856828 (41%)]\tLoss: 1.006076\n",
            "Train Epoch: 102 [4915200/10856828 (45%)]\tLoss: 1.040375\n",
            "Train Epoch: 102 [5324800/10856828 (49%)]\tLoss: 1.007567\n",
            "Train Epoch: 102 [5734400/10856828 (53%)]\tLoss: 1.001496\n",
            "Train Epoch: 102 [6144000/10856828 (57%)]\tLoss: 1.052320\n",
            "Train Epoch: 102 [6553600/10856828 (60%)]\tLoss: 0.939480\n",
            "Train Epoch: 102 [6963200/10856828 (64%)]\tLoss: 1.025550\n",
            "Train Epoch: 102 [7372800/10856828 (68%)]\tLoss: 1.003709\n",
            "Train Epoch: 102 [7782400/10856828 (72%)]\tLoss: 0.981202\n",
            "Train Epoch: 102 [8192000/10856828 (75%)]\tLoss: 1.005514\n",
            "Train Epoch: 102 [8601600/10856828 (79%)]\tLoss: 1.060135\n",
            "Train Epoch: 102 [9011200/10856828 (83%)]\tLoss: 1.001115\n",
            "Train Epoch: 102 [9420800/10856828 (87%)]\tLoss: 0.996158\n",
            "Train Epoch: 102 [9830400/10856828 (91%)]\tLoss: 1.007588\n",
            "Train Epoch: 102 [10240000/10856828 (94%)]\tLoss: 0.962255\n",
            "Train Epoch: 102 [10649600/10856828 (98%)]\tLoss: 1.060353\n",
            "Dev accuracy:  0.8699568928142303\n",
            " 31.396710300445555 minutes\n",
            "Train Epoch: 103 [0/25334306 (0%)]\tLoss: 0.991089\n",
            "Train Epoch: 103 [409600/25334306 (2%)]\tLoss: 1.023560\n",
            "Train Epoch: 103 [819200/25334306 (3%)]\tLoss: 1.017229\n",
            "Train Epoch: 103 [1228800/25334306 (5%)]\tLoss: 0.953762\n",
            "Train Epoch: 103 [1638400/25334306 (6%)]\tLoss: 0.964639\n",
            "Train Epoch: 103 [2048000/25334306 (8%)]\tLoss: 1.054687\n",
            "Train Epoch: 103 [2457600/25334306 (10%)]\tLoss: 0.961806\n",
            "Train Epoch: 103 [2867200/25334306 (11%)]\tLoss: 0.984786\n",
            "Train Epoch: 103 [3276800/25334306 (13%)]\tLoss: 1.003415\n",
            "Train Epoch: 103 [3686400/25334306 (15%)]\tLoss: 1.011732\n",
            "Train Epoch: 103 [4096000/25334306 (16%)]\tLoss: 0.951810\n",
            "Train Epoch: 103 [4505600/25334306 (18%)]\tLoss: 1.032966\n",
            "Train Epoch: 103 [4915200/25334306 (19%)]\tLoss: 0.985351\n",
            "Train Epoch: 103 [5324800/25334306 (21%)]\tLoss: 1.048245\n",
            "Train Epoch: 103 [5734400/25334306 (23%)]\tLoss: 0.962506\n",
            "Train Epoch: 103 [6144000/25334306 (24%)]\tLoss: 1.019236\n",
            "Train Epoch: 103 [6553600/25334306 (26%)]\tLoss: 1.011700\n",
            "Train Epoch: 103 [6963200/25334306 (27%)]\tLoss: 1.044246\n",
            "Train Epoch: 103 [7372800/25334306 (29%)]\tLoss: 0.969977\n",
            "Train Epoch: 103 [7782400/25334306 (31%)]\tLoss: 1.069190\n",
            "Train Epoch: 103 [8192000/25334306 (32%)]\tLoss: 0.993327\n",
            "Train Epoch: 103 [8601600/25334306 (34%)]\tLoss: 0.990896\n",
            "Train Epoch: 103 [9011200/25334306 (36%)]\tLoss: 1.041238\n",
            "Train Epoch: 103 [9420800/25334306 (37%)]\tLoss: 1.047762\n",
            "Train Epoch: 103 [9830400/25334306 (39%)]\tLoss: 0.987956\n",
            "Train Epoch: 103 [10240000/25334306 (40%)]\tLoss: 1.016181\n",
            "Train Epoch: 103 [10649600/25334306 (42%)]\tLoss: 1.048140\n",
            "Train Epoch: 103 [11059200/25334306 (44%)]\tLoss: 0.987549\n",
            "Train Epoch: 103 [11468800/25334306 (45%)]\tLoss: 1.012031\n",
            "Train Epoch: 103 [11878400/25334306 (47%)]\tLoss: 0.945471\n",
            "Train Epoch: 103 [12288000/25334306 (49%)]\tLoss: 0.974876\n",
            "Train Epoch: 103 [12697600/25334306 (50%)]\tLoss: 0.992668\n",
            "Train Epoch: 103 [13107200/25334306 (52%)]\tLoss: 1.034662\n",
            "Train Epoch: 103 [13516800/25334306 (53%)]\tLoss: 0.967261\n",
            "Train Epoch: 103 [13926400/25334306 (55%)]\tLoss: 1.035192\n",
            "Train Epoch: 103 [14336000/25334306 (57%)]\tLoss: 0.978336\n",
            "Train Epoch: 103 [14745600/25334306 (58%)]\tLoss: 0.984947\n",
            "Train Epoch: 103 [15155200/25334306 (60%)]\tLoss: 1.002557\n",
            "Train Epoch: 103 [15564800/25334306 (61%)]\tLoss: 1.079078\n",
            "Train Epoch: 103 [15974400/25334306 (63%)]\tLoss: 0.994400\n",
            "Train Epoch: 103 [16384000/25334306 (65%)]\tLoss: 0.993062\n",
            "Train Epoch: 103 [16793600/25334306 (66%)]\tLoss: 1.014186\n",
            "Train Epoch: 103 [17203200/25334306 (68%)]\tLoss: 1.083494\n",
            "Train Epoch: 103 [17612800/25334306 (70%)]\tLoss: 1.025254\n",
            "Train Epoch: 103 [18022400/25334306 (71%)]\tLoss: 1.035348\n",
            "Train Epoch: 103 [18432000/25334306 (73%)]\tLoss: 0.961594\n",
            "Train Epoch: 103 [18841600/25334306 (74%)]\tLoss: 1.021644\n",
            "Train Epoch: 103 [19251200/25334306 (76%)]\tLoss: 0.969066\n",
            "Train Epoch: 103 [19660800/25334306 (78%)]\tLoss: 0.999167\n",
            "Train Epoch: 103 [20070400/25334306 (79%)]\tLoss: 1.011318\n",
            "Train Epoch: 103 [20480000/25334306 (81%)]\tLoss: 1.040645\n",
            "Train Epoch: 103 [20889600/25334306 (82%)]\tLoss: 0.999444\n",
            "Train Epoch: 103 [21299200/25334306 (84%)]\tLoss: 1.027574\n",
            "Train Epoch: 103 [21708800/25334306 (86%)]\tLoss: 1.029728\n",
            "Train Epoch: 103 [22118400/25334306 (87%)]\tLoss: 1.012803\n",
            "Train Epoch: 103 [22528000/25334306 (89%)]\tLoss: 1.095454\n",
            "Train Epoch: 103 [22937600/25334306 (91%)]\tLoss: 0.974223\n",
            "Train Epoch: 103 [23347200/25334306 (92%)]\tLoss: 0.999043\n",
            "Train Epoch: 103 [23756800/25334306 (94%)]\tLoss: 1.021538\n",
            "Train Epoch: 103 [24166400/25334306 (95%)]\tLoss: 0.979300\n",
            "Train Epoch: 103 [24576000/25334306 (97%)]\tLoss: 0.958069\n",
            "Train Epoch: 103 [24985600/25334306 (99%)]\tLoss: 1.004200\n",
            "Train Epoch: 103 [0/10856828 (0%)]\tLoss: 1.025216\n",
            "Train Epoch: 103 [409600/10856828 (4%)]\tLoss: 1.061677\n",
            "Train Epoch: 103 [819200/10856828 (8%)]\tLoss: 1.015042\n",
            "Train Epoch: 103 [1228800/10856828 (11%)]\tLoss: 0.989729\n",
            "Train Epoch: 103 [1638400/10856828 (15%)]\tLoss: 0.958663\n",
            "Train Epoch: 103 [2048000/10856828 (19%)]\tLoss: 1.004970\n",
            "Train Epoch: 103 [2457600/10856828 (23%)]\tLoss: 1.039891\n",
            "Train Epoch: 103 [2867200/10856828 (26%)]\tLoss: 1.037979\n",
            "Train Epoch: 103 [3276800/10856828 (30%)]\tLoss: 1.024163\n",
            "Train Epoch: 103 [3686400/10856828 (34%)]\tLoss: 1.026250\n",
            "Train Epoch: 103 [4096000/10856828 (38%)]\tLoss: 1.037795\n",
            "Train Epoch: 103 [4505600/10856828 (41%)]\tLoss: 1.053665\n",
            "Train Epoch: 103 [4915200/10856828 (45%)]\tLoss: 0.948486\n",
            "Train Epoch: 103 [5324800/10856828 (49%)]\tLoss: 1.002618\n",
            "Train Epoch: 103 [5734400/10856828 (53%)]\tLoss: 0.974063\n",
            "Train Epoch: 103 [6144000/10856828 (57%)]\tLoss: 1.045491\n",
            "Train Epoch: 103 [6553600/10856828 (60%)]\tLoss: 0.977845\n",
            "Train Epoch: 103 [6963200/10856828 (64%)]\tLoss: 1.046013\n",
            "Train Epoch: 103 [7372800/10856828 (68%)]\tLoss: 1.009795\n",
            "Train Epoch: 103 [7782400/10856828 (72%)]\tLoss: 0.987963\n",
            "Train Epoch: 103 [8192000/10856828 (75%)]\tLoss: 1.065454\n",
            "Train Epoch: 103 [8601600/10856828 (79%)]\tLoss: 0.989825\n",
            "Train Epoch: 103 [9011200/10856828 (83%)]\tLoss: 1.016545\n",
            "Train Epoch: 103 [9420800/10856828 (87%)]\tLoss: 0.949921\n",
            "Train Epoch: 103 [9830400/10856828 (91%)]\tLoss: 1.007959\n",
            "Train Epoch: 103 [10240000/10856828 (94%)]\tLoss: 0.994858\n",
            "Train Epoch: 103 [10649600/10856828 (98%)]\tLoss: 1.006708\n",
            "Dev accuracy:  0.8697535375556904\n",
            " 31.36292969385783 minutes\n",
            "Train Epoch: 104 [0/25334306 (0%)]\tLoss: 0.981842\n",
            "Train Epoch: 104 [409600/25334306 (2%)]\tLoss: 0.985899\n",
            "Train Epoch: 104 [819200/25334306 (3%)]\tLoss: 0.961971\n",
            "Train Epoch: 104 [1228800/25334306 (5%)]\tLoss: 0.979623\n",
            "Train Epoch: 104 [1638400/25334306 (6%)]\tLoss: 0.974182\n",
            "Train Epoch: 104 [2048000/25334306 (8%)]\tLoss: 1.052984\n",
            "Train Epoch: 104 [2457600/25334306 (10%)]\tLoss: 0.890122\n",
            "Train Epoch: 104 [2867200/25334306 (11%)]\tLoss: 0.935026\n",
            "Train Epoch: 104 [3276800/25334306 (13%)]\tLoss: 1.074542\n",
            "Train Epoch: 104 [3686400/25334306 (15%)]\tLoss: 1.040316\n",
            "Train Epoch: 104 [4096000/25334306 (16%)]\tLoss: 0.914548\n",
            "Train Epoch: 104 [4505600/25334306 (18%)]\tLoss: 0.959790\n",
            "Train Epoch: 104 [4915200/25334306 (19%)]\tLoss: 1.005360\n",
            "Train Epoch: 104 [5324800/25334306 (21%)]\tLoss: 1.050634\n",
            "Train Epoch: 104 [5734400/25334306 (23%)]\tLoss: 1.003531\n",
            "Train Epoch: 104 [6144000/25334306 (24%)]\tLoss: 0.979667\n",
            "Train Epoch: 104 [6553600/25334306 (26%)]\tLoss: 0.958646\n",
            "Train Epoch: 104 [6963200/25334306 (27%)]\tLoss: 0.969760\n",
            "Train Epoch: 104 [7372800/25334306 (29%)]\tLoss: 0.967636\n",
            "Train Epoch: 104 [7782400/25334306 (31%)]\tLoss: 1.009323\n",
            "Train Epoch: 104 [8192000/25334306 (32%)]\tLoss: 0.964756\n",
            "Train Epoch: 104 [8601600/25334306 (34%)]\tLoss: 1.017049\n",
            "Train Epoch: 104 [9011200/25334306 (36%)]\tLoss: 1.020325\n",
            "Train Epoch: 104 [9420800/25334306 (37%)]\tLoss: 0.988932\n",
            "Train Epoch: 104 [9830400/25334306 (39%)]\tLoss: 1.062489\n",
            "Train Epoch: 104 [10240000/25334306 (40%)]\tLoss: 1.015041\n",
            "Train Epoch: 104 [10649600/25334306 (42%)]\tLoss: 1.008952\n",
            "Train Epoch: 104 [11059200/25334306 (44%)]\tLoss: 0.938856\n",
            "Train Epoch: 104 [11468800/25334306 (45%)]\tLoss: 1.052950\n",
            "Train Epoch: 104 [11878400/25334306 (47%)]\tLoss: 1.053322\n",
            "Train Epoch: 104 [12288000/25334306 (49%)]\tLoss: 1.055373\n",
            "Train Epoch: 104 [12697600/25334306 (50%)]\tLoss: 0.991466\n",
            "Train Epoch: 104 [13107200/25334306 (52%)]\tLoss: 1.029908\n",
            "Train Epoch: 104 [13516800/25334306 (53%)]\tLoss: 0.967501\n",
            "Train Epoch: 104 [13926400/25334306 (55%)]\tLoss: 0.967792\n",
            "Train Epoch: 104 [14336000/25334306 (57%)]\tLoss: 1.057991\n",
            "Train Epoch: 104 [14745600/25334306 (58%)]\tLoss: 1.015624\n",
            "Train Epoch: 104 [15155200/25334306 (60%)]\tLoss: 1.007822\n",
            "Train Epoch: 104 [15564800/25334306 (61%)]\tLoss: 0.968756\n",
            "Train Epoch: 104 [15974400/25334306 (63%)]\tLoss: 1.039518\n",
            "Train Epoch: 104 [16384000/25334306 (65%)]\tLoss: 0.960536\n",
            "Train Epoch: 104 [16793600/25334306 (66%)]\tLoss: 1.026311\n",
            "Train Epoch: 104 [17203200/25334306 (68%)]\tLoss: 1.054011\n",
            "Train Epoch: 104 [17612800/25334306 (70%)]\tLoss: 0.977122\n",
            "Train Epoch: 104 [18022400/25334306 (71%)]\tLoss: 1.064900\n",
            "Train Epoch: 104 [18432000/25334306 (73%)]\tLoss: 1.013076\n",
            "Train Epoch: 104 [18841600/25334306 (74%)]\tLoss: 1.006655\n",
            "Train Epoch: 104 [19251200/25334306 (76%)]\tLoss: 1.012218\n",
            "Train Epoch: 104 [19660800/25334306 (78%)]\tLoss: 1.064624\n",
            "Train Epoch: 104 [20070400/25334306 (79%)]\tLoss: 0.987592\n",
            "Train Epoch: 104 [20480000/25334306 (81%)]\tLoss: 0.961075\n",
            "Train Epoch: 104 [20889600/25334306 (82%)]\tLoss: 1.033206\n",
            "Train Epoch: 104 [21299200/25334306 (84%)]\tLoss: 0.976175\n",
            "Train Epoch: 104 [21708800/25334306 (86%)]\tLoss: 1.036714\n",
            "Train Epoch: 104 [22118400/25334306 (87%)]\tLoss: 0.976477\n",
            "Train Epoch: 104 [22528000/25334306 (89%)]\tLoss: 1.029775\n",
            "Train Epoch: 104 [22937600/25334306 (91%)]\tLoss: 1.000118\n",
            "Train Epoch: 104 [23347200/25334306 (92%)]\tLoss: 1.025342\n",
            "Train Epoch: 104 [23756800/25334306 (94%)]\tLoss: 0.938056\n",
            "Train Epoch: 104 [24166400/25334306 (95%)]\tLoss: 1.047538\n",
            "Train Epoch: 104 [24576000/25334306 (97%)]\tLoss: 0.941827\n",
            "Train Epoch: 104 [24985600/25334306 (99%)]\tLoss: 0.963386\n",
            "Train Epoch: 104 [0/10856828 (0%)]\tLoss: 1.028892\n",
            "Train Epoch: 104 [409600/10856828 (4%)]\tLoss: 0.988549\n",
            "Train Epoch: 104 [819200/10856828 (8%)]\tLoss: 1.009565\n",
            "Train Epoch: 104 [1228800/10856828 (11%)]\tLoss: 1.007317\n",
            "Train Epoch: 104 [1638400/10856828 (15%)]\tLoss: 1.010064\n",
            "Train Epoch: 104 [2048000/10856828 (19%)]\tLoss: 0.997139\n",
            "Train Epoch: 104 [2457600/10856828 (23%)]\tLoss: 1.052096\n",
            "Train Epoch: 104 [2867200/10856828 (26%)]\tLoss: 0.988152\n",
            "Train Epoch: 104 [3276800/10856828 (30%)]\tLoss: 0.971781\n",
            "Train Epoch: 104 [3686400/10856828 (34%)]\tLoss: 0.976289\n",
            "Train Epoch: 104 [4096000/10856828 (38%)]\tLoss: 1.020478\n",
            "Train Epoch: 104 [4505600/10856828 (41%)]\tLoss: 1.018926\n",
            "Train Epoch: 104 [4915200/10856828 (45%)]\tLoss: 0.987453\n",
            "Train Epoch: 104 [5324800/10856828 (49%)]\tLoss: 1.031420\n",
            "Train Epoch: 104 [5734400/10856828 (53%)]\tLoss: 0.983063\n",
            "Train Epoch: 104 [6144000/10856828 (57%)]\tLoss: 1.036486\n",
            "Train Epoch: 104 [6553600/10856828 (60%)]\tLoss: 1.031213\n",
            "Train Epoch: 104 [6963200/10856828 (64%)]\tLoss: 1.032903\n",
            "Train Epoch: 104 [7372800/10856828 (68%)]\tLoss: 1.014122\n",
            "Train Epoch: 104 [7782400/10856828 (72%)]\tLoss: 1.025097\n",
            "Train Epoch: 104 [8192000/10856828 (75%)]\tLoss: 0.938178\n",
            "Train Epoch: 104 [8601600/10856828 (79%)]\tLoss: 0.964400\n",
            "Train Epoch: 104 [9011200/10856828 (83%)]\tLoss: 1.007797\n",
            "Train Epoch: 104 [9420800/10856828 (87%)]\tLoss: 0.978405\n",
            "Train Epoch: 104 [9830400/10856828 (91%)]\tLoss: 0.996571\n",
            "Train Epoch: 104 [10240000/10856828 (94%)]\tLoss: 0.951636\n",
            "Train Epoch: 104 [10649600/10856828 (98%)]\tLoss: 1.063369\n",
            "Dev accuracy:  0.8696508276662248\n",
            " 31.28167654275894 minutes\n",
            "Train Epoch: 105 [0/25334306 (0%)]\tLoss: 0.975860\n",
            "Train Epoch: 105 [409600/25334306 (2%)]\tLoss: 0.937600\n",
            "Train Epoch: 105 [819200/25334306 (3%)]\tLoss: 1.025310\n",
            "Train Epoch: 105 [1228800/25334306 (5%)]\tLoss: 0.927420\n",
            "Train Epoch: 105 [1638400/25334306 (6%)]\tLoss: 1.012455\n",
            "Train Epoch: 105 [2048000/25334306 (8%)]\tLoss: 0.985388\n",
            "Train Epoch: 105 [2457600/25334306 (10%)]\tLoss: 1.007862\n",
            "Train Epoch: 105 [2867200/25334306 (11%)]\tLoss: 0.986997\n",
            "Train Epoch: 105 [3276800/25334306 (13%)]\tLoss: 0.996763\n",
            "Train Epoch: 105 [3686400/25334306 (15%)]\tLoss: 0.958412\n",
            "Train Epoch: 105 [4096000/25334306 (16%)]\tLoss: 1.000449\n",
            "Train Epoch: 105 [4505600/25334306 (18%)]\tLoss: 1.044554\n",
            "Train Epoch: 105 [4915200/25334306 (19%)]\tLoss: 1.028931\n",
            "Train Epoch: 105 [5324800/25334306 (21%)]\tLoss: 0.926216\n",
            "Train Epoch: 105 [5734400/25334306 (23%)]\tLoss: 1.043049\n",
            "Train Epoch: 105 [6144000/25334306 (24%)]\tLoss: 0.997616\n",
            "Train Epoch: 105 [6553600/25334306 (26%)]\tLoss: 0.969204\n",
            "Train Epoch: 105 [6963200/25334306 (27%)]\tLoss: 0.973017\n",
            "Train Epoch: 105 [7372800/25334306 (29%)]\tLoss: 0.971397\n",
            "Train Epoch: 105 [7782400/25334306 (31%)]\tLoss: 0.955662\n",
            "Train Epoch: 105 [8192000/25334306 (32%)]\tLoss: 0.944289\n",
            "Train Epoch: 105 [8601600/25334306 (34%)]\tLoss: 1.023286\n",
            "Train Epoch: 105 [9011200/25334306 (36%)]\tLoss: 1.022167\n",
            "Train Epoch: 105 [9420800/25334306 (37%)]\tLoss: 1.033383\n",
            "Train Epoch: 105 [9830400/25334306 (39%)]\tLoss: 0.972483\n",
            "Train Epoch: 105 [10240000/25334306 (40%)]\tLoss: 0.964284\n",
            "Train Epoch: 105 [10649600/25334306 (42%)]\tLoss: 1.012086\n",
            "Train Epoch: 105 [11059200/25334306 (44%)]\tLoss: 0.970773\n",
            "Train Epoch: 105 [11468800/25334306 (45%)]\tLoss: 1.058717\n",
            "Train Epoch: 105 [11878400/25334306 (47%)]\tLoss: 1.040712\n",
            "Train Epoch: 105 [12288000/25334306 (49%)]\tLoss: 1.017334\n",
            "Train Epoch: 105 [12697600/25334306 (50%)]\tLoss: 0.992131\n",
            "Train Epoch: 105 [13107200/25334306 (52%)]\tLoss: 1.071576\n",
            "Train Epoch: 105 [13516800/25334306 (53%)]\tLoss: 1.017831\n",
            "Train Epoch: 105 [13926400/25334306 (55%)]\tLoss: 0.998683\n",
            "Train Epoch: 105 [14336000/25334306 (57%)]\tLoss: 1.027182\n",
            "Train Epoch: 105 [14745600/25334306 (58%)]\tLoss: 1.006944\n",
            "Train Epoch: 105 [15155200/25334306 (60%)]\tLoss: 1.002013\n",
            "Train Epoch: 105 [15564800/25334306 (61%)]\tLoss: 1.014655\n",
            "Train Epoch: 105 [15974400/25334306 (63%)]\tLoss: 0.981097\n",
            "Train Epoch: 105 [16384000/25334306 (65%)]\tLoss: 1.014078\n",
            "Train Epoch: 105 [16793600/25334306 (66%)]\tLoss: 1.011632\n",
            "Train Epoch: 105 [17203200/25334306 (68%)]\tLoss: 1.016133\n",
            "Train Epoch: 105 [17612800/25334306 (70%)]\tLoss: 0.995405\n",
            "Train Epoch: 105 [18022400/25334306 (71%)]\tLoss: 1.081962\n",
            "Train Epoch: 105 [18432000/25334306 (73%)]\tLoss: 0.974967\n",
            "Train Epoch: 105 [18841600/25334306 (74%)]\tLoss: 0.987472\n",
            "Train Epoch: 105 [19251200/25334306 (76%)]\tLoss: 0.974572\n",
            "Train Epoch: 105 [19660800/25334306 (78%)]\tLoss: 1.005653\n",
            "Train Epoch: 105 [20070400/25334306 (79%)]\tLoss: 1.033843\n",
            "Train Epoch: 105 [20480000/25334306 (81%)]\tLoss: 0.960053\n",
            "Train Epoch: 105 [20889600/25334306 (82%)]\tLoss: 1.053057\n",
            "Train Epoch: 105 [21299200/25334306 (84%)]\tLoss: 1.057812\n",
            "Train Epoch: 105 [21708800/25334306 (86%)]\tLoss: 1.028780\n",
            "Train Epoch: 105 [22118400/25334306 (87%)]\tLoss: 0.984985\n",
            "Train Epoch: 105 [22528000/25334306 (89%)]\tLoss: 0.994328\n",
            "Train Epoch: 105 [22937600/25334306 (91%)]\tLoss: 0.983251\n",
            "Train Epoch: 105 [23347200/25334306 (92%)]\tLoss: 1.019538\n",
            "Train Epoch: 105 [23756800/25334306 (94%)]\tLoss: 0.984977\n",
            "Train Epoch: 105 [24166400/25334306 (95%)]\tLoss: 0.970219\n",
            "Train Epoch: 105 [24576000/25334306 (97%)]\tLoss: 0.935008\n",
            "Train Epoch: 105 [24985600/25334306 (99%)]\tLoss: 0.953945\n",
            "Train Epoch: 105 [0/10856828 (0%)]\tLoss: 1.036742\n",
            "Train Epoch: 105 [409600/10856828 (4%)]\tLoss: 0.984280\n",
            "Train Epoch: 105 [819200/10856828 (8%)]\tLoss: 0.983111\n",
            "Train Epoch: 105 [1228800/10856828 (11%)]\tLoss: 0.947673\n",
            "Train Epoch: 105 [1638400/10856828 (15%)]\tLoss: 1.015278\n",
            "Train Epoch: 105 [2048000/10856828 (19%)]\tLoss: 0.982088\n",
            "Train Epoch: 105 [2457600/10856828 (23%)]\tLoss: 0.974009\n",
            "Train Epoch: 105 [2867200/10856828 (26%)]\tLoss: 0.970779\n",
            "Train Epoch: 105 [3276800/10856828 (30%)]\tLoss: 0.989610\n",
            "Train Epoch: 105 [3686400/10856828 (34%)]\tLoss: 1.008070\n",
            "Train Epoch: 105 [4096000/10856828 (38%)]\tLoss: 0.968681\n",
            "Train Epoch: 105 [4505600/10856828 (41%)]\tLoss: 0.981261\n",
            "Train Epoch: 105 [4915200/10856828 (45%)]\tLoss: 0.966155\n",
            "Train Epoch: 105 [5324800/10856828 (49%)]\tLoss: 1.026072\n",
            "Train Epoch: 105 [5734400/10856828 (53%)]\tLoss: 1.050588\n",
            "Train Epoch: 105 [6144000/10856828 (57%)]\tLoss: 0.997816\n",
            "Train Epoch: 105 [6553600/10856828 (60%)]\tLoss: 1.066465\n",
            "Train Epoch: 105 [6963200/10856828 (64%)]\tLoss: 1.027189\n",
            "Train Epoch: 105 [7372800/10856828 (68%)]\tLoss: 0.944036\n",
            "Train Epoch: 105 [7782400/10856828 (72%)]\tLoss: 1.016279\n",
            "Train Epoch: 105 [8192000/10856828 (75%)]\tLoss: 1.026321\n",
            "Train Epoch: 105 [8601600/10856828 (79%)]\tLoss: 0.953737\n",
            "Train Epoch: 105 [9011200/10856828 (83%)]\tLoss: 0.993415\n",
            "Train Epoch: 105 [9420800/10856828 (87%)]\tLoss: 0.957168\n",
            "Train Epoch: 105 [9830400/10856828 (91%)]\tLoss: 1.023701\n",
            "Train Epoch: 105 [10240000/10856828 (94%)]\tLoss: 0.956155\n",
            "Train Epoch: 105 [10649600/10856828 (98%)]\tLoss: 1.046506\n",
            "Dev accuracy:  0.8696286340720187\n",
            " 31.280490136146547 minutes\n",
            "Train Epoch: 106 [0/25334306 (0%)]\tLoss: 1.002182\n",
            "Train Epoch: 106 [409600/25334306 (2%)]\tLoss: 1.002555\n",
            "Train Epoch: 106 [819200/25334306 (3%)]\tLoss: 0.968147\n",
            "Train Epoch: 106 [1228800/25334306 (5%)]\tLoss: 1.014740\n",
            "Train Epoch: 106 [1638400/25334306 (6%)]\tLoss: 0.953329\n",
            "Train Epoch: 106 [2048000/25334306 (8%)]\tLoss: 0.937376\n",
            "Train Epoch: 106 [2457600/25334306 (10%)]\tLoss: 0.997126\n",
            "Train Epoch: 106 [2867200/25334306 (11%)]\tLoss: 1.013758\n",
            "Train Epoch: 106 [3276800/25334306 (13%)]\tLoss: 1.004712\n",
            "Train Epoch: 106 [3686400/25334306 (15%)]\tLoss: 0.993933\n",
            "Train Epoch: 106 [4096000/25334306 (16%)]\tLoss: 1.021110\n",
            "Train Epoch: 106 [4505600/25334306 (18%)]\tLoss: 0.977388\n",
            "Train Epoch: 106 [4915200/25334306 (19%)]\tLoss: 0.977760\n",
            "Train Epoch: 106 [5324800/25334306 (21%)]\tLoss: 0.973514\n",
            "Train Epoch: 106 [5734400/25334306 (23%)]\tLoss: 0.974976\n",
            "Train Epoch: 106 [6144000/25334306 (24%)]\tLoss: 1.027089\n",
            "Train Epoch: 106 [6553600/25334306 (26%)]\tLoss: 0.972047\n",
            "Train Epoch: 106 [6963200/25334306 (27%)]\tLoss: 0.965028\n",
            "Train Epoch: 106 [7372800/25334306 (29%)]\tLoss: 0.993224\n",
            "Train Epoch: 106 [7782400/25334306 (31%)]\tLoss: 0.994099\n",
            "Train Epoch: 106 [8192000/25334306 (32%)]\tLoss: 1.033259\n",
            "Train Epoch: 106 [8601600/25334306 (34%)]\tLoss: 0.929407\n",
            "Train Epoch: 106 [9011200/25334306 (36%)]\tLoss: 1.002398\n",
            "Train Epoch: 106 [9420800/25334306 (37%)]\tLoss: 0.965003\n",
            "Train Epoch: 106 [9830400/25334306 (39%)]\tLoss: 1.017701\n",
            "Train Epoch: 106 [10240000/25334306 (40%)]\tLoss: 0.993662\n",
            "Train Epoch: 106 [10649600/25334306 (42%)]\tLoss: 1.008228\n",
            "Train Epoch: 106 [11059200/25334306 (44%)]\tLoss: 1.055558\n",
            "Train Epoch: 106 [11468800/25334306 (45%)]\tLoss: 1.017744\n",
            "Train Epoch: 106 [11878400/25334306 (47%)]\tLoss: 0.977321\n",
            "Train Epoch: 106 [12288000/25334306 (49%)]\tLoss: 1.045792\n",
            "Train Epoch: 106 [12697600/25334306 (50%)]\tLoss: 0.995199\n",
            "Train Epoch: 106 [13107200/25334306 (52%)]\tLoss: 0.919429\n",
            "Train Epoch: 106 [13516800/25334306 (53%)]\tLoss: 0.989187\n",
            "Train Epoch: 106 [13926400/25334306 (55%)]\tLoss: 1.034517\n",
            "Train Epoch: 106 [14336000/25334306 (57%)]\tLoss: 0.957457\n",
            "Train Epoch: 106 [14745600/25334306 (58%)]\tLoss: 0.919999\n",
            "Train Epoch: 106 [15155200/25334306 (60%)]\tLoss: 0.977158\n",
            "Train Epoch: 106 [15564800/25334306 (61%)]\tLoss: 0.962420\n",
            "Train Epoch: 106 [15974400/25334306 (63%)]\tLoss: 0.997320\n",
            "Train Epoch: 106 [16384000/25334306 (65%)]\tLoss: 1.064353\n",
            "Train Epoch: 106 [16793600/25334306 (66%)]\tLoss: 0.951008\n",
            "Train Epoch: 106 [17203200/25334306 (68%)]\tLoss: 1.000574\n",
            "Train Epoch: 106 [17612800/25334306 (70%)]\tLoss: 0.994076\n",
            "Train Epoch: 106 [18022400/25334306 (71%)]\tLoss: 0.958406\n",
            "Train Epoch: 106 [18432000/25334306 (73%)]\tLoss: 1.010466\n",
            "Train Epoch: 106 [18841600/25334306 (74%)]\tLoss: 0.984830\n",
            "Train Epoch: 106 [19251200/25334306 (76%)]\tLoss: 1.010643\n",
            "Train Epoch: 106 [19660800/25334306 (78%)]\tLoss: 0.970861\n",
            "Train Epoch: 106 [20070400/25334306 (79%)]\tLoss: 1.031842\n",
            "Train Epoch: 106 [20480000/25334306 (81%)]\tLoss: 1.014222\n",
            "Train Epoch: 106 [20889600/25334306 (82%)]\tLoss: 1.033463\n",
            "Train Epoch: 106 [21299200/25334306 (84%)]\tLoss: 1.010476\n",
            "Train Epoch: 106 [21708800/25334306 (86%)]\tLoss: 1.004523\n",
            "Train Epoch: 106 [22118400/25334306 (87%)]\tLoss: 0.959364\n",
            "Train Epoch: 106 [22528000/25334306 (89%)]\tLoss: 1.005687\n",
            "Train Epoch: 106 [22937600/25334306 (91%)]\tLoss: 0.990798\n",
            "Train Epoch: 106 [23347200/25334306 (92%)]\tLoss: 0.990355\n",
            "Train Epoch: 106 [23756800/25334306 (94%)]\tLoss: 1.028171\n",
            "Train Epoch: 106 [24166400/25334306 (95%)]\tLoss: 0.931474\n",
            "Train Epoch: 106 [24576000/25334306 (97%)]\tLoss: 0.911374\n",
            "Train Epoch: 106 [24985600/25334306 (99%)]\tLoss: 0.941381\n",
            "Train Epoch: 106 [0/10856828 (0%)]\tLoss: 1.032714\n",
            "Train Epoch: 106 [409600/10856828 (4%)]\tLoss: 0.967447\n",
            "Train Epoch: 106 [819200/10856828 (8%)]\tLoss: 0.998599\n",
            "Train Epoch: 106 [1228800/10856828 (11%)]\tLoss: 0.959294\n",
            "Train Epoch: 106 [1638400/10856828 (15%)]\tLoss: 1.032300\n",
            "Train Epoch: 106 [2048000/10856828 (19%)]\tLoss: 0.995538\n",
            "Train Epoch: 106 [2457600/10856828 (23%)]\tLoss: 0.964834\n",
            "Train Epoch: 106 [2867200/10856828 (26%)]\tLoss: 1.025229\n",
            "Train Epoch: 106 [3276800/10856828 (30%)]\tLoss: 1.022567\n",
            "Train Epoch: 106 [3686400/10856828 (34%)]\tLoss: 0.995715\n",
            "Train Epoch: 106 [4096000/10856828 (38%)]\tLoss: 1.022662\n",
            "Train Epoch: 106 [4505600/10856828 (41%)]\tLoss: 1.016429\n",
            "Train Epoch: 106 [4915200/10856828 (45%)]\tLoss: 1.059125\n",
            "Train Epoch: 106 [5324800/10856828 (49%)]\tLoss: 0.964096\n",
            "Train Epoch: 106 [5734400/10856828 (53%)]\tLoss: 1.001783\n",
            "Train Epoch: 106 [6144000/10856828 (57%)]\tLoss: 1.037338\n",
            "Train Epoch: 106 [6553600/10856828 (60%)]\tLoss: 0.979276\n",
            "Train Epoch: 106 [6963200/10856828 (64%)]\tLoss: 0.994973\n",
            "Train Epoch: 106 [7372800/10856828 (68%)]\tLoss: 1.017314\n",
            "Train Epoch: 106 [7782400/10856828 (72%)]\tLoss: 1.032265\n",
            "Train Epoch: 106 [8192000/10856828 (75%)]\tLoss: 0.951404\n",
            "Train Epoch: 106 [8601600/10856828 (79%)]\tLoss: 0.976815\n",
            "Train Epoch: 106 [9011200/10856828 (83%)]\tLoss: 0.947114\n",
            "Train Epoch: 106 [9420800/10856828 (87%)]\tLoss: 0.970073\n",
            "Train Epoch: 106 [9830400/10856828 (91%)]\tLoss: 1.041249\n",
            "Train Epoch: 106 [10240000/10856828 (94%)]\tLoss: 0.993843\n",
            "Train Epoch: 106 [10649600/10856828 (98%)]\tLoss: 0.985628\n",
            "Dev accuracy:  0.8698438603228084\n",
            " 31.285792485872904 minutes\n",
            "Train Epoch: 107 [0/25334306 (0%)]\tLoss: 0.986915\n",
            "Train Epoch: 107 [409600/25334306 (2%)]\tLoss: 0.998771\n",
            "Train Epoch: 107 [819200/25334306 (3%)]\tLoss: 0.990350\n",
            "Train Epoch: 107 [1228800/25334306 (5%)]\tLoss: 0.991027\n",
            "Train Epoch: 107 [1638400/25334306 (6%)]\tLoss: 0.985771\n",
            "Train Epoch: 107 [2048000/25334306 (8%)]\tLoss: 0.987887\n",
            "Train Epoch: 107 [2457600/25334306 (10%)]\tLoss: 0.949876\n",
            "Train Epoch: 107 [2867200/25334306 (11%)]\tLoss: 0.993080\n",
            "Train Epoch: 107 [3276800/25334306 (13%)]\tLoss: 0.948817\n",
            "Train Epoch: 107 [3686400/25334306 (15%)]\tLoss: 0.993869\n",
            "Train Epoch: 107 [4096000/25334306 (16%)]\tLoss: 0.946852\n",
            "Train Epoch: 107 [4505600/25334306 (18%)]\tLoss: 1.032876\n",
            "Train Epoch: 107 [4915200/25334306 (19%)]\tLoss: 0.983437\n",
            "Train Epoch: 107 [5324800/25334306 (21%)]\tLoss: 1.024128\n",
            "Train Epoch: 107 [5734400/25334306 (23%)]\tLoss: 1.002465\n",
            "Train Epoch: 107 [6144000/25334306 (24%)]\tLoss: 0.972375\n",
            "Train Epoch: 107 [6553600/25334306 (26%)]\tLoss: 1.010124\n",
            "Train Epoch: 107 [6963200/25334306 (27%)]\tLoss: 1.030627\n",
            "Train Epoch: 107 [7372800/25334306 (29%)]\tLoss: 1.022876\n",
            "Train Epoch: 107 [7782400/25334306 (31%)]\tLoss: 0.979181\n",
            "Train Epoch: 107 [8192000/25334306 (32%)]\tLoss: 1.033144\n",
            "Train Epoch: 107 [8601600/25334306 (34%)]\tLoss: 0.990829\n",
            "Train Epoch: 107 [9011200/25334306 (36%)]\tLoss: 0.943397\n",
            "Train Epoch: 107 [9420800/25334306 (37%)]\tLoss: 0.982315\n",
            "Train Epoch: 107 [9830400/25334306 (39%)]\tLoss: 1.024367\n",
            "Train Epoch: 107 [10240000/25334306 (40%)]\tLoss: 0.959351\n",
            "Train Epoch: 107 [10649600/25334306 (42%)]\tLoss: 1.063563\n",
            "Train Epoch: 107 [11059200/25334306 (44%)]\tLoss: 1.054126\n",
            "Train Epoch: 107 [11468800/25334306 (45%)]\tLoss: 1.053447\n",
            "Train Epoch: 107 [11878400/25334306 (47%)]\tLoss: 0.985347\n",
            "Train Epoch: 107 [12288000/25334306 (49%)]\tLoss: 1.007626\n",
            "Train Epoch: 107 [12697600/25334306 (50%)]\tLoss: 1.026602\n",
            "Train Epoch: 107 [13107200/25334306 (52%)]\tLoss: 1.028760\n",
            "Train Epoch: 107 [13516800/25334306 (53%)]\tLoss: 1.007421\n",
            "Train Epoch: 107 [13926400/25334306 (55%)]\tLoss: 0.972882\n",
            "Train Epoch: 107 [14336000/25334306 (57%)]\tLoss: 0.985049\n",
            "Train Epoch: 107 [14745600/25334306 (58%)]\tLoss: 1.059179\n",
            "Train Epoch: 107 [15155200/25334306 (60%)]\tLoss: 1.024270\n",
            "Train Epoch: 107 [15564800/25334306 (61%)]\tLoss: 0.966627\n",
            "Train Epoch: 107 [15974400/25334306 (63%)]\tLoss: 1.037483\n",
            "Train Epoch: 107 [16384000/25334306 (65%)]\tLoss: 0.991292\n",
            "Train Epoch: 107 [16793600/25334306 (66%)]\tLoss: 0.994053\n",
            "Train Epoch: 107 [17203200/25334306 (68%)]\tLoss: 0.951552\n",
            "Train Epoch: 107 [17612800/25334306 (70%)]\tLoss: 0.998170\n",
            "Train Epoch: 107 [18022400/25334306 (71%)]\tLoss: 0.960539\n",
            "Train Epoch: 107 [18432000/25334306 (73%)]\tLoss: 1.037189\n",
            "Train Epoch: 107 [18841600/25334306 (74%)]\tLoss: 1.021267\n",
            "Train Epoch: 107 [19251200/25334306 (76%)]\tLoss: 0.978063\n",
            "Train Epoch: 107 [19660800/25334306 (78%)]\tLoss: 1.042288\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import Counter\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
        "from torch.utils.data import ConcatDataset\n",
        "\n",
        "import time\n",
        "import sys\n",
        "\n",
        "class Layer(nn.Module):\n",
        "    def __init__(self, in_size, out_size, dropout=0.25):\n",
        "        super(Layer, self).__init__()\n",
        "\n",
        "        if args['activation'].lower() == \"Tanh\".lower():\n",
        "            nn_activation = nn.Tanh()\n",
        "        elif args['activation'].lower() == \"LeakyReLU\".lower():\n",
        "            nn_activation = nn.LeakyReLU()\n",
        "        elif args['activation'].lower() == \"ELU\".lower():\n",
        "            nn_activation = nn.ELU()\n",
        "        else:\n",
        "            nn_activation = nn.ReLU()\n",
        "\n",
        "        standard_layer = [\n",
        "            nn.Linear(in_features=in_size, out_features=out_size),\n",
        "            nn.BatchNorm1d(out_size),\n",
        "            nn_activation,\n",
        "            nn.Dropout(dropout)\n",
        "        ]\n",
        "        self.layer = nn.Sequential(*standard_layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "        \n",
        "class Network(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        # each is 13 dim letter\n",
        "        context = args['context']\n",
        "        in_size = 13 * (2 * context + 1)\n",
        "        layers = [\n",
        "           Layer(in_size, 4096),\n",
        "            Layer(4096, 4096),\n",
        "            Layer(4096, 4096),\n",
        "            Layer(4096, 4096),\n",
        "            Layer(4096, 4096),\n",
        "            Layer(4096, 2048),            \n",
        "            Layer(2048, 1024),\n",
        "            Layer(1024, 40),\n",
        "        ]\n",
        "        self.laysers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, A0):\n",
        "        x = self.laysers(A0)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LibriSamples(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_path, sample=20000, shuffle=True, partition=\"dev-clean\", csvpath=None, submission=None):\n",
        "        # sample represent how many npy files will be preloaded for one __getitem__ call\n",
        "        self.sample = sample\n",
        "\n",
        "        self.X_dir = data_path + \"/\" + partition + \"/mfcc/\"\n",
        "        self.Y_dir = data_path + \"/\" + partition + \"/transcript/\"\n",
        "\n",
        "        self.X_names = os.listdir(self.X_dir)\n",
        "        self.Y_names = os.listdir(self.Y_dir)\n",
        "\n",
        "        # using a small part of the dataset to debug\n",
        "        if csvpath:\n",
        "            subset = self.parse_csv(csvpath)\n",
        "            self.X_names = [i for i in self.X_names if i in subset]\n",
        "            self.Y_names = [i for i in self.Y_names if i in subset]\n",
        "\n",
        "        if shuffle == True:\n",
        "            XY_names = list(zip(self.X_names, self.Y_names))\n",
        "            random.shuffle(XY_names)\n",
        "            self.X_names, self.Y_names = zip(*XY_names)\n",
        "\n",
        "        assert (len(self.X_names) == len(self.Y_names))\n",
        "        self.length = len(self.X_names)\n",
        "\n",
        "        self.PHONEMES = [\n",
        "            'SIL', 'AA', 'AE', 'AH', 'AO', 'AW', 'AY',\n",
        "            'B', 'CH', 'D', 'DH', 'EH', 'ER', 'EY',\n",
        "            'F', 'G', 'HH', 'IH', 'IY', 'JH', 'K',\n",
        "            'L', 'M', 'N', 'NG', 'OW', 'OY', 'P',\n",
        "            'R', 'S', 'SH', 'T', 'TH', 'UH', 'UW',\n",
        "            'V', 'W', 'Y', 'Z', 'ZH', '<sos>', '<eos>']\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_csv(filepath):\n",
        "        subset = []\n",
        "        with open(filepath) as f:\n",
        "            f_csv = csv.reader(f)\n",
        "            for row in f_csv:\n",
        "                subset.append(row[1])\n",
        "        return subset[1:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(self.length / self.sample))\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        sample_range = range(i * self.sample, min((i + 1) * self.sample, self.length))\n",
        "\n",
        "        X, Y = [], []\n",
        "        for j in sample_range:\n",
        "            X_path = self.X_dir + self.X_names[j]\n",
        "            Y_path = self.Y_dir + self.Y_names[j]\n",
        "\n",
        "            label = [self.PHONEMES.index(yy) for yy in np.load(Y_path)][1:-1]\n",
        "\n",
        "            X_data = np.load(X_path)\n",
        "            X_data = (X_data - X_data.mean(axis=0)) / X_data.std(axis=0)\n",
        "            X.append(X_data)\n",
        "            Y.append(np.array(label))\n",
        "\n",
        "        X, Y = np.concatenate(X), np.concatenate(Y)\n",
        "        return X, Y\n",
        "\n",
        "\n",
        "\n",
        "class LibriItems(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, Y, context=0):\n",
        "        assert (X.shape[0] == Y.shape[0])\n",
        "\n",
        "        self.length = X.shape[0]\n",
        "        self.context = context\n",
        "\n",
        "        if context == 0:\n",
        "            self.X, self.Y = X, Y\n",
        "        else:\n",
        "            self.X = np.pad(X, ((context, context), (0, 0)), 'constant', constant_values=0)\n",
        "            self.Y = Y\n",
        "            pass\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        if self.context == 0:\n",
        "            xx = self.X[i].flatten()\n",
        "            yy = self.Y[i]\n",
        "        else:\n",
        "            start_i = i\n",
        "            end_i = i + 2 * self.context + 1\n",
        "            xx = self.X[start_i:end_i].flatten()\n",
        "            yy = self.Y[i]\n",
        "\n",
        "            pass\n",
        "        return xx, yy\n",
        "\n",
        "\n",
        "def train(args, model, device, train_samples, optimizer, criterion, epoch, scaler):\n",
        "    model.train()\n",
        "    running_loss = 0.0      \n",
        "    for i in range(len(train_samples)):\n",
        "        X, Y = train_samples[i]\n",
        "        train_items = LibriItems(X, Y, context=args['context'])\n",
        "        train_loader = torch.utils.data.DataLoader(train_items, batch_size=args['batch_size'], shuffle=True)\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data = data.float().to(device)\n",
        "            target = target.long().to(device)\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                optimizer.zero_grad()\n",
        "                output = model(data)\n",
        "                loss = criterion(output, target)\n",
        "\n",
        "                # loss.backward()\n",
        "\n",
        "                scaler.scale(loss).backward()  # get gradients w.r.t the loss\n",
        "                scaler.step(optimizer)  # update the weights\n",
        "\n",
        "                scaler.update()\n",
        "                if batch_idx % args['log_interval'] == 0:\n",
        "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                               100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "    running_loss = running_loss / len(train_samples)\n",
        "    # print(\"Training loss is: \",running_loss)\n",
        "    return running_loss\n",
        "\n",
        "\n",
        "def test(args, model, device, dev_samples, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0    \n",
        "    true_y_list = []\n",
        "    pred_y_list = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(dev_samples)):\n",
        "            X, Y = dev_samples[i]\n",
        "\n",
        "            test_items = LibriItems(X, Y, context=args['context'])\n",
        "            test_loader = torch.utils.data.DataLoader(test_items, batch_size=args['batch_size'], shuffle=False)\n",
        "\n",
        "            for data, true_y in test_loader:\n",
        "                data = data.float().to(device)\n",
        "                true_y = true_y.long().to(device)\n",
        "\n",
        "                output = model(data)\n",
        "                pred_y = torch.argmax(output, axis=1)\n",
        "                loss = criterion(output,true_y)\n",
        "                running_loss+=loss.item()\n",
        "\n",
        "                pred_y_list.extend(pred_y.tolist())\n",
        "                true_y_list.extend(true_y.tolist())\n",
        "    \n",
        "    running_loss = running_loss / len(dev_samples)\n",
        "    # print(\"Dev loss is: \",running_loss)\n",
        "    train_accuracy = accuracy_score(true_y_list, pred_y_list)\n",
        "    return train_accuracy, running_loss\n",
        "\n",
        "\n",
        "def create_folder(folder_path):\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.mkdir(folder_path)\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    save_path_dir = args['model_type']+ \"_context_\"+str(args['context']) + \"_lr_\"+str(args['lr'])+\"_\"+args['schedular']+\"_\"+args['activation']\n",
        "    # save_path_dir = args['model_type']+ \"_context_\"+str(args['context']) + \"_lr_\"+str(args['lr'])+\"_\"+args['schedular']\n",
        "\n",
        "    print(\"We are reloading files from\",save_path_dir)\n",
        "\n",
        "    model_path= \"/content/drive/hw1/checkpoints/\"+save_path_dir+\"/\"\n",
        "    args['save_path_dir'] = model_path\n",
        "\n",
        "    results_path = \"/content/drive/hw1/results/{}\".format(save_path_dir)\n",
        "    # create_folder(results_path)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = Network().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    # scheduler1 = StepLR(optimizer, step_size=4, gamma=0.1)\n",
        "    scheduler1 = ReduceLROnPlateau(optimizer, factor=0.5,patience=2, mode='max',threshold=0.01,verbose=True)\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    # Reload the checkpoint\n",
        "    checkpoint = torch.load(\"/content/drive/hw1/checkpoints/{}/model_epoch_{}.txt\".format(save_path_dir,args['resume_epoch']))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler1=checkpoint['scheduler']\n",
        "    epoch = checkpoint['epoch']\n",
        "\n",
        "    # If you want to use full Dataset, please pass None to csvpath\n",
        "    train_samples = LibriSamples(data_path=args['LIBRI_PATH'], shuffle=True, partition=\"train-clean-100\",\n",
        "                                 csvpath= None,submission=args['submission'])\n",
        "    dev_samples = LibriSamples(data_path=args['LIBRI_PATH'], shuffle=True, partition=\"dev-clean\")\n",
        "    \n",
        "    if args['submission']:\n",
        "        train_samples = ConcatDataset([train_samples,dev_samples])\n",
        "    \n",
        "    txt_file = args['save_path_dir']+'resumed_from_epoch'+str(args['resume_epoch'])+\"dev_acc.txt\"\n",
        "    with open(txt_file,'w') as f :\n",
        "\n",
        "        for epoch in range(args['resume_epoch']+1, args['epoch'] + 1):\n",
        "            start = time.time()\n",
        "            # train the model, with parameters changed in the train step\n",
        "            train(args, model, device, train_samples, optimizer, criterion, epoch, scaler )\n",
        "\n",
        "            # validation step, with validation accuracy generated\n",
        "            test_acc, dev_loss = test(args, model, device, dev_samples,criterion)\n",
        "            scheduler1.step(test_acc)\n",
        "\n",
        "            print('Dev accuracy: ', test_acc)\n",
        "            print_content = \"Dev accuracy in epoch {} is {}\\n\".format(epoch,test_acc)\n",
        "            f.write(print_content)\n",
        "\n",
        "            end = time.time()\n",
        "            print(\"\", (end - start)/60,\"minutes\")\n",
        "\n",
        "            # specify the path\n",
        "            path = \"/content/drive/hw1/checkpoints/{}/model_epoch_{}.txt\".format(save_path_dir,epoch)\n",
        "\n",
        "            # save the checkpoint\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(), \n",
        "                'scheduler': scheduler1,\n",
        "            }, path)\n",
        "\n",
        "# /content/drive/hw1/checkpoints/Diamond_8_context_30_lr_0.001_ReduceLROnPlateau_factor0.6_max_LeakyReLU\n",
        "if __name__ == '__main__':\n",
        "    args = {\n",
        "        'batch_size': 2048,\n",
        "        'context': 30,\n",
        "        'log_interval': 200,\n",
        "        'LIBRI_PATH': '/content/hw1p2_student_data',\n",
        "        'lr': 1e-3,\n",
        "        'epoch': 120,\n",
        "        'save_path_dir':'',\n",
        "        'model_type':'Pyramid',\n",
        "        'schedular':'ReduceLROnPlateau_factor0.5_max_patience_2',\n",
        "        'activation':'LeakyReLU',\n",
        "        'submission':False,\n",
        "        'resume_epoch':100,\n",
        "    }\n",
        "    main(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dEXTcQ6qwHW"
      },
      "outputs": [],
      "source": [
        " "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "train.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}