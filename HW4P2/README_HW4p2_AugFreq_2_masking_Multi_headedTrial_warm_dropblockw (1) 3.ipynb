{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# README: How to run this notebook\n",
        "\n",
        "The whole jupyter notebook could be seperated into several parts.\n",
        "\n",
        "The first part is basic library import and model configuration, including wandb initialization, log set up and the finish of creation of dictionary and the function `transform_index_to_letter`.For hyperparameter setting, batch size \n",
        "\n",
        "The next part is to finish the import of dataset and create the dataloader for model training. In this part, I included the cepstral normalization, which could make greate contributation towards the convergence. \n",
        "\n",
        "The following is encoder part, which i implemented the pblstm and lockedDropout with reference. Then, I used the torch summary to validate the function of this model. \n",
        "\n",
        "The next part is attention, where i initially implemented the single head attention (without scaled version), then I switched to multihead, with head equals four suggested by TAs, but in my case, head size set to two achieves best. \n",
        "\n",
        "Then it follows the decoding part, in which teaching force and two fully connected layer with tanh suggested. I tried other activation function, but tanh achieves the best. Also, layer initlization methods were also included. I also tried add more lstm cells/layers (2->3) in decoding part, but the performance don't improve. I also used a torch summary to validate the correct implementation of the whole seq2seq model. In addition, as suggested by my classmates, I could transform the key value of multihead attention in seq2seq model to avoid repeated calculation .\n",
        "\n",
        "For the training part, I used an warm up schedular with cosine annealing to warm up the whole training process, after the 100 epochs, I could achieve 9.2 in LD on test data on kaggle. \n",
        "\n",
        "All my ablation tests could be viewed in wandb\n",
        "https://wandb.ai/saltedfish/11785-HW4?workspace=user-saltedfish\n"
      ],
      "metadata": {
        "id": "oczRyaHbvDmk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2F_5VGse0bv-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03a97aac-850b-4552-99b3-593ad795be00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Apr 22 00:45:29 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    42W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ronCGwCE0y4o",
        "outputId": "5d03c5e1-316d-47cf-daee-55b9cd44404c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: www-browser: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: links2: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: elinks: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: links: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: lynx: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: w3m: not found\n",
            "xdg-open: no method available for opening 'https://accounts.google.com/o/oauth2/auth?client_id=564921029129.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fgd-ocaml-auth.appspot.com%2Foauth2callback&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force&state=Lg89Mhh53JfccGOQcA16Z0qxUzFQ51UeBABhMjLBdg8'\n",
            "/bin/sh: 1: firefox: not found\n",
            "/bin/sh: 1: google-chrome: not found\n",
            "/bin/sh: 1: chromium-browser: not found\n",
            "/bin/sh: 1: open: not found\n",
            "Cannot retrieve auth tokens.\n",
            "Failure(\"Error opening URL:https://accounts.google.com/o/oauth2/auth?client_id=564921029129.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fgd-ocaml-auth.appspot.com%2Foauth2callback&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force&state=Lg89Mhh53JfccGOQcA16Z0qxUzFQ51UeBABhMjLBdg8\")\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libgpm2:amd64.\n",
            "(Reading database ... 155506 files and directories currently installed.)\n",
            "Preparing to unpack .../libgpm2_1.20.7-5_amd64.deb ...\n",
            "Unpacking libgpm2:amd64 (1.20.7-5) ...\n",
            "Selecting previously unselected package w3m.\n",
            "Preparing to unpack .../w3m_0.5.3-36build1_amd64.deb ...\n",
            "Unpacking w3m (0.5.3-36build1) ...\n",
            "Setting up libgpm2:amd64 (1.20.7-5) ...\n",
            "Setting up w3m (0.5.3-36build1) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "/content\n",
            "/content/drive\n",
            "/content\n",
            "/\n",
            "Access token retrieved correctly.\n"
          ]
        }
      ],
      "source": [
        "!sudo add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!sudo apt-get update -qq 2>&1 > /dev/null\n",
        "!sudo apt -y install -qq google-drive-ocamlfuse 2>&1 > /dev/null\n",
        "!google-drive-ocamlfuse\n",
        "\n",
        "!sudo apt-get install -qq w3m # to act as web browser \n",
        "!xdg-settings set default-web-browser w3m.desktop # to set default browser\n",
        "%cd /content\n",
        "!mkdir drive\n",
        "%cd drive\n",
        "!mkdir MyDrive\n",
        "%cd ..\n",
        "%cd ..\n",
        "!google-drive-ocamlfuse /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pueIzbxUwyY",
        "outputId": "c926b388-aed4-4a7a-ea70-95d487ddbe5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kaggle==1.5.8\n",
            "  Downloading kaggle-1.5.8.tar.gz (59 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▌                          | 10 kB 37.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 20 kB 20.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 30 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 40 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 51 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 59 kB 3.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.8-py3-none-any.whl size=73275 sha256=bbab5ec351ebf49c4a7f586db40ea2c2227eab74fa891aa2c0baad8dc350da5c\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/f7/d8/c3902cacb7e62cb611b1ad343d7cc07f42f7eb76ae3a52f3d1\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.8\n",
            "Collecting kaggle\n",
            "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 3.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=a11d3cb97d6ca3dfc8d469e0b3dac00757f6fade5a69354a07ae6d77c224c228\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.8\n",
            "    Uninstalling kaggle-1.5.8:\n",
            "      Successfully uninstalled kaggle-1.5.8\n",
            "Successfully installed kaggle-1.5.12\n",
            "kaggle.json\n",
            "- path is now set to: /content/drive/MyDrive/\n",
            "11-785-s22-hw4p2.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Collecting python-levenshtein\n",
            "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-levenshtein) (57.4.0)\n",
            "Building wheels for collected packages: python-levenshtein\n",
            "  Building wheel for python-levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149864 sha256=fc4350ec4702193f327f10f597f6fc62991c593eb2a0018db3c6ad5d6ec89450\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/5f/ca/7c4367734892581bb5ff896f15027a932c551080b2abd3e00d\n",
            "Successfully built python-levenshtein\n",
            "Installing collected packages: python-levenshtein\n",
            "Successfully installed python-levenshtein-0.12.2\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "TOKEN = {\"username\":\"yuxuanwucmu\",\"key\":\"81ded4babfd327efdc7655be369299b5\"}\n",
        "\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "! mkdir -p .kaggle\n",
        "! mkdir -p /content & mkdir -p /content/.kaggle & mkdir -p /root/.kaggle/\n",
        "\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(TOKEN, file)\n",
        "\n",
        "! pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "! ls \"/content/.kaggle\"\n",
        "! chmod 600 /content/.kaggle/kaggle.json\n",
        "! cp /content/.kaggle/kaggle.json /root/.kaggle/\n",
        "\n",
        "! kaggle config set -n path -v /content/drive/MyDrive/\n",
        "\n",
        "! kaggle competitions download -c 11-785-s22-hw4p2\n",
        "! unzip -q /content/drive/MyDrive/competitions/11-785-s22-hw4p2/11-785-s22-hw4p2.zip\n",
        "\n",
        "!pip install python-levenshtein"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBrQ8OHjW6qC"
      },
      "source": [
        "# Libraries and Initial Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTGPr98x0yjO",
        "outputId": "98a22586-4752-4e06-b15a-3467aec5816a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchsummaryX\n",
            "  Downloading torchsummaryX-1.3.0-py3-none-any.whl (3.6 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.3.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsummaryX) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsummaryX) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torchsummaryX) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchsummaryX) (4.1.1)\n",
            "Installing collected packages: torchsummaryX\n",
            "Successfully installed torchsummaryX-1.3.0\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.12.15-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.10-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 76.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 85.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.5 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=747d2b9a11f010716cf4ed4c506b655bed8d5c374ff02341c9d1cab75e27b2e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.10 setproctitle-1.2.3 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.15\n",
            "Collecting pytorch_warmup\n",
            "  Downloading pytorch-warmup-0.1.0.tar.gz (314 kB)\n",
            "\u001b[K     |████████████████████████████████| 314 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_warmup) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1->pytorch_warmup) (4.1.1)\n",
            "Building wheels for collected packages: pytorch-warmup\n",
            "  Building wheel for pytorch-warmup (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-warmup: filename=pytorch_warmup-0.1.0-py3-none-any.whl size=5802 sha256=be559fa079fd5f121db1123d8994388012915a907e4fbf0d5c8dfc6d5b43a98c\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/5d/24/9475e442daa1e9332c122c79fb5131b9e4e91946009365902e\n",
            "Successfully built pytorch-warmup\n",
            "Installing collected packages: pytorch-warmup\n",
            "Successfully installed pytorch-warmup-0.1.0\n",
            "True 3.7.13 (default, Mar 16 2022, 17:37:17) \n",
            "[GCC 7.5.0]\n",
            "Cuda = True with num_workers = 4\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import Levenshtein as lev\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio.transforms as T\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.utils as utils\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import random\n",
        "import datetime\n",
        "from torch.utils import data\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "import csv\n",
        "! pip install torchsummaryX\n",
        "from torchsummaryX import summary\n",
        "!pip install wandb\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "!pip install pytorch_warmup\n",
        "import pytorch_warmup as warmup\n",
        "def create_folder(folder_path):\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.mkdir(folder_path)\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "print(cuda, sys.version)\n",
        "\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "num_workers = 4 if cuda else 0\n",
        "print(\"Cuda = \"+str(cuda)+\" with num_workers = \"+str(num_workers))\n",
        "np.random.seed(11785)\n",
        "torch.manual_seed(11785)\n",
        "\n",
        "# The labels of the dataset contain letters in LETTER_LIST.\n",
        "# You should use this to convert the letters to the corresponding indices\n",
        "# and train your model with numerical labels.\n",
        "LETTER_LIST = ['<sos>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', \\\n",
        "         'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \"'\", ' ', '<eos>']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Configuration"
      ],
      "metadata": {
        "id": "D-OyrZDA4r-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoints_store_path = \"/content/drive/MyDrive/hw4/store_checkpoints/\"\n",
        "create_folder(checkpoints_store_path)\n",
        "\n",
        "n_epochs = 100\n",
        "batch_size = 128\n",
        "lr = 1e-3\n",
        "normalization = \"Cepstral\"\n",
        "tf = 0.9 \n",
        "tf_sep_epoch = 5\n",
        "warmup_type = \"radam\"\n",
        "init = \"embedding_init\"\n",
        "dropoutVal = 0.2\n",
        "\n",
        "model_id = f\"Freq_Aug_2_Multiheaded_Masking_AdamW_SameKeyValueIncreseTest_{normalization}_tf_{tf}_sep{tf_sep_epoch}_{init}_warmup_{warmup_type}_lockeddropout_{dropoutVal}\"\n",
        "data_path = checkpoints_store_path + model_id\n",
        "create_folder(data_path)\n",
        "\n",
        "\n",
        "wandb.init(project=\"11785-HW4\", entity=\"saltedfish\",name=model_id)\n",
        "wandb.config = {\n",
        "  \"learning_rate\": lr,\n",
        "  \"epochs\": n_epochs,\n",
        "  \"batch_size\": batch_size,\n",
        "  \"tf\":tf,\n",
        "  \"init\": init\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "0lIKviAt4rhf",
        "outputId": "b9819073-40c9-43b3-a310-b4f3ec284706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.15"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/wandb/run-20220422_004747-7gj9ashi</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/saltedfish/11785-HW4/runs/7gj9ashi\" target=\"_blank\">Freq_Aug_2_Multiheaded_Masking_AdamW_SameKeyValueIncreseTest_Cepstral_tf_0.9_sep5_embedding_init_warmup_radam_lockeddropout_0.2</a></strong> to <a href=\"https://wandb.ai/saltedfish/11785-HW4\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xc7E_BuxZ1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "709ecab2-614a-4a1d-d754-f25e090820ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<sos>': 0, 'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, 'I': 9, 'J': 10, 'K': 11, 'L': 12, 'M': 13, 'N': 14, 'O': 15, 'P': 16, 'Q': 17, 'R': 18, 'S': 19, 'T': 20, 'U': 21, 'V': 22, 'W': 23, 'X': 24, 'Y': 25, 'Z': 26, \"'\": 27, ' ': 28, '<eos>': 29}\n",
            "{0: '<sos>', 1: 'A', 2: 'B', 3: 'C', 4: 'D', 5: 'E', 6: 'F', 7: 'G', 8: 'H', 9: 'I', 10: 'J', 11: 'K', 12: 'L', 13: 'M', 14: 'N', 15: 'O', 16: 'P', 17: 'Q', 18: 'R', 19: 'S', 20: 'T', 21: 'U', 22: 'V', 23: 'W', 24: 'X', 25: 'Y', 26: 'Z', 27: \"'\", 28: ' ', 29: '<eos>'}\n"
          ]
        }
      ],
      "source": [
        "def create_dictionaries(letter_list):\n",
        "    '''\n",
        "    Create dictionaries for letter2index and index2letter transformations\n",
        "    based on LETTER_LIST\n",
        "\n",
        "    Args:\n",
        "        letter_list: LETTER_LIST\n",
        "\n",
        "    Return:\n",
        "        letter2index: Dictionary mapping from letters to indices\n",
        "        index2letter: Dictionary mapping from indices to letters\n",
        "    '''\n",
        "    letter2index = dict()\n",
        "    index2letter = dict()\n",
        "    \n",
        "    for i in range(len(LETTER_LIST)):\n",
        "        letter2index[LETTER_LIST[i]] = i\n",
        "        index2letter[i] = LETTER_LIST[i]\n",
        "\n",
        "    return letter2index, index2letter\n",
        "    \n",
        "\n",
        "def transform_index_to_letter(batch_indices):\n",
        "    '''\n",
        "    Transforms numerical index input to string output by converting each index \n",
        "    to its corresponding letter from LETTER_LIST\n",
        "\n",
        "    Args:\n",
        "        batch_indices: List of indices from LETTER_LIST with the shape of (N, )\n",
        "    \n",
        "    Return:\n",
        "        transcripts: List of converted string transcripts. This would be a list with a length of N\n",
        "    '''\n",
        "    transcripts = []\n",
        "    start_index = letter2index['<sos>']\n",
        "    stop_index = letter2index['<eos>']\n",
        "\n",
        "    for tr in batch_indices:\n",
        "      curr = \"\"\n",
        "      for i in tr:\n",
        "        if i == stop_index:\n",
        "          break\n",
        "        elif i == start_index:\n",
        "          pass\n",
        "        else:\n",
        "          curr += index2letter[i]\n",
        "      transcripts.append(curr)\n",
        "\n",
        "    # TODO\n",
        "    return transcripts\n",
        "\n",
        "# Create the letter2index and index2letter dictionary\n",
        "letter2index, index2letter = create_dictionaries(LETTER_LIST)\n",
        "print(letter2index)\n",
        "print(index2letter)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5ioyn6ldQB9"
      },
      "source": [
        "# Dataset and Dataloading\n",
        "\n",
        "You will need to implement the Dataset class by your own. You can implement it similar to HW3P2. However, you are welcomed to do it your own way if it is more comfortable or efficient.\n",
        "\n",
        "Note that you need to use LETTER_LIST to convert the transcript into numerical labels for the model.\n",
        "\n",
        "\n",
        "Example of raw transcript:\n",
        "\n",
        "    ['<sos>', 'N', 'O', 'R', 'T', 'H', 'A', 'N', 'G', 'E', 'R', ' ','A', 'B', 'B', 'E', 'Y', '<eos>']\n",
        "\n",
        "Example of converted transcript ready to process for the model:\n",
        "\n",
        "    [0, 14, 15, 18, 20, 8, 1, 14, 7, 5, 18, 28, 1, 2, 2, 5, 25, 29]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SndiVRVqBMa"
      },
      "outputs": [],
      "source": [
        "class LibriSamples(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data_path, partition= \"train\"):\n",
        "        self.X_dir = data_path + \"/\" + partition + \"/mfcc/\"\n",
        "        self.Y_dir = data_path + \"/\" + partition + \"/transcript/\"\n",
        "\n",
        "        self.X_files = os.listdir(self.X_dir)\n",
        "        self.Y_files = os.listdir(self.Y_dir)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X_files)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        X = np.load(self.X_dir + self.X_files[ind])\n",
        "        X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "        Y = [letter2index[yy] for yy in np.load(self.Y_dir + self.X_files[ind])]  # TODO: Load the corresponding index\n",
        "        # print(Y)\n",
        "        Yy = torch.LongTensor(Y)\n",
        "\n",
        "        return X, Yy\n",
        "    \n",
        "    def collate_fn(batch):\n",
        "        batch_x = [x for (x, y) in batch]\n",
        "        batch_y = [y for (x, y) in batch]\n",
        "\n",
        "        batch_x_pad = pad_sequence([torch.Tensor(x) for x in batch_x],batch_first=True)  # TODO: pad the sequence with pad_sequence (already imported)\n",
        "        lengths_x = torch.LongTensor([len(x) for x in batch_x])  # TODO: Get original lengths of the sequence before padding\n",
        "\n",
        "        batch_y_pad = pad_sequence(batch_y,batch_first=True,padding_value=0)  # TODO: pad the sequence with pad_sequence (already imported)\n",
        "        lengths_y = torch.LongTensor([len(y) for y in batch_y])   # TODO: Get original lengths of the sequence before padding\n",
        "\n",
        "        return batch_x_pad, batch_y_pad, torch.tensor(lengths_x), torch.tensor(lengths_y)\n",
        "\n",
        "\n",
        "class LibriSamplesTest(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data_path, test_order):  # test_order is the csv similar to what you used in hw1\n",
        "        file_path = data_path + '/test/' + test_order\n",
        "        self.X_dir = data_path + \"/test/mfcc/\"\n",
        "\n",
        "        test_order_list = self.parse_csv(file_path)  # TODO: open test_order.csv as a list\n",
        "        self.X = [i for i in test_order_list]  # TODO: Load the npy files from test_order.csv and append into a list\n",
        "        # You can load the files here or save the paths here and load inside __getitem__ like the previous class\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        # TODOs: Need to return only X because this is the test dataset\n",
        "        # print(self.X[ind])\n",
        "        # print(self.X_dir+self.X[ind][0])\n",
        "        X = np.load(self.X_dir+self.X[ind][0])\n",
        "        X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "        return X\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        batch_x_pad = pad_sequence(\n",
        "            [torch.Tensor(x) for x in batch],batch_first=True)  # TODO: pad the sequence with pad_sequence (already imported)\n",
        "        lengths_x = torch.LongTensor(\n",
        "            [len(x) for x in batch])  # TODO: Get original lengths of the sequence before padding\n",
        "\n",
        "        return batch_x_pad, torch.tensor(lengths_x)\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_csv(filepath):\n",
        "        subset = []\n",
        "        with open(filepath) as f:\n",
        "            f_csv = csv.reader(f)\n",
        "            for row in f_csv:\n",
        "                subset.append(row)\n",
        "        return subset[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mzoYfTKu14s",
        "outputId": "9f846d8b-2321-4a16-a12e-bd6325bb1954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size:  128\n",
            "Train dataset samples = 28539, batches = 223\n",
            "Val dataset samples = 2703, batches = 22\n",
            "Test dataset samples = 2620, batches = 21\n"
          ]
        }
      ],
      "source": [
        "root = '/hw4p2_student_data/hw4p2_student_data'\n",
        "\n",
        "train_data = LibriSamples(root, 'train')\n",
        "val_data = LibriSamples(root, 'dev')\n",
        "test_data = LibriSamplesTest(root, 'test_order.csv')\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True,\n",
        "                                           collate_fn=LibriSamples.collate_fn)  # TODO: Define the train loader. Remember to pass in a parameter (function) for the collate_fn argument\n",
        "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True,\n",
        "                                         collate_fn=LibriSamples.collate_fn)  # TODO: Define the val loader. Remember to pass in a parameter (function) for the collate_fn argument\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False,\n",
        "                                          collate_fn=LibriSamplesTest.collate_fn)  # TODO: Define the test loader. Remember to pass in a parameter (function) for the collate_fn argument\n",
        "\n",
        "print(\"Batch size: \", batch_size)\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9FwVZ9I2da0",
        "outputId": "5f314619-1575-48b3-f2c6-8a6280137f7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 1678, 13]) torch.Size([128, 330]) torch.Size([128]) 128\n",
            "tensor([ 0,  9, 28, 23,  1, 19, 28,  2,  5,  7,  9, 14, 14,  9, 14,  7, 28, 20,\n",
            "        15, 28, 20,  1, 11,  5, 28,  1, 14, 28,  9, 14, 20,  5, 18,  5, 19, 20,\n",
            "        28,  9, 14, 28, 10,  9, 13, 28, 19, 15, 28,  9, 28,  2, 18, 15, 21,  7,\n",
            "         8, 20, 28,  8,  9, 13, 28, 21, 16, 28,  9, 14, 20, 15, 28, 20,  8,  5,\n",
            "        28, 15,  6,  6,  9,  3,  5, 28,  1, 14,  4, 28, 19,  5, 20, 28,  8,  9,\n",
            "        13, 28, 20, 15, 28,  3, 15, 16, 25,  9, 14,  7, 28,  3,  9, 18,  3, 21,\n",
            "        12,  1, 18, 28, 12,  5, 20, 20,  5, 18, 19, 28, 23,  5, 28, 21, 19,  5,\n",
            "         4, 28, 20, 15, 28, 19,  5, 14,  4, 28, 15, 21, 20, 28,  1, 28, 18,  1,\n",
            "         6, 20, 28, 15,  6, 28, 20,  8,  5, 13, 28, 20, 15, 28, 20,  8,  5, 28,\n",
            "        20, 18,  1,  4,  5, 28, 20,  8,  1, 20, 28, 23,  1, 19, 28, 10, 21, 19,\n",
            "        20, 28,  2,  5,  6, 15, 18,  5, 28, 20,  8,  5, 28,  7,  5, 14,  5, 18,\n",
            "         1, 12, 28,  1,  4, 15, 16, 20,  9, 15, 14, 28, 15,  6, 28, 20, 25, 16,\n",
            "         5, 23, 18,  9, 20,  5, 18, 19, 28, 23,  8,  5, 14, 28, 20,  8,  5, 25,\n",
            "        28, 23,  5, 18,  5, 28, 19, 20,  9, 12, 12, 28,  9, 14, 28, 20,  8,  5,\n",
            "        28,  5, 24, 16,  5, 18,  9, 13,  5, 14, 20,  1, 12, 28, 19, 20,  1,  7,\n",
            "         5, 29,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0])\n",
            "torch.Size([128, 3003, 13]) torch.Size([128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        }
      ],
      "source": [
        "# test code for checking shapes\n",
        "for data in train_loader:\n",
        "    x, y, lx, ly = data\n",
        "    print(x.shape, y.shape, lx.shape, len(ly))\n",
        "    print(y[0]) # desired \n",
        "    # print(x[0].shape)\n",
        "    # print(x[0,:,:].shape)\n",
        "    break\n",
        "\n",
        "# Test code for checking shapes and return arguments of the train and val loaders\n",
        "for data in test_loader:\n",
        "    x, lx = data # if you face an error saying \"Cannot unpack\", then you are not passing the collate_fn argument\n",
        "    # print(\"===================\")\n",
        "    # print(data)\n",
        "    # print(\"===================\")\n",
        "    print(x.shape, lx.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test code for checking shapes\n",
        "for data in test_loader:\n",
        "    x, lx = data\n",
        "    print(x.shape, lx.shape)\n",
        "    print(x[0].shape) # desired \n",
        "    # print(x[0,:,:])\n",
        "    # print(x[0,:,:].shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQ9ag8hRR9uD",
        "outputId": "5f847f96-7407-4121-d043-8e65771540d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 3003, 13]) torch.Size([128])\n",
            "torch.Size([3003, 13])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I--VjKlEhwi8"
      },
      "source": [
        "# Model (Encoder)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lockdropout\n",
        "# Code from TA and two externel links\n",
        "# https://github.com/Lobbii/CMU-11785-Homework/blob/main/Attention-based%20End-to-End%20Speech-to-Text%20Deep%20Neural%20Network/HW4P2_template_Final.ipynb\n",
        "# https://pytorchnlp.readthedocs.io/en/latest/_modules/torchnlp/nn/lock_dropout.html\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class LockedDropout(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x, dropout=0.2):\n",
        "    \"\"\"\n",
        "    :param x: (B,T,C)\n",
        "    :param p: probability of lockedDropout val \n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    if dropout == 0 or not self.training:\n",
        "      return x\n",
        "    mask = x.data.new(x.size(0), 1, x.size(2))\n",
        "    mask = mask.bernoulli_(1 - dropout)\n",
        "    mask = Variable(mask, requires_grad=False) / (1 - dropout)\n",
        "    mask = mask.expand_as(x)\n",
        "    return mask * xty"
      ],
      "metadata": {
        "id": "2uDIJ-MFJnmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfpIMUDzCvT3"
      },
      "outputs": [],
      "source": [
        "\n",
        "class pBLSTM(nn.Module):\n",
        "    '''\n",
        "    Pyramidal BiLSTM\n",
        "    Read paper and understand the concepts and then write your implementation here.\n",
        "\n",
        "    At each step,\n",
        "    1. Pad your input if it is packed\n",
        "    2. Truncate the input length dimension by concatenating feature dimension\n",
        "        (i) How should  you deal with odd/even length input? \n",
        "        (ii) How should you deal with input length array (x_lens) after truncating the input?\n",
        "    3. Pack your input\n",
        "    4. Pass it into LSTM layer\n",
        "\n",
        "    To make our implementation modular, we pass 1 layer at a time.\n",
        "    '''\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(pBLSTM, self).__init__()\n",
        "        self.blstm = nn.LSTM(input_size=input_dim*2,hidden_size=hidden_dim, num_layers=1,bidirectional=True,batch_first=True)\n",
        "        self.dropout = LockedDropout()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #  1. Pad your input if it is packed # (B,T,C)\n",
        "        #  x is a pack_padded_sequence unpack the LSTM output using pad_packed_sequence\n",
        "        # print(x)\n",
        "        x, x_lens = pad_packed_sequence(x,batch_first=True)\n",
        "\n",
        "        B,T,C = x.shape\n",
        "        # 2. Truncate the input length dimension by concatenating feature dimension\n",
        "        #     (i) How should  you deal with odd/even length input? \n",
        "        if T%2==1: \n",
        "            x = x[:,:-1:] # remove the last one\n",
        "\n",
        "        #     (ii) How should you deal with input length array (x_lens) after truncating the input?\n",
        "        x_lens = x_lens // 2\n",
        "\n",
        "        x = x.contiguous().reshape(B,T//2, C*2)\n",
        "\n",
        "        x = self.dropout(x) # lock dropout layer\n",
        "\n",
        "        # 3. Pack your input\n",
        "        x = pack_padded_sequence(x,x_lens,batch_first=True,enforce_sorted=False)\n",
        "        out = self.blstm(x)[0]\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEmFI9It54mk"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    Encoder takes the utterances as inputs and returns the key, value and unpacked_x_len.\n",
        "\n",
        "    '''\n",
        "    def __init__(self, input_dim, encoder_hidden_dim, key_value_size=128):\n",
        "        super(Encoder, self).__init__()\n",
        "        # The first LSTM layer at the bottom\n",
        "        self.lstm = nn.LSTM(input_size=input_dim,hidden_size=encoder_hidden_dim,batch_first=True,bidirectional=True)\n",
        "\n",
        "        # Define the blocks of pBLSTMs\n",
        "        # Dimensions should be chosen carefully\n",
        "        # Hint: Bidirectionality, truncation...\n",
        "\n",
        "        # stack 3 pBLSTMs on top of the bottom BLSTM layer to reduce the time resolution 2^3 = 8 times\n",
        "        self.pBLSTMs = nn.Sequential(\n",
        "            pBLSTM(encoder_hidden_dim*2,encoder_hidden_dim),\n",
        "            pBLSTM(encoder_hidden_dim*2,encoder_hidden_dim),\n",
        "            pBLSTM(encoder_hidden_dim*2,encoder_hidden_dim),\n",
        "            # Optional: dropout\n",
        "        )\n",
        "         \n",
        "        # The linear transformations for producing Key and Value for attention\n",
        "        # Hint: Dimensions when bidirectional lstm? \n",
        "        self.key_network = nn.Linear(in_features=encoder_hidden_dim*2,out_features=key_value_size)\n",
        "        self.value_network = nn.Linear(in_features=encoder_hidden_dim*2,out_features=key_value_size)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, x_len):\n",
        "        \"\"\"\n",
        "        input x.shape(B,T,C) x[0] = (128,3204,13)\n",
        "        1. Pack your input and pass it through the first LSTM layer (no truncation)\n",
        "        2. Pass it through the pyramidal LSTM layer\n",
        "        3. Pad your input back to (B, T, *) or (T, B, *) shape\n",
        "        4. Output Key, Value, and truncated input lens\n",
        "\n",
        "        Key and value could be\n",
        "            (i) Concatenated hidden vectors from all time steps (key == value).\n",
        "            (ii) Linear projections of the output from the last pBLSTM network.\n",
        "                If you choose this way, you can use the final output of\n",
        "                your pBLSTM network.\n",
        "        \"\"\"\n",
        "        # 1. Pack your input and pass it through the first LSTM layer (no truncation)\n",
        "        # print(\"x.shape\",x.shape)\n",
        "        # print(\"x_lens.shape\",x_len.shape)\n",
        "        # print(\"x_lens\",x_len)\n",
        "        packed_input = pack_padded_sequence(x,x_len.cpu(), enforce_sorted=False,batch_first =True) #(B,T,C) \n",
        "\n",
        "        x = self.lstm(packed_input)[0] \n",
        "        # 2. Pass it through the pyramidal LSTM layer\n",
        "        x = self.pBLSTMs(x)\n",
        "        out, lens =  pad_packed_sequence(x,batch_first=True) \n",
        "        k = self.key_network(out)\n",
        "        v = self.value_network(out)\n",
        "        # print(out.shape)\n",
        "        # print(k.shape)\n",
        "        # print(v.shape)\n",
        "        # print(lens)\n",
        "\n",
        "        return k, v, lens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7TEBpEF6VFh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0e10c02-1bcc-4579-9242-ea8e715600a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "/usr/local/lib/python3.7/dist-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  df_sum = df.sum()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====================================================================================\n",
            "                                  Kernel Shape      Output Shape    Params  \\\n",
            "Layer                                                                        \n",
            "0_lstm                                       -      [72304, 256]  146.432k   \n",
            "1_pBLSTMs.0.LockedDropout_dropout            -  [128, 1501, 512]         -   \n",
            "2_pBLSTMs.0.LSTM_blstm                       -      [36124, 256]  657.408k   \n",
            "3_pBLSTMs.1.LockedDropout_dropout            -   [128, 750, 512]         -   \n",
            "4_pBLSTMs.1.LSTM_blstm                       -      [18034, 256]  657.408k   \n",
            "5_pBLSTMs.2.LockedDropout_dropout            -   [128, 375, 512]         -   \n",
            "6_pBLSTMs.2.LSTM_blstm                       -       [8987, 256]  657.408k   \n",
            "7_key_network                       [256, 128]   [128, 375, 128]   32.896k   \n",
            "8_value_network                     [256, 128]   [128, 375, 128]   32.896k   \n",
            "\n",
            "                                  Mult-Adds  \n",
            "Layer                                        \n",
            "0_lstm                             144.384k  \n",
            "1_pBLSTMs.0.LockedDropout_dropout         -  \n",
            "2_pBLSTMs.0.LSTM_blstm              655.36k  \n",
            "3_pBLSTMs.1.LockedDropout_dropout         -  \n",
            "4_pBLSTMs.1.LSTM_blstm              655.36k  \n",
            "5_pBLSTMs.2.LockedDropout_dropout         -  \n",
            "6_pBLSTMs.2.LSTM_blstm              655.36k  \n",
            "7_key_network                       32.768k  \n",
            "8_value_network                     32.768k  \n",
            "-------------------------------------------------------------------------------------\n",
            "                         Totals\n",
            "Total params          2.184448M\n",
            "Trainable params      2.184448M\n",
            "Non-trainable params        0.0\n",
            "Mult-Adds                2.176M\n",
            "=====================================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                  Kernel Shape      Output Shape    Params  \\\n",
              "Layer                                                                        \n",
              "0_lstm                                       -      [72304, 256]  146432.0   \n",
              "1_pBLSTMs.0.LockedDropout_dropout            -  [128, 1501, 512]       NaN   \n",
              "2_pBLSTMs.0.LSTM_blstm                       -      [36124, 256]  657408.0   \n",
              "3_pBLSTMs.1.LockedDropout_dropout            -   [128, 750, 512]       NaN   \n",
              "4_pBLSTMs.1.LSTM_blstm                       -      [18034, 256]  657408.0   \n",
              "5_pBLSTMs.2.LockedDropout_dropout            -   [128, 375, 512]       NaN   \n",
              "6_pBLSTMs.2.LSTM_blstm                       -       [8987, 256]  657408.0   \n",
              "7_key_network                       [256, 128]   [128, 375, 128]   32896.0   \n",
              "8_value_network                     [256, 128]   [128, 375, 128]   32896.0   \n",
              "\n",
              "                                   Mult-Adds  \n",
              "Layer                                         \n",
              "0_lstm                              144384.0  \n",
              "1_pBLSTMs.0.LockedDropout_dropout        NaN  \n",
              "2_pBLSTMs.0.LSTM_blstm              655360.0  \n",
              "3_pBLSTMs.1.LockedDropout_dropout        NaN  \n",
              "4_pBLSTMs.1.LSTM_blstm              655360.0  \n",
              "5_pBLSTMs.2.LockedDropout_dropout        NaN  \n",
              "6_pBLSTMs.2.LSTM_blstm              655360.0  \n",
              "7_key_network                        32768.0  \n",
              "8_value_network                      32768.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d202deb5-2626-4e2b-810f-73c4690ae67d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_lstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[72304, 256]</td>\n",
              "      <td>146432.0</td>\n",
              "      <td>144384.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_pBLSTMs.0.LockedDropout_dropout</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 1501, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_pBLSTMs.0.LSTM_blstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[36124, 256]</td>\n",
              "      <td>657408.0</td>\n",
              "      <td>655360.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_pBLSTMs.1.LockedDropout_dropout</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 750, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_pBLSTMs.1.LSTM_blstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[18034, 256]</td>\n",
              "      <td>657408.0</td>\n",
              "      <td>655360.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5_pBLSTMs.2.LockedDropout_dropout</th>\n",
              "      <td>-</td>\n",
              "      <td>[128, 375, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6_pBLSTMs.2.LSTM_blstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[8987, 256]</td>\n",
              "      <td>657408.0</td>\n",
              "      <td>655360.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7_key_network</th>\n",
              "      <td>[256, 128]</td>\n",
              "      <td>[128, 375, 128]</td>\n",
              "      <td>32896.0</td>\n",
              "      <td>32768.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8_value_network</th>\n",
              "      <td>[256, 128]</td>\n",
              "      <td>[128, 375, 128]</td>\n",
              "      <td>32896.0</td>\n",
              "      <td>32768.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d202deb5-2626-4e2b-810f-73c4690ae67d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d202deb5-2626-4e2b-810f-73c4690ae67d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d202deb5-2626-4e2b-810f-73c4690ae67d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "encoder = Encoder(13,128).to(device)\n",
        "summary(encoder, x.to(device), lx) # x and lx are from the previous cell\n",
        "# print(encoder)\n",
        "# print(x.shape)\n",
        "# print(lx.shape)\n",
        "# Try out your encoder on a tiny input before moving to the next step..."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention "
      ],
      "metadata": {
        "id": "ojp-BNY4Bz_z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqu-MUM8TjUO"
      },
      "outputs": [],
      "source": [
        "def plot_attention(attention):\n",
        "    # utility function for debugging\n",
        "    plt.clf()\n",
        "    sns.heatmap(attention, cmap='GnBu')\n",
        "    plt.show()\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    '''\n",
        "    Attention is calculated using key and value from encoder and query from decoder.\n",
        "    Here are different ways to compute attention and context:\n",
        "    1. Dot-product attention\n",
        "        energy = bmm(key, query) \n",
        "        # Optional: Scaled dot-product by normalizing with sqrt key dimension\n",
        "        # Check \"attention is all you need\" Section 3.2.1\n",
        "    * 1st way is what most TAs are comfortable with, but if you want to explore...\n",
        "    2. Cosine attention\n",
        "        energy = cosine(query, key) # almost the same as dot-product xD \n",
        "    3. Bi-linear attention\n",
        "        W = Linear transformation (learnable parameter): d_k -> d_q\n",
        "        energy = bmm(key @ W, query)\n",
        "    4. Multi-layer perceptron\n",
        "        # Check \"Neural Machine Translation and Sequence-to-sequence Models: A Tutorial\" Section 8.4\n",
        "    \n",
        "    After obtaining unnormalized attention weights (energy), compute and return attention and context, i.e.,\n",
        "    energy = mask(energy) # mask out padded elements with big negative number (e.g. -1e9)\n",
        "    attention = softmax(energy)\n",
        "    context = bmm(attention, value)\n",
        "\n",
        "    5. Multi-Head Attention\n",
        "        # Check \"attention is all you need\" Section 3.2.2\n",
        "        h = Number of heads\n",
        "        W_Q, W_K, W_V: Weight matrix for Q, K, V (h of them in total)\n",
        "        W_O: d_v -> d_v\n",
        "\n",
        "        Reshape K: (B, T, d_k)\n",
        "        to (B, T, h, d_k // h) and transpose to (B, h, T, d_k // h)\n",
        "        Reshape V: (B, T, d_v)\n",
        "        to (B, T, h, d_v // h) and transpose to (B, h, T, d_v // h)\n",
        "        Reshape Q: (B, d_q)\n",
        "        to (B, h, d_q // h)\n",
        "\n",
        "        energy = Q @ K^T\n",
        "        energy = mask(energy)\n",
        "        attention = softmax(energy)\n",
        "        multi_head = attention @ V\n",
        "        multi_head = multi_head reshaped to (B, d_v)\n",
        "        context = multi_head @ W_O\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "        # Optional: dropout\n",
        "\n",
        "    def forward(self, query, key, value, mask):\n",
        "        \"\"\"\n",
        "        input:\n",
        "            key: (batch_size, seq_len, d_k)\n",
        "            value: (batch_size, seq_len, d_v)\n",
        "            query: (batch_size, d_q)\n",
        "        * Hint: d_k == d_v == d_q is often true if you use linear projections\n",
        "        return:\n",
        "            context: (batch_size, key_val_dim)\n",
        "        \n",
        "        \"\"\"\n",
        "        key,mask = key.cuda(),mask.cuda()\n",
        "        # (B,T,K_dim) mat-mat product (B, Q_dim,1)\n",
        "        energy = torch.bmm(key,query.unsqueeze(2)).squeeze(2).cuda()\n",
        "\n",
        "        energy = energy.masked_fill(mask,float(\"-inf\"))\n",
        "\n",
        "        attention = F.softmax(energy,dim=1)\n",
        "        context = torch.bmm(attention.unsqueeze(1),value).squeeze(1)\n",
        "\n",
        "        return context, attention\n",
        "        # we return attention weights for plotting (for debugging)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    '''\n",
        "    5. Multi-Head Attention\n",
        "        # Check \"attention is all you need\" Section 3.2.2\n",
        "        h = Number of heads\n",
        "        W_Q, W_K, W_V: Weight matrix for Q, K, V (h of them in total)\n",
        "        W_O: d_v -> d_v\n",
        "\n",
        "        Reshape K: (B, T, d_k)\n",
        "        to (B, T, h, d_k // h) and transpose to (B, h, T, d_k // h)\n",
        "        Reshape V: (B, T, d_v)\n",
        "        to (B, T, h, d_v // h) and transpose to (B, h, T, d_v // h)\n",
        "        Reshape Q: (B, d_q)\n",
        "        to (B, h, d_q // h)\n",
        "\n",
        "        energy = Q @ K^T\n",
        "        energy = mask(energy)\n",
        "        attention = softmax(energy)\n",
        "        multi_head = attention @ V\n",
        "        multi_head = multi_head reshaped to (B, d_v)\n",
        "        context = multi_head @ W_O\n",
        "    '''\n",
        "    def __init__(self,embed_dim, d_qkv, head):\n",
        "        \"Take in model size (embedding dim) and number of heads\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.head_dim = d_qkv // head\n",
        "        self.head = head\n",
        "        self.W_Q = nn.Linear(d_qkv, head * self.head_dim)\n",
        "        self.W_K = nn.Linear(d_qkv, head * self.head_dim)\n",
        "        self.W_V = nn.Linear(d_qkv, head * self.head_dim)\n",
        "        self.W_O = nn.Linear(d_qkv, d_qkv)\n",
        "\n",
        "    def linear_transform(self,key,value):\n",
        "        \"\"\"\n",
        "        This is to used in muti-head attention to avoid repeated calculation\n",
        "        \"\"\"\n",
        "        key = self.W_K(key)\n",
        "        value = self.W_V(value)\n",
        "        return key,value\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, query, key, value, mask):\n",
        "        \"\"\"\n",
        "        input:\n",
        "            key: (batch_size, seq_len, d_k)\n",
        "            value: (batch_size, seq_len, d_v)\n",
        "            query: (batch_size, d_q)\n",
        "        * Hint: d_k == d_v == d_q is often true if you use linear projections\n",
        "        return:\n",
        "            context: (batch_size, key_val_dim)\n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        h = self.head\n",
        "        B, T, d_k = key.shape\n",
        "        key = key.reshape(B, T, h, d_k // h).permute(0,2,1,3)\n",
        "        \n",
        "        B, T, d_v = value.shape\n",
        "        value = value.reshape(B, T, h, d_v // h).permute(0,2,1,3)\n",
        "        \n",
        "        B, d_q = query.shape        \n",
        "        query = self.W_Q(query)\n",
        "        query = query.reshape(B, h, d_q // h)\n",
        "\n",
        "        assert d_k == d_v == d_q\n",
        "\n",
        "        mask = mask.cuda()\n",
        "\n",
        "        # energy = Q @ K^T\n",
        "        # (B, h, d_q // h) mat-mat product (B, h, T, d_k // h)\n",
        "        \"\"\"\n",
        "        torch.Size([64, 2, 1, 128])\n",
        "        torch.Size([64, 2, 128, 206])\n",
        "        \"\"\"\n",
        "        energy = query.unsqueeze(2)@key.permute(0,1,3,2)\n",
        "        energy = energy.squeeze(2).cuda()\n",
        "        energy = energy.masked_fill(mask.unsqueeze(1),float(\"-inf\"))\n",
        "        attention = F.softmax(energy,dim=-1)\n",
        "\n",
        "        '''\n",
        "        multi_head = attention @ V\n",
        "        multi_head = multi_head reshaped to (B, d_v)\n",
        "        context = multi_head @ W_O\n",
        "\n",
        "        attention: torch.Size([64, 2, 203])\n",
        "        value: torch.Size([64, 2, 203, 128])\n",
        "        '''\n",
        "\n",
        "        multi_head = attention.unsqueeze(2) @ value\n",
        "        multi_head = multi_head.squeeze(2).reshape(B,-1)\n",
        "        context = self.W_O(multi_head)\n",
        "\n",
        "        return context, attention\n",
        "        # we return attention weights for plotting (for debugging)\n"
      ],
      "metadata": {
        "id": "TffAyu9nmJZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoding "
      ],
      "metadata": {
        "id": "1lJZDgXUOUQ3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcTC4cK95TYT"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    '''\n",
        "    As mentioned in a previous recitation, each forward call of decoder deals with just one time step.\n",
        "    Thus we use LSTMCell instead of LSTM here.\n",
        "    The output from the last LSTMCell can be used as a query for calculating attention.\n",
        "    Methods like Gumble noise and teacher forcing can also be incorporated for improving the performance.\n",
        "    '''\n",
        "    def __init__(self, vocab_size, decoder_hidden_dim, embed_dim, key_value_size=128):\n",
        "        super(Decoder, self).__init__()\n",
        "        # Hint: Be careful with the padding_idx\n",
        "        self.embedding = nn.Embedding(vocab_size,embed_dim,padding_idx=letter2index['<eos>'])\n",
        "\n",
        "        # The number of cells is defined based on the paper\n",
        "        self.lstm1 = nn.LSTMCell(embed_dim+key_value_size,decoder_hidden_dim)\n",
        "        self.lstm2 = nn.LSTMCell(decoder_hidden_dim, key_value_size)\n",
        "        \n",
        "        self.fc1 = nn.Linear(2 * key_value_size, 4 * key_value_size)\n",
        "        self.tanh1 = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(4 * key_value_size, embed_dim)\n",
        "        self.tanh2 = nn.Tanh()\n",
        "\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "        # self.attention = Attention()     \n",
        "        self.multihead_attention =  MultiHeadAttention(embed_dim,key_value_size,2)   \n",
        "        # self.multihead_attention = nn.MultiheadAttention(embed_dim,2)     \n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        # Optional: Weight-tying\n",
        "        self.character_prob = nn.Linear(2 * key_value_size, vocab_size) #: d_v -> vocab_size\n",
        "        self.key_value_size = key_value_size\n",
        "        \n",
        "        # Weight tying\n",
        "        self.character_prob.weight = self.embedding.weight\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    # This initilization methods was referenced from of the github repository, but I could not find the link when\n",
        "    # I plan to reference it. The first embedding and linear initlization was same as the paper in HW4 suggested,\n",
        "    # but the init methods in LSTM is new to me. I evaluated the performance between my version with this version,\n",
        "    # the remaining version could achieve higher performance.\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Embedding):\n",
        "                nn.init.uniform_(m.weight, -1, 1)\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0.01)\t\n",
        "            if isinstance(m, nn.LSTM):\n",
        "                for param in m.parameters():\n",
        "                    if len(param.shape) >= 2:\n",
        "                        nn.init.orthogonal_(param.data)\n",
        "                    else:\n",
        "                        nn.init.normal_(param.data)\n",
        "    \n",
        "    # def _init_weights(self,hidden_dim):\n",
        "    #     for m in self.modules():\n",
        "    #       if isinstance(m,nn.Embedding):\n",
        "    #           nn.init.uniform_(m.weight, -0.1, 0.1)\n",
        "    #       if isinstance(m, nn.Linear):\n",
        "    #           nn.init.xavier_normal_(m.weight)\n",
        "    #           nn.init.constant_(m.bias, 0.01)\t\n",
        "    #       if isinstance(m, nn.LSTM):\n",
        "    #           nn.init.uniform_(m.weight,-1/np.sqrt(hidden_dim),1/np.sqrt(hidden_dim))\n",
        "\n",
        "\n",
        "    def forward(self, key, value, encoder_len, y=None, mode='train',Teacher_forcing_rate=0.9):\n",
        "        '''\n",
        "        Args:\n",
        "            key :(B, T, d_k) - Output of the Encoder (possibly from the Key projection layer)\n",
        "            value: (B, T, d_v) - Output of the Encoder (possibly from the Value projection layer) [128, 205, 128]\n",
        "            y: (B, text_len) - Batch input of text with text_length\n",
        "            mode: Train or eval mode for teacher forcing\n",
        "        Return:\n",
        "            predictions: the character perdiction probability \n",
        "        '''\n",
        "\n",
        "        B, T, d_k = key.shape\n",
        "\n",
        "        if mode == 'train':\n",
        "            max_len =  y.shape[1]\n",
        "            char_embeddings = self.embedding(y.long())\n",
        "            # print(\"char_embedding shape\",char_embeddings.size()) #(128,307,56)\n",
        "        else:\n",
        "            max_len = 600\n",
        "\n",
        "        # TODO: Create the attention mask here (outside the for loop rather than inside) to aviod repetition \n",
        "        # generate mask\n",
        "        mask = encoder_len.unsqueeze(1)< torch.arange(T).unsqueeze(0)# encoder_len (L,1) < sequence length (1,N) \n",
        "        mask = mask.cuda() \n",
        "        # print(\"mask size\",mask.size()) # ([128, 205])\n",
        "        \n",
        "        predictions = []\n",
        "        # This is the first input to the decoder\n",
        "        # What should the fill_value be? => <SOS> 0 \n",
        "        prediction = torch.full((B,1), fill_value=0, device=device) # (batch,1)\n",
        "        # print(\"prediction's shape\",prediction.shape)\n",
        "\n",
        "        # The length of hidden_states vector should depend on the number of LSTM Cells defined in init\n",
        "        # The paper uses 2\n",
        "        hidden_states = [None, None] \n",
        "        \n",
        "        # TODO: Initialize the context\n",
        "        context =  value[:,0,:].squeeze(1) # (B, T, d_v)\n",
        "        # print(\"value shape\",value.size()) #[128, 205, 128]\n",
        "        # print(\"context's shape\",context.size())#([128, 128])\n",
        "\n",
        "        attention_plot = [] # this is for debugging\n",
        "\n",
        "\n",
        "        for i in range(max_len):\n",
        "            if mode == 'train':\n",
        "                # TODO: Implement Teacher Forcing \n",
        "                \"\"\"\n",
        "                if using teacher_forcing:\n",
        "                    if i == 0:\n",
        "                        # This is the first time step\n",
        "                        # Hint: How did you initialize \"prediction\" variable above?\n",
        "                    else:\n",
        "                        # Otherwise, feed the label of the **previous** time step\n",
        "                else:\n",
        "                    char_embed = embedding of the previous prediction\n",
        "                \"\"\"   \n",
        "\n",
        "                if np.random.random_sample() < Teacher_forcing_rate and i > 0:\n",
        "                  # just direct copy it from the previous embeddings\n",
        "                    char_embed = char_embeddings[:,i-1]\n",
        "                else:\n",
        "                  # This is the first time step, it is just filled with <SOS>\n",
        "                    char_embed = self.embedding(prediction.argmax(dim=-1))\n",
        "            else:\n",
        "                # mode = evaluation, no teaching force required\n",
        "                # embedding of the previous predicitons\n",
        "                char_embed = self.embedding(prediction.argmax(dim=-1))\n",
        "\n",
        "            # print(\"char embedding size\",char_embed.size()) # (128,256)\n",
        "            # print(\"context size\",context.size()) #[128, 128])\n",
        "\n",
        "            # what vectors should be concatenated as a context?\n",
        "            y_context = torch.cat([char_embed, context], dim=1)\n",
        "            # context and hidden states of lstm 1 from the previous time step should be fed\n",
        "            hidden_states[0] = self.lstm1(y_context, hidden_states[0])\n",
        "            # hidden states of lstm1 and hidden states of lstm2 from the previous time step should be fed\n",
        "            hidden_states[1] = self.lstm2(hidden_states[0][0], hidden_states[1])\n",
        "           \n",
        "            # What then is the query?\n",
        "            query = hidden_states[1][0]\n",
        "            \n",
        "            # Compute attention from the output of the second LSTM Cell\n",
        "            # context, attention = self.attention(query, key, value, mask)\n",
        "            \n",
        "            # Implement the multi-head attention:\n",
        "            \"\"\"\n",
        "            print(query.shape) torch.Size([64, 256]) # (B,L,Eq)\n",
        "            print(key.shape)   torch.Size([64, 204, 256]) # (B,N,Ek)\n",
        "            print(value.shape) torch.Size([64, 204, 256]) # (B,L,Ev)\n",
        "            print(mask.shape)  torch.Size([64, 204]) # \n",
        "            context, attention = self.multihead_attention(query.unsqueeze(1), key, value, attn_mask = mask)\n",
        "            \"\"\"\n",
        "            context, attention = self.multihead_attention(query, key, value, mask)\n",
        "\n",
        "            # We store the first attention of this batch for debugging\n",
        "            attention_plot.append(attention[0].detach().cpu())\n",
        "            \n",
        "            # What should be concatenated as the output context?\n",
        "            output_context = torch.cat([query,context], dim=1)\n",
        "            \n",
        "            output_context = self.fc1(output_context)\n",
        "            output_context = self.tanh1(output_context)\n",
        "            # output_context = self.dropout(output_context)\n",
        "            output_context = self.fc2(output_context)\n",
        "            output_context = self.tanh2(output_context)\n",
        "            # output_context = self.dropout(output_context)\n",
        "            \n",
        "            prediction = self.character_prob(output_context)\n",
        "            # store predictions\n",
        "            predictions.append(prediction.unsqueeze(1))\n",
        "        \n",
        "        # Concatenate the attention and predictions to return\n",
        "        attentions = torch.stack(attention_plot, dim=0)\n",
        "        predictions = torch.cat(predictions, dim=1)\n",
        "        return predictions, attentions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d35FEZhz5Uhx"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    '''\n",
        "    We train an end-to-end sequence to sequence model comprising of Encoder and Decoder.\n",
        "    This is simply a wrapper \"model\" for your encoder and decoder.\n",
        "    '''\n",
        "    def __init__(self, input_dim, vocab_size, encoder_hidden_dim, decoder_hidden_dim, embed_dim, key_value_size=128):\n",
        "        super(Seq2Seq,self).__init__()\n",
        "        self.encoder = Encoder(input_dim, encoder_hidden_dim, key_value_size=key_value_size)\n",
        "        self.decoder = Decoder(vocab_size, decoder_hidden_dim, embed_dim, key_value_size=key_value_size)\n",
        "        \n",
        "    def forward(self, x, x_len, y=None, mode='train', tf_rate=0.9):\n",
        "        x = T.FrequencyMasking(freq_mask_param=2)(x)\n",
        "        key, value, encoder_len = self.encoder(x, x_len)\n",
        "        key, value = self.decoder.multihead_attention.linear_transform(key,value)\n",
        "        predictions, attentions = self.decoder(key, value, encoder_len, y=y, mode=mode,Teacher_forcing_rate=tf_rate)\n",
        "        return predictions, attentions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9h-vz1fT09K"
      },
      "outputs": [],
      "source": [
        "model = Seq2Seq(input_dim=13, vocab_size=len(LETTER_LIST), encoder_hidden_dim=256, decoder_hidden_dim=512, embed_dim=256, key_value_size=256)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# print(model)\n",
        "# summary(encoder, x.to(device), lx) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHMzR6fLht5n"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HwmgDSvbtmd"
      },
      "outputs": [],
      "source": [
        "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.75, patience=5)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=0.01)\n",
        "num_steps = len(train_loader) * n_epochs\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_steps)\n",
        "warmup_scheduler = warmup.RAdamWarmup(optimizer)\n",
        "criterion = nn.CrossEntropyLoss(reduction='none').to(device)\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIXzhQclhs98"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer, mode, tf_rate,warmup_scheduler,lr_scheduler):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    attentions =None\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "\n",
        "    # 0) Iterate through your data loader\n",
        "    for i, (x, y, lx, ly) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        # 1) Send the inputs to the device\n",
        "        x,y = x.cuda(),y.long().cuda()\n",
        "        # print(x)\n",
        "        # print(y)\n",
        "\n",
        "        # 2) Pass your inputs, and length of speech into the model.\n",
        "        with torch.cuda.amp.autocast():  \n",
        "            predictions, attentions = model(x, lx, y, mode=mode,tf_rate=tf_rate)\n",
        "        \n",
        "            # 3) Generate a mask based on target length. This is to mark padded elements\n",
        "            # so that we can exclude them from computing loss.\n",
        "            # Ensure that the mask is on the device and is the correct shape.\n",
        "            mask =torch.zeros(y.size()).T.cuda()\n",
        "            for idx, length_Y in enumerate(ly):\n",
        "                mask[:length_Y,idx] = 1\n",
        "                \n",
        "            # 4) Make sure you have the correct shape of predictions when putting into criterion\n",
        "            loss = criterion(predictions.view(-1, predictions.size(2)), y.view(-1)) \n",
        "            # Use the mask you defined above to compute the average loss\n",
        "            masked_loss = torch.sum(loss * mask.reshape(-1)) / torch.sum(mask)\n",
        "        \n",
        "        running_loss += masked_loss.item()\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "                loss=\"{:.04f}\".format(float(masked_loss)),\n",
        "                lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "        \n",
        "        lr_wb = round(float(optimizer.param_groups[0]['lr']),8)\n",
        "\n",
        "        # 5) backprop\n",
        "        scaler.scale(masked_loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        with warmup_scheduler.dampening():\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        # Optional: Gradient clipping\n",
        "        # When computing Levenshtein distance, make sure you truncate prediction/target\n",
        "\n",
        "        batch_bar.update()\n",
        "    batch_bar.close()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    del x,y,lx, ly\n",
        "    torch.cuda.empty_cache()\n",
        "    return avg_loss, lr_wb\n",
        "\n",
        "        # Optional: plot your attention for debugging\n",
        "    # plot_attention(attentions)\n",
        "        \n",
        "def val(model, valid_loader):\n",
        "    model.eval()\n",
        "    model.cuda()\n",
        "    running_dist = 0\n",
        "    # running_loss = 0\n",
        "    total_num = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (x, y, lx, ly) in enumerate(valid_loader):\n",
        "            x, y= x.cuda(),y.long().cuda()\n",
        "            predictions, _ = model(x, lx, y=y, mode='eval')\n",
        "            \n",
        "            # text prediction\n",
        "            pred_text = transform_index_to_letter(predictions.argmax(-1).detach().cpu().numpy())    \n",
        "            target_text =transform_index_to_letter(y.detach().cpu().numpy())\n",
        "\n",
        "            for pred, target in zip(pred_text, target_text):\n",
        "                dist = lev.distance(pred, target)\n",
        "                running_dist += dist\n",
        "                total_num += 1\n",
        "    \n",
        "    avg_dist_eval = running_dist / total_num\n",
        "    # avg_loss_eval = running_loss / len(valid_loader)\n",
        "\n",
        "    del x,y,lx,ly\n",
        "    torch.cuda.empty_cache()\n",
        "    return  avg_dist_eval\n",
        "    # return avg_loss_eval, avg_dist_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gsc7IbxrpQI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "012136de-e5ee-4826-a132-d22ec1b173e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   0%|          | 0/223 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Avg-Loss: 1.8926060376145915, lr: 0.00032641\n",
            "Validation: Avg-distance: 555.0192378838328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Avg-Loss: 1.4430679050796236, lr: 0.00045332\n",
            "Validation: Avg-distance: 534.457269700333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Avg-Loss: 1.2996605132192773, lr: 0.00054383\n",
            "Validation: Avg-distance: 534.2571217166112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Avg-Loss: 1.2294078512042093, lr: 0.00061424\n",
            "Validation: Avg-distance: 534.0828708842027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Avg-Loss: 1.1649569406637696, lr: 0.00067099\n",
            "Validation: Avg-distance: 533.8830928597854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Avg-Loss: 1.1278275419778354, lr: 0.00071748\n",
            "Validation: Avg-distance: 533.9389567147614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Avg-Loss: 1.1632468446487803, lr: 0.0007558\n",
            "Validation: Avg-distance: 532.0281169071402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Avg-Loss: 1.145334342670013, lr: 0.00078739\n",
            "Validation: Avg-distance: 532.7580466148723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Avg-Loss: 1.1246213028249186, lr: 0.0008133\n",
            "Validation: Avg-distance: 532.3466518682945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Avg-Loss: 1.0311890968827389, lr: 0.00083433\n",
            "Validation: Avg-distance: 183.07954125046246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Avg-Loss: 0.5136403818301556, lr: 0.00085115\n",
            "Validation: Avg-distance: 57.93451720310766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: Avg-Loss: 0.3294719448939567, lr: 0.00086429\n",
            "Validation: Avg-distance: 32.990751017388085\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12: Avg-Loss: 0.256668900628261, lr: 0.00087422\n",
            "Validation: Avg-distance: 25.374768775434703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13: Avg-Loss: 0.2157791101210855, lr: 0.00088132\n",
            "Validation: Avg-distance: 22.22456529781724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14: Avg-Loss: 0.18746701898596213, lr: 0.00088593\n",
            "Validation: Avg-distance: 19.13984461709212\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15: Avg-Loss: 0.16173531389022622, lr: 0.00088834\n",
            "Validation: Avg-distance: 17.519792822789494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16: Avg-Loss: 0.16653002858696497, lr: 0.00088881\n",
            "Validation: Avg-distance: 17.20458749537551\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17: Avg-Loss: 0.15370811381682153, lr: 0.00088756\n",
            "Validation: Avg-distance: 15.554568997410286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18: Avg-Loss: 0.13964910574691713, lr: 0.0008848\n",
            "Validation: Avg-distance: 14.621161672216056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19: Avg-Loss: 0.126399547229166, lr: 0.00088068\n",
            "Validation: Avg-distance: 13.721790603033666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20: Avg-Loss: 0.12145601801006249, lr: 0.00087537\n",
            "Validation: Avg-distance: 14.223085460599334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21: Avg-Loss: 0.12787595518234066, lr: 0.00086899\n",
            "Validation: Avg-distance: 13.887532371439141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22: Avg-Loss: 0.11555570754903315, lr: 0.00086166\n",
            "Validation: Avg-distance: 13.199408065112838\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23: Avg-Loss: 0.109867776723186, lr: 0.00085347\n",
            "Validation: Avg-distance: 12.702182759896411\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24: Avg-Loss: 0.10501974600580241, lr: 0.00084452\n",
            "Validation: Avg-distance: 12.555678875323714\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25: Avg-Loss: 0.09816799620208184, lr: 0.00083488\n",
            "Validation: Avg-distance: 12.282278949315575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26: Avg-Loss: 0.1071558508504132, lr: 0.00082462\n",
            "Validation: Avg-distance: 12.23196448390677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27: Avg-Loss: 0.10230268664958765, lr: 0.00081379\n",
            "Validation: Avg-distance: 11.211986681465039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28: Avg-Loss: 0.09415356557599097, lr: 0.00080245\n",
            "Validation: Avg-distance: 12.016648168701442\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29: Avg-Loss: 0.08929679562826327, lr: 0.00079065\n",
            "Validation: Avg-distance: 11.963374028856826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30: Avg-Loss: 0.08360050307929248, lr: 0.00077842\n",
            "Validation: Avg-distance: 12.268220495745467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31: Avg-Loss: 0.08225604415674916, lr: 0.00076579\n",
            "Validation: Avg-distance: 10.631890492045875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32: Avg-Loss: 0.07699674200851286, lr: 0.00075282\n",
            "Validation: Avg-distance: 11.385127635960044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33: Avg-Loss: 0.08047362148026714, lr: 0.00073951\n",
            "Validation: Avg-distance: 11.792082870884203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34: Avg-Loss: 0.07085604928935055, lr: 0.0007259\n",
            "Validation: Avg-distance: 10.808731039585645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35: Avg-Loss: 0.07251947991249273, lr: 0.00071202\n",
            "Validation: Avg-distance: 12.36440991490936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36: Avg-Loss: 0.06560655920494832, lr: 0.00069789\n",
            "Validation: Avg-distance: 11.163522012578616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37: Avg-Loss: 0.059294243414172144, lr: 0.00068352\n",
            "Validation: Avg-distance: 10.516833148353681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38: Avg-Loss: 0.059955118416136155, lr: 0.00066895\n",
            "Validation: Avg-distance: 10.672216056233815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39: Avg-Loss: 0.054712193537787474, lr: 0.00065419\n",
            "Validation: Avg-distance: 10.506104328523863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40: Avg-Loss: 0.050145390121926106, lr: 0.00063925\n",
            "Validation: Avg-distance: 10.747687754347021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41: Avg-Loss: 0.047895762551525785, lr: 0.00062416\n",
            "Validation: Avg-distance: 10.633000369959305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42: Avg-Loss: 0.044734032974983545, lr: 0.00060894\n",
            "Validation: Avg-distance: 10.345172031076581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43: Avg-Loss: 0.04281221096291136, lr: 0.0005936\n",
            "Validation: Avg-distance: 10.4110247872734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44: Avg-Loss: 0.041316910483978786, lr: 0.00057816\n",
            "Validation: Avg-distance: 10.215316315205328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45: Avg-Loss: 0.03953916125099755, lr: 0.00056264\n",
            "Validation: Avg-distance: 10.338142804291527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46: Avg-Loss: 0.03890333760680104, lr: 0.00054704\n",
            "Validation: Avg-distance: 10.71624121346652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47: Avg-Loss: 0.036179249705287374, lr: 0.0005314\n",
            "Validation: Avg-distance: 10.43544210136885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48: Avg-Loss: 0.03581042561863837, lr: 0.00051573\n",
            "Validation: Avg-distance: 10.530151683314836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49: Avg-Loss: 0.0332425160692679, lr: 0.00050003\n",
            "Validation: Avg-distance: 10.341472438031817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50: Avg-Loss: 0.030175523331280246, lr: 0.00048433\n",
            "Validation: Avg-distance: 9.73510913799482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 51: Avg-Loss: 0.027698402781657572, lr: 0.00046865\n",
            "Validation: Avg-distance: 9.844617092119867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 52: Avg-Loss: 0.02698209356817296, lr: 0.000453\n",
            "Validation: Avg-distance: 9.818719940806512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 53: Avg-Loss: 0.025877041708794943, lr: 0.00043739\n",
            "Validation: Avg-distance: 10.371069182389936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 54: Avg-Loss: 0.024018754067542575, lr: 0.00042184\n",
            "Validation: Avg-distance: 9.752867184609693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 55: Avg-Loss: 0.023544930623251227, lr: 0.00040637\n",
            "Validation: Avg-distance: 9.645209027007029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 56: Avg-Loss: 0.02287157665052756, lr: 0.00039099\n",
            "Validation: Avg-distance: 9.965593784683685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 57: Avg-Loss: 0.021245777732254143, lr: 0.00037572\n",
            "Validation: Avg-distance: 10.13762486126526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 58: Avg-Loss: 0.018438258692789238, lr: 0.00036057\n",
            "Validation: Avg-distance: 9.493895671476137\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 59: Avg-Loss: 0.018240123870260513, lr: 0.00034555\n",
            "Validation: Avg-distance: 9.73103958564558\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 60: Avg-Loss: 0.016922664448552067, lr: 0.00033069\n",
            "Validation: Avg-distance: 9.712541620421753\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 61: Avg-Loss: 0.015313322929406514, lr: 0.000316\n",
            "Validation: Avg-distance: 9.751757306696263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 62: Avg-Loss: 0.01412334880110862, lr: 0.00030149\n",
            "Validation: Avg-distance: 9.484276729559749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 63: Avg-Loss: 0.013656042946393982, lr: 0.00028717\n",
            "Validation: Avg-distance: 9.384757676655568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 64: Avg-Loss: 0.012531962997442938, lr: 0.00027307\n",
            "Validation: Avg-distance: 9.465408805031446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 65: Avg-Loss: 0.01135633157507721, lr: 0.00025918\n",
            "Validation: Avg-distance: 9.280059193488716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 66: Avg-Loss: 0.010602592252562399, lr: 0.00024554\n",
            "Validation: Avg-distance: 9.439881613022568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 67: Avg-Loss: 0.010157747359136162, lr: 0.00023215\n",
            "Validation: Avg-distance: 9.603033666296707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 68: Avg-Loss: 0.009368919500153963, lr: 0.00021902\n",
            "Validation: Avg-distance: 9.210506844247133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 69: Avg-Loss: 0.008722931996505758, lr: 0.00020616\n",
            "Validation: Avg-distance: 9.418793932667407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 70: Avg-Loss: 0.008783398684732316, lr: 0.0001936\n",
            "Validation: Avg-distance: 9.274879763226044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 71: Avg-Loss: 0.007846009506188299, lr: 0.00018134\n",
            "Validation: Avg-distance: 9.295967443581207\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 72: Avg-Loss: 0.007212430748236553, lr: 0.0001694\n",
            "Validation: Avg-distance: 9.085460599334073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 73: Avg-Loss: 0.006860026678346064, lr: 0.00015778\n",
            "Validation: Avg-distance: 9.195708472068073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 74: Avg-Loss: 0.006119388659931552, lr: 0.0001465\n",
            "Validation: Avg-distance: 8.885682574916759\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 75: Avg-Loss: 0.0057169594353006425, lr: 0.00013556\n",
            "Validation: Avg-distance: 9.026637069922309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 76: Avg-Loss: 0.005383299875611882, lr: 0.00012499\n",
            "Validation: Avg-distance: 8.89197188309286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 77: Avg-Loss: 0.005341396338176901, lr: 0.00011479\n",
            "Validation: Avg-distance: 9.081761006289309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 78: Avg-Loss: 0.004608636488093924, lr: 0.00010497\n",
            "Validation: Avg-distance: 8.827968923418425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 79: Avg-Loss: 0.0044531617804342725, lr: 9.553e-05\n",
            "Validation: Avg-distance: 8.66518682944876\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 80: Avg-Loss: 0.004209498146072765, lr: 8.65e-05\n",
            "Validation: Avg-distance: 8.76174620791713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 81: Avg-Loss: 0.0038660115770152, lr: 7.787e-05\n",
            "Validation: Avg-distance: 8.645209027007029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 82: Avg-Loss: 0.0034531081850210314, lr: 6.966e-05\n",
            "Validation: Avg-distance: 8.692193858675546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 83: Avg-Loss: 0.0032436929229697035, lr: 6.188e-05\n",
            "Validation: Avg-distance: 8.700332963374029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 84: Avg-Loss: 0.003191027652227942, lr: 5.453e-05\n",
            "Validation: Avg-distance: 8.645209027007029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 85: Avg-Loss: 0.0030412662099846407, lr: 4.762e-05\n",
            "Validation: Avg-distance: 8.761006289308176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 86: Avg-Loss: 0.0029334113160524133, lr: 4.115e-05\n",
            "Validation: Avg-distance: 8.637069922308546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 87: Avg-Loss: 0.0028661693980908984, lr: 3.514e-05\n",
            "Validation: Avg-distance: 8.694043655197929\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 88: Avg-Loss: 0.002689988411349065, lr: 2.958e-05\n",
            "Validation: Avg-distance: 8.731409544950056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 89: Avg-Loss: 0.002605336162025702, lr: 2.449e-05\n",
            "Validation: Avg-distance: 8.686644469108398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 90: Avg-Loss: 0.002317739015282236, lr: 1.987e-05\n",
            "Validation: Avg-distance: 8.692563817980021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 91: Avg-Loss: 0.002325269850670473, lr: 1.573e-05\n",
            "Validation: Avg-distance: 8.706252312245653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 92: Avg-Loss: 0.002309668377968963, lr: 1.206e-05\n",
            "Validation: Avg-distance: 8.7055123936367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 93: Avg-Loss: 0.002228943240826724, lr: 8.87e-06\n",
            "Validation: Avg-distance: 8.605253422123566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 94: Avg-Loss: 0.0021696616838072246, lr: 6.17e-06\n",
            "Validation: Avg-distance: 8.685904550499446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 95: Avg-Loss: 0.0022693722941620367, lr: 3.95e-06\n",
            "Validation: Avg-distance: 8.65778764335923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 96: Avg-Loss: 0.0021108175187283967, lr: 2.23e-06\n",
            "Validation: Avg-distance: 8.642619311875693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 97: Avg-Loss: 0.0022211883657177567, lr: 9.9e-07\n",
            "Validation: Avg-distance: 8.619311875693674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 98: Avg-Loss: 0.002021765347096167, lr: 2.5e-07\n",
            "Validation: Avg-distance: 8.620421753607102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 99: Avg-Loss: 0.0020278023246304037, lr: 0.0\n",
            "Validation: Avg-distance: 8.608953015168332\n"
          ]
        }
      ],
      "source": [
        "# TODO: Define your model and put it on the device here\n",
        "# ...\n",
        "torch.cuda.empty_cache()\n",
        "nn.CrossEntropyLoss(reduction='none').to(device)\n",
        "# Make sure you understand the implication of setting reduction = 'none'\n",
        "criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "mode = 'train'\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    avg_loss, lr_wb = train(model, train_loader, criterion, optimizer, mode,tf_rate=tf,warmup_scheduler=warmup_scheduler,lr_scheduler=scheduler)\n",
        "    print(f'Epoch {epoch}: Avg-Loss: {avg_loss}, lr: {lr_wb}')\n",
        "\n",
        "    avg_dist_eval = val(model, val_loader)\n",
        "    print(f'Validation: Avg-distance: {avg_dist_eval}')\n",
        "\n",
        "    if epoch % tf_sep_epoch == 0:\n",
        "        tf = tf - 0.05\n",
        "        tf = max(0.6,tf)\n",
        "    \n",
        "    wandb.log({\"Avg-distance:\":avg_dist_eval,\"Training avg loss\":avg_loss, \"lr\":lr_wb, \"tf\":tf})\n",
        "\n",
        "\n",
        "    path = data_path+f\"/model_epoch_{epoch}.ckpt\"\n",
        "    torch.save({'epoch': epoch,'model_state_dict': model.state_dict(),'optimizer_state_dict': optimizer.state_dict()}, path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path"
      ],
      "metadata": {
        "id": "7wXexjAjQ23c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b9932196-eefb-4fee-c828-38153c913ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/hw4/store_checkpoints/Freq_Aug_2_Multiheaded_Masking_AdamW_SameKeyValueIncreseTest_Cepstral_tf_0.9_sep5_embedding_init_warmup_radam_lockeddropout_0.2/model_epoch_99.ckpt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# path = \"/content/drive/MyDrive/hw4/store_checkpoints/KeyValueIncreseTest_Cepstral_tf_0.9_sep5_embedding_init_warmup_radam_lockeddropout_0.2/model_epoch_87.ckpt\"\n",
        "# checkpoint = torch.load(path)\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ],
      "metadata": {
        "id": "CthJ3MabMxub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission"
      ],
      "metadata": {
        "id": "A0-nEa6P_Fpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def testing(model, test_loader):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    pred_text_cat = []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (x,lx) in enumerate(test_loader):\n",
        "            x = x.to(device)\n",
        "            predictions,_ = model(x, lx, mode='eval')\n",
        "            pred_text = transform_index_to_letter(predictions.argmax(-1).detach().cpu().numpy())\n",
        "            # print(predText)    \n",
        "            pred_text_cat.extend(pred_text)\n",
        "\n",
        "    del x,lx\n",
        "    torch.cuda.empty_cache()\n",
        "    return pred_text_cat\n",
        "\n",
        "res = testing(model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X94V9zmH_ObR",
        "outputId": "73808581-2643-41ff-f505-09bc91615564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write csv file\n",
        "import pandas as pd\n",
        "id = np.array(range(len(res)))\n",
        "predictions = res\n",
        "df = pd.DataFrame({\"id\":id,\"predictions\":predictions})\n",
        "df.to_csv(\"submission.csv\",index=False)\n",
        "\n",
        "!kaggle competitions submit -c 11-785-s22-hw4p2 -f submission.csv -m \"Message\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqNUFlGYnbPD",
        "outputId": "cc332207-8af8-45f0-d5dc-d7ecf76ba003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r  0% 0.00/288k [00:00<?, ?B/s]\r100% 288k/288k [00:00<00:00, 1.48MB/s]\n",
            "Successfully submitted to Attention-Based Speech Recognition"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "id": "n567AYILUI77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRdVGDh0aoiN"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Debugging suggestions from Eason, a TA from previous semesters:\n",
        "\n",
        "(1) Decrease your batch_size to 2 and print out the value and shape of all intermediate variables to check if they satisfy the expectation\n",
        "(2) Be super careful about the LR, don't make it too high. Too large LR would lead to divergence and your attention plot will never make sense\n",
        "(3) Make sure you have correctly handled the situation for time_step = 0 when teacher forcing\n",
        "\n",
        "(1) is super important and is the most efficient way for debugging. \n",
        "'''\n",
        "'''\n",
        "Tips for passing A from B (from easy to hard):\n",
        "** You need to implement all of these yourself without utilizing any library **\n",
        "(1) Increase model capacity. E.g. increase num_layer of lstm\n",
        "(2) LR and Teacher Forcing are also very important, you can tune them or their scheduler as well. Do NOT change lr or tf during the warm-up stage!\n",
        "(3) Weight tying\n",
        "(4) Locked Dropout - insert between the plstm layers\n",
        "(5) Pre-training decoder or train an LM to help make predictions\n",
        "(5) Pre-training decoder to speed up the convergence: \n",
        "    disable your encoder and only train the decoder like train a language model\n",
        "(6) Better weight initialization technique\n",
        "(7) Batch Norm between plstm. You definitely can try other positions as well\n",
        "(8) Data Augmentation. Time-masking, frequency masking\n",
        "(9) Weight smoothing (avg the last few epoch's weight)\n",
        "(10) You can try CNN + Maxpooling (Avg). Some students replace the entire plstm blocks with it and some just combine them together.\n",
        "(11) Beam Search\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "letter2index"
      ],
      "metadata": {
        "id": "nzg_hxMhjoh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnhNnKH4aoiN"
      },
      "source": [
        "# Toy Data for debugging"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /hw4p2_simple/hw4p2_simple\n",
        "# Load the training, validation and testing data\n",
        "train_data = np.load('train.npy', allow_pickle=True, encoding='bytes')\n",
        "valid_data = np.load('dev.npy', allow_pickle=True, encoding='bytes')\n",
        "\n",
        "# Load the training, validation raw text transcripts\n",
        "raw_train_transcript = np.load('train_transcripts.npy', allow_pickle=True,encoding='bytes')\n",
        "raw_valid_transcript = np.load('dev_transcripts.npy', allow_pickle=True,encoding='bytes')"
      ],
      "metadata": {
        "id": "hUJDLe9rLkiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # For training and validation set, return x and y\n",
        "        X = torch.tensor(self.X[index])\n",
        "        # print(self.Y[index])\n",
        "        Y = [letter2index[yy] for yy in self.Y[index]]\n",
        "        Yy = torch.LongTensor(Y)\n",
        "        return X, Yy\n",
        "    \n",
        "    def collate_fn(batch):\n",
        "        batch_x = [x for (x, y) in batch]\n",
        "        batch_y = [y for (x, y) in batch]\n",
        "\n",
        "        batch_x_pad = pad_sequence([torch.Tensor(x) for x in batch_x],batch_first=True)  # TODO: pad the sequence with pad_sequence (already imported)\n",
        "        lengths_x = torch.LongTensor([len(x) for x in batch_x])  # TODO: Get original lengths of the sequence before padding\n",
        "\n",
        "        batch_y_pad = pad_sequence(batch_y,batch_first=True,padding_value=0)  # TODO: pad the sequence with pad_sequence (already imported)\n",
        "        lengths_y = torch.LongTensor([len(y) for y in batch_y])   # TODO: Get original lengths of the sequence before padding\n",
        "\n",
        "        return batch_x_pad, batch_y_pad, torch.tensor(lengths_x), torch.tensor(lengths_y)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = MyDataset(train_data,raw_train_transcript)\n",
        "valid_dataset = MyDataset(valid_data,raw_valid_transcript)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=64, collate_fn = MyDataset.collate_fn,num_workers=0, pin_memory=True)\n",
        "val_loader = DataLoader(valid_dataset, shuffle=False, batch_size=128, collate_fn = MyDataset.collate_fn,num_workers=0, pin_memory=True)\n"
      ],
      "metadata": {
        "id": "1rQLTyCPLpQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, criterion, optimizer, mode):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    attentions = None\n",
        "    \n",
        "    # 0) Iterate through your data loader\n",
        "    for i, (x, y, lx, ly) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        # 1) Send the inputs to the device\n",
        "        x,y = x.cuda(),y.long().cuda()\n",
        "        # print(x)\n",
        "        # print(y)\n",
        "\n",
        "        # 2) Pass your inputs, and length of speech into the model.\n",
        "        predictions, attentions = model(x, lx, y, mode=mode)\n",
        "        \n",
        "        # 3) Generate a mask based on target length. This is to mark padded elements\n",
        "        # so that we can exclude them from computing loss.\n",
        "        # Ensure that the mask is on the device and is the correct shape.\n",
        "        mask =torch.zeros(y.size()).T.to(device)\n",
        "        for idx, length_Y in enumerate(ly):\n",
        "            mask[:length_Y,idx] = 1\n",
        "            \n",
        "        # 4) Make sure you have the correct shape of predictions when putting into criterion\n",
        "        loss =  criterion(predictions.view(-1, predictions.size(2)), y.view(-1)) \n",
        "        # Use the mask you defined above to compute the average loss\n",
        "        masked_loss = torch.sum(loss * mask.reshape(-1)) / torch.sum(mask)\n",
        "\n",
        "        # 5) backprop\n",
        "        masked_loss.backward()\n",
        "        # Optional: Gradient clipping\n",
        "\n",
        "        # When computing Levenshtein distance, make sure you truncate prediction/target\n",
        "        optimizer.step()\n",
        "\n",
        "        # Optional: plot your attention for debugging\n",
        "    plot_attention(attentions)\n",
        "        \n",
        "def val(model, valid_loader):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    runningDist = 0\n",
        "    tot_sec = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (x,y,x_len,y_len) in tqdm(enumerate(valid_loader),position=0, leave=True):\n",
        "            x,y,x_len,y_len = x.to(device),y.long().to(device),x_len.to(device),y_len.to(device)\n",
        "            predictions,_ = model(x, x_len, y=y, mode='eval')           \n",
        "\n",
        "            predText = transform_index_to_letter(predictions.argmax(-1).detach().cpu().numpy())    \n",
        "            targetText =transform_index_to_letter(y.detach().cpu().numpy())\n",
        "\n",
        "            print(\"=============predText=================\")\n",
        "            print(predText)\n",
        "            print(\"y\",y)\n",
        "            print(\"y.shape\",y.shape) # (100,36)\n",
        "            print(\"=============predText=================\")\n",
        "\n",
        "            # print(\"=============TargetText=================\")\n",
        "            # print(targetText)\n",
        "            # print(\"=============TargetText=================\")\n",
        "            for pred, target in zip(predText, targetText):\n",
        "                dist = lev.distance(pred, target)\n",
        "                runningDist += dist\n",
        "                tot_sec += 1\n",
        "\n",
        "    print('eval:')\n",
        "    print('Avg-distance: {:.5f}'.format(runningDist / tot_sec))\n",
        "    del x,y,x_len,y_len\n",
        "    torch.cuda.empty_cache()\n",
        "    return runningDist / tot_sec\n",
        "\n",
        "def transform_index_to_letter(batch_indices):\n",
        "    '''\n",
        "    Transforms numerical index input to string output by converting each index \n",
        "    to its corresponding letter from LETTER_LIST\n",
        "\n",
        "    Args:\n",
        "        batch_indices: List of indices from LETTER_LIST with the shape of (N, )\n",
        "    \n",
        "    Return:\n",
        "        transcripts: List of converted string transcripts. This would be a list with a length of N\n",
        "    '''\n",
        "    transcripts = []\n",
        "    start_index = letter2index['<sos>']\n",
        "    stop_index = letter2index['<eos>']\n",
        "\n",
        "    print(\"batch indices\",batch_indices)\n",
        "\n",
        "    for tr in batch_indices:\n",
        "      curr = \"\"\n",
        "      for i in tr:\n",
        "        if i == stop_index:\n",
        "          break\n",
        "        elif i == start_index:\n",
        "          pass\n",
        "        else:\n",
        "          curr += index2letter[i]\n",
        "      transcripts.append(curr)\n",
        "    \n",
        "    print(transcripts)\n"
      ],
      "metadata": {
        "id": "J4AmyP7jLv-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Define your model and put it on the device here\n",
        "# ...\n",
        "torch.cuda.empty_cache()\n",
        "n_epochs = 20\n",
        "nn.CrossEntropyLoss(reduction='none').to(device)\n",
        "# Make sure you understand the implication of setting reduction = 'none'\n",
        "criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "mode = 'train'\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train(model, train_loader, criterion, optimizer, mode)\n",
        "    val(model, val_loader)"
      ],
      "metadata": {
        "id": "aFksaiIAL7dw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "i5ioyn6ldQB9"
      ],
      "name": "README_HW4p2_AugFreq_2_masking_Multi_headedTrial_warm_dropblockw.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}